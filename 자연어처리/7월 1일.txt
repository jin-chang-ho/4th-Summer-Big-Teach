Train data : 훈련하는 데이터
Validatation data : 하나의 epoch마다 정확도를 매겨보는 데이터
Test data : 모델을 검증하는 데이터
epoch가 최적인 걸 validation data가 알려준다. so, 모델이 validation data에 과적합될 수 있다.
Test에 맞춰서 hyperparameter를 튜닝하는 것은 무조건 한 번만 시도해야 test data에 과적합되지 않는다. so, test data를 많이 확보하는 게 중요하다.

Word Embedding
유사어 찾기 문제를 상당히 해결(반의어도 어느정도 해결)
layer에 word embedding을 넣어줌으로써 
유사한 단어들을 같은 분류로 처리할 수 있어 공간 효율성이 높아진다.
고빈도 어휘에서는 높은 성능, 저빈도 어휘에서는 낮은 성능을 보인다.

Word Embedding 활용
유사어 찾기
사전 훈련된 임베딩을 이용한 이진 분류
학습 데이터로 훈련된 임베딩을 이용한 이진 분류

차원(단어의 특징)의 표현
다차원이 될수록 표현이 정밀해진다.
차원의 값이 유사하면 유사한 단어이다.
차원의 방향성 : 2차원부터는 방향성을 가진다. (이것 또한 유사성)

임베딩 결과
one-hot-encoding : 0, 1로 구성 VS embedding 여러개의 실수로 구성

최적의 임베딩 차원
정답은 없으나 50~200 차원을 많이 사용하고 속도가 문제되지 않으면 300 차원
차원을 늘일수록 훈련시간이 늘어난다. 200차원 이상부터는 가성비가 떨어진다.

워드 임베딩을 이용한 모델
워드 임베딩을 입력층에서 첫번째 은닉층 사이에 사용하는이유
첫번째 이유 : 첫 번째 은닉층이 큰 일반화를 해주고 두, 세 .. 번째 은닉층이 세부적으로 분류한다. 첫번째 은닉층의 분류가 중요하므로 해당 위치에 둔다.
두번째 이유 : 첫 번째 층의 weight를 바꾸지 않아도 되기 때문에 시간을 아낄 수 있다.
one-hot encoding을 쓰지 않는데, one-hot encoding을 쓰면 word embedding 모델이 0, 1만 들어왔다고 생각하게 된다. 
모델 설계시에는 모델의 임베딩 차원이 얼마인지 알고 그대로 적용해야 한다.
입력층 -> 임베딩(batch_size, input_length(3), feature_dim(5)) -> Flatten(input_length와 feature_dim을 하나로 엮어줌(batch_size, 15)) -> Dense layer
RNN을 쓴다면 틀림없이 word_embedding을 썼다고 생각함 : 입력 3D, 출력 2D or 3D -> So, Flatten이 필요가 없어짐

Word2Vec(기본적인 워드 임베딩 기법)
CBOW(문맥 단어로 타깃 단어 하나를 예측), Skip-gram(타깃 단어로 주변 문맥 단어를 예측)이라는 두 개의 하위 모델
Skip-gram의 임베딩 품질이 CBOW보다 좋다. (출력 데이터 차이 때문에)
평균값
Word2Vec의 출력 값은 희소 행렬이 아닌 밀집 행렬을 사용해도 성능이 개선되는 경우가 있다. 

RNN
bag 개념을 인정하지 않고 순서가 정확도에 영향을 끼친다고 생각하는 모델 (ex. 뒤 단어가 예상이 된다.)
처음 입력값은 입력층 -> 은닉층 -> 출력층
다음 입력값은 입력측 -> 은닉층 & 그 전 은닉층 -> 출력층
일반 순환신경망 : 마지막 타입스텝에서 최종 출력 되기 전까지 나머지 입력층의 결과는 출력하지 않는다.
input이 3D(batch_size, time_steps(input_length), feature_len(embedding dim))
문제점 
은닉층의 가중치가 계속 곱해지므로 가중치 절댓값이 1보다 작으면 매우 작은 값이 나오고 가중치 절댓값이 1보다 크면 매우 큰 값이 나온다. 
일반적으로 그래디언트 소실이 많이 일어난다.
이는 문장이 길어질수록 심해진다. 이를 앞의 정보가 뒤로 잘 전달되니 못한다고 표현한다.

units -> 70 why?

LSTM
RNN의 과거 정보의 소실을 막은 모델이다.
장기 메모리, 입력 게이트, 삭제 게이트, 출력 게이트
4 가지 장치를 위한 연산이 각각 별도로 존재해서 RNN에 비해 계산량이 매우 많다.
장기 메모리 때문에 긴 문장에서도 그래디언트 소실이 일어나지 않는다.
Cell State(장기 메모리) -> Forget Gate : 곱하기를 하여 값이 작아진다. -> Input Gate : 더하기를 하여 값을 더해준다. -> 
Output Gate : 해당 값은 2개로 복사되는데 한 값은 Cell State 형태로 다른 은닉층으로 가고, 한 값은 tanh를 적용하고 곱하기를 하여 값을 줄인 후 2개로 복사된다.
한 값은 출력, 다른 한 값은 Weight 형태로 다른 은닉층으로 간다.
Input Gate는 tanh, sigmoid를 적용한 후 두 값을 곱하는 과정을 하는 데 더할 값이 너무 커질까봐 이를 줄이는 데 목적이 있다.

GRU
LSTM의 계량형 GPU의 보급에 따라 최근에는 다시 LSTM으로 돌아가는 추세이다.
Cell State가 없이 하나의 상태 벡터를 가진다.
Reset Gate는 삭제 게이트, Update Gate는 입력 게이트, 출력 게이트를 담당한다.
Update Gate
Zt가 1에 가까우면 Input Gate에 거의 0인 값이 전달되 Input Gate가 닫히고, Zt가 0에 가까우면 Forget Gate에 거의 0인 값이 전달되 Forget Gate가 닫힌다. 
즉 Zt - 1을 통해 Forget Gate, Input Gate 간의 적절한 균형을 맞춘다.






























































































































