{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "뉴스 기사의 댓글 정보 수집하기\n",
      "================================================================================\n",
      "1.크롤링 할 건수는 몇건입니까?(10건단위로 입력): 30\n"
     ]
    }
   ],
   "source": [
    "# Chap 16. 다양한 댓글 모으기\n",
    "# 뉴스 기사의 댓글 모으기 - 미세먼지 / 스모그  \n",
    "# 테스트 기사 URL : https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=102&oid=056&aid=0010661268\n",
    "\n",
    "#Step 1. 필요한 모듈과 라이브러리를 로딩합니다.\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import math\n",
    "import numpy  \n",
    "import pandas as pd  \n",
    "import random\n",
    "import os\n",
    "import re\n",
    "\n",
    "#Step 2. 사용자에게 검색어 키워드를 입력 받고 저장할 폴더와 파일명을 설정합니다.\n",
    "print(\"=\" *80)\n",
    "print(\"뉴스 기사의 댓글 정보 수집하기\")\n",
    "print(\"=\" *80)\n",
    "\n",
    "query_txt = '뉴스기사댓글'\n",
    "query_url = \"https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=102&oid=056&aid=0010661268\"\n",
    "cnt = int(input('1.크롤링 할 건수는 몇건입니까?(10건단위로 입력): '))\n",
    "page_cnt = math.ceil(cnt / 20)\n",
    "\n",
    "f_dir = \"C:\\\\Users\\\\Windows\\\\Desktop\\\\대학교\\\\4학년 여름방학\\\\Big_AI\\\\크롤링\\\\코드\\\\Chap_5\\\\출력 파일\\\\\"\n",
    "\n",
    "# 저장될 파일위치와 이름을 지정합니다\n",
    "now = time.localtime()\n",
    "s = '%04d-%02d-%02d-%02d-%02d-%02d' % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "\n",
    "os.makedirs(f_dir+s+'-'+query_txt)\n",
    "os.chdir(f_dir+s+'-'+query_txt)\n",
    "\n",
    "ff_name=f_dir+s+'-'+query_txt+'\\\\'+s+'-'+query_txt+'.txt'\n",
    "fc_name=f_dir+s+'-'+query_txt+'\\\\'+s+'-'+query_txt+'.csv'\n",
    "fx_name=f_dir+s+'-'+query_txt+'\\\\'+s+'-'+query_txt+'.xls'\n",
    "\n",
    "#Step 3. 크롬 드라이버를 사용해서 웹 브라우저를 실행합니다.\n",
    "s_time = time.time( )\n",
    "\n",
    "s = Service(\"C:\\\\chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "driver.get(query_url)\n",
    "driver.maximize_window()\n",
    "time.sleep(8)\n",
    "\n",
    "#Step 4. 현재 총 리뷰 건수를 확인하여 사용자의 요청건수와 비교 후 동기화합니다\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "result= soup.find('div', class_='u_cbox_head').find('span','u_cbox_count')\n",
    "result2 = result.get_text()\n",
    "\n",
    "print(\"=\" *80)\n",
    "result3 = result2.replace(\",\",\"\")\n",
    "result4 = re.search(\"\\d+\",result3)\n",
    "search_cnt = int(result4.group())\n",
    "\n",
    "if cnt > search_cnt :\n",
    "    cnt = search_cnt\n",
    "\n",
    "print(\"전체 검색 결과 건수 :\",search_cnt,\"건\")\n",
    "print(\"실제 최종 출력 건수\",cnt)\n",
    "print(\"실체 출력될 최종 페이지수\" , page_cnt)\n",
    "\n",
    "# Step 5. 사용자가 요청한 건수가 많을 경우 리뷰 더보기 버튼을 클릭합니다\n",
    "# 최초 10건 수집후 댓글 더보기 버튼 클릭\n",
    "# 아래 버튼을 눌러 첫 화면에 총 20건의 댓글이 나오게 만듦\n",
    "driver.find_element(By.LINK_TEXT,'댓글 더보기').click()\n",
    "time.sleep(3)\n",
    "\n",
    "#Step 6. 20건 출력되어 있는 현재 페이지 리뷰와 점수 등 내용 수집\n",
    "writer_id2=[]       # 리뷰 작성자 ID\n",
    "review2=[]          # 리뷰 내용\n",
    "write_date2=[]      # 리뷰 작성 일자\n",
    "gogam_0=[]          # 공감 횟수\n",
    "gogam_1=[]          # 비공감 횟수\n",
    "count = 0\n",
    "\n",
    "for a in range(1,page_cnt+1) :\n",
    "    \n",
    "    if a == page_cnt :\n",
    "          break\n",
    "    else :\n",
    "        driver.find_element(By.XPATH,'//*[@id=\"cbox_module\"]/div[2]/div[10]/a').click() \n",
    "        time.sleep(3)\n",
    "        print(\"%s페이지 이동 완료========================================================\" %a)\n",
    "        time.sleep(random.randrange(1,3))  # 1-2 초 사이에 랜덤으로 시간 선택\n",
    "\n",
    "print('이제 리뷰 정보를 수집합니다. 잠시만 기다려 주세요~~~~~~~~')\n",
    "\n",
    "#txt 파일에 저장하기 위해 파일 open하기\n",
    "f = open(ff_name, 'a',encoding='UTF-8')\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "reple_result = soup.find('div', class_='u_cbox_content_wrap').find('ul')\n",
    "slist = reple_result.find_all('li')\n",
    "\n",
    "for li in slist:\n",
    "    count += 1\n",
    "    print(\"\\n\")\n",
    "    print(\"총 %s건 중 %s번째 댓글 수집 중입니다 ==================\" %(cnt,count))\n",
    "\n",
    "    writer_id = li.find('span', class_='u_cbox_nick').get_text()\n",
    "    print(\"1.작성자ID:\", writer_id)\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"총 %s 건 중 %s 번째 리뷰 데이터를 수집합니다==============\" %(cnt,count) + \"\\n\")\n",
    "    f.write(\"1.작성자ID:\"+writer_id + \"\\n\")\n",
    "    writer_id2.append(writer_id)\n",
    "\n",
    "    try :\n",
    "        review = li.find('span', class_='u_cbox_contents').get_text()\n",
    "    except AttributeError :\n",
    "        review='작성자에 의해 삭제된 댓글입니다'\n",
    "        print(\"2.리뷰 :\",review)\n",
    "    else :\n",
    "        print(\"2.리뷰:\",review)\n",
    "    f.write(\"2.리뷰:\" + review + \"\\n\")\n",
    "    review2.append(review)\n",
    "\n",
    "    write_date = li.find('span',class_='u_cbox_date').get_text()\n",
    "    print('3.작성일자:',write_date)\n",
    "    f.write(\"3.작성일자:\" + write_date + \"\\n\")\n",
    "    write_date2.append(write_date)\n",
    "\n",
    "    gogam = li.find('div', class_='u_cbox_recomm_set').find_all('em')\n",
    "\n",
    "    try :\n",
    "        g_gogam = gogam[0].text\n",
    "        print('4.공감:',g_gogam)\n",
    "    except IndexError :\n",
    "        g_gogam = '0'\n",
    "        print('4.공감 :',g_gogam)\n",
    "    f.write(\"4.공감:\" + g_gogam + \"\\n\")\n",
    "    gogam_0.append(g_gogam)\n",
    "\n",
    "    gogam = li.find('div', class_='u_cbox_recomm_set').find_all('em')\n",
    "\n",
    "    try :\n",
    "        b_gogam = gogam[1].text\n",
    "        print('5.비공감:',b_gogam) \n",
    "    except IndexError :\n",
    "        b_gogam = '0'\n",
    "        print('5.비공감 :',b_gogam)\n",
    "    f.write(\"5.비공감:\" + b_gogam + \"\\n\")\n",
    "    gogam_1.append(b_gogam)\n",
    "      \n",
    "    time.sleep(0.2)      \n",
    "\n",
    "    if count == cnt :\n",
    "         break\n",
    "                   \n",
    "# 학습목표 3. 수집된 데이터를 표 형태로 저장하기\n",
    "#Step 7. xls 형태와 csv 형태로 저장하기\n",
    "news_reple = pd.DataFrame()\n",
    "news_reple['작성자ID']=pd.Series(writer_id2)\n",
    "news_reple['리뷰내용']=pd.Series(review2)\n",
    "news_reple['작성일자']=pd.Series(write_date2)\n",
    "news_reple['공감횟수']=pd.Series(gogam_0)\n",
    "news_reple['비공감횟수']=pd.Series(gogam_1)\n",
    "\n",
    "# csv 형태로 저장하기\n",
    "news_reple.to_csv(fc_name,encoding=\"utf-8-sig\",index=True)\n",
    "\n",
    "# 엑셀 형태로 저장하기\n",
    "news_reple.to_excel(fx_name ,index=True , engine='openpyxl')\n",
    "\n",
    "\n",
    "# Step 8. 요약 정보 출력하기\n",
    "e_time = time.time( )\n",
    "t_time = e_time - s_time\n",
    "\n",
    "print(\"\\n\") \n",
    "print(\"=\" *120)\n",
    "print(\"1.요청된 총 %s 건의 리뷰 중에서 실제 크롤링 된 리뷰수는 %s 건입니다\" %(cnt,count))\n",
    "print(\"2.총 소요시간은 %s 초 입니다 \" %round(t_time,1))\n",
    "print(\"3.파일 저장 완료: txt 파일명 : %s \" %ff_name)\n",
    "print(\"4.파일 저장 완료: csv 파일명 : %s \" %fc_name)\n",
    "print(\"5.파일 저장 완료: xls 파일명 : %s \" %fx_name)\n",
    "print(\"=\" *120)\n",
    "\n",
    "driver.close( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
