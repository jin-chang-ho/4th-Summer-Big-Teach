{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      " 이 크롤러는 RISS 사이트의 논문 및 학술자료 수집용 웹크롤러입니다.\n",
      "====================================================================================================\n",
      "1.수집할 자료의 키워드는 무엇입니까? : 해양자원\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "2. 위 키워드로 아래의 장르 중 어떤 장르의 정보를 수집할까요?\n",
      "\n",
      "1.학위논문\t2.국내학술논문\t3.해외학술논문\t4.학술지\n",
      "5.단행본\t6.공개강의\t7.연구보고서\n",
      "\n",
      "위 장르 중 수집할 장르의 번호를 입력하세요 : 2\n",
      "1발표논문 / 해양생물자원으로서 해조류 : 생물활성물질의 정제와 분자적 응용홍용기(Yong Ki Hong)한국조류학회2000국제심포지움 일정 및 발표논문집 - 21세기, 해양환경과 해양생물자원의 전망Vol.- No.-해조생물공학은 유용해조류 및 그들의 유용물질들을 경제적으로 생산할 수 있도록 개발하는 학문 분야이다. 본 논문에서는 이들 해조생물공학에 대한 실험방법적 접근을 강조하여 설명하고자 한다. 우선 해조류의 활력측정은 조직 자체의 효소활성 즉 무색의 2,3,5-triphenyltetrazolium chloride를 붉은색의 triphenylformazan으로 환원시키는 정도를 측정함으로서 정량적으로 간단하게 활력을 평가하는 방법을 확립하였다. 그리고 callus와 엽체 재생 등을 위한 무균 고체 조직배양에 가장 적합한 gelling agent들을 비교한바 high gel strength agar가 톳의 callus 형성 및 엽체 재생에 지지체로 가장 효율적이였다. 유용 생리활성물질의 탐색작업에서는 보라색우무 추출물에서 Taq DNA polymerase에 대한 저해작용을 보였으며, 청각, 구멍갈파래, 잎파래, 미역에는 Herpes simplex virus에 대한 항virus효과를 지녔다. 홑파래물추출물에서는 생먹이용 미세조류들의 성장에 촉진효과를 보였으며, 작은구슬산호말은 적조들의 성장을 억제시켰다. 넓패는 가시파래 및 담치에 대한 부착 억제작용을 보였다. 또한 김과 홑파래는 고cholesterol혈증에서 cholesterol수치를 강하시켰다.  그리고 해조류들의 유전분석을 위하여 DNA와 RNA 추출은 LiCl를 사용하여 간단하게 분리할수있다. 긴잎돌김, 여름김, 양식김 5종에 대한 18s rDNA 염기서열을 밝혔으며 RAPD 기법을 사용하여 지역간 톳 및 갈파래목의 유전적 다양성을 확인하였다. 김의 분화조직 들간 및 김의 산처리시 조직에서 반응을 나타내는 유전자들을 differential display 방법으로 분리하였다. 끝으로 김의 유전적 표식자를 gene gun을 사용하여 쉽게 김 엽체에 beta-glucuronidase gene을 도입할수 있었으며, 김의 품종 개량을 위하여 산 내성 및 필수 아미노산 고함유 품종을 선별중에 있다. Seaweed biotechnology is the field dealing with economical production of useful seaweeds and their chemicals. In this paper I will mainly discuss technical studies for the screening of biologically active substances and seaweed molecular characterization. At first, a seaweed viability assay was developed to evaluate tissue intactness using the enzymatic reduction of 2,3,5-triphenyltetrazolium chloride. Callus and blade formation, under axenic culture conditions of Hizikia fusiformis, depended on the selling agents used as a substrate. The most calli and blades were produced on solid media composed of 2% and 0.5% high gel strength agars, respectively. For biologically active substances from seaweed extracts, we have found a Taq DNA polymerase inhibitor, 2,3,6-tribromo-4,5-dihydroxyl benzyl methyl ether, from Symphyocladia latiuscula. Antiviral activity against Herpes simplex was detected from several seaweed extracts after light irradiation. Growth of feed phytoplanktons was enhanced by the addition of aqueous extract of the seaweed Monostroma nitidum. Algicidal activity of the seaweed Corallina pilulifera was detected against red tide microalgae. Antifouling activity of the Ishige folicea was detected against marine green algae and blue mussel. For the molecular characterization of seaweeds, DNA and RNA were extracted with the use of lithium chloride. The DNA was used as a template for PCR amplification to discriminate algae and for RAPD analysis. The RNA was of sufficient quality to be used as a cDNA template for differential display. We are now selecting acid-responding genes from the Porphyra yezoensis. Finally we have tried gene transfer into young P. yezoensis thalli by particle bombardment. Thalli, bombarded with tungsten particles coated with a beta-glucuronidase gene, showed a transient expression of the gene at 18.6 unit/mg/h. Selections of acid resistant stain and amino acid rich strain are going on for strain improvement of P. yezoensis.  Key words: biologically active substance, differential display, genetic analysis, seaweed, seaweed biotechnology\n",
      "2수거된 해양폐기물 자원화 기술 개발(Ⅰ)길상인(Sang-In Keel),윤진한(Jin-Han Yun),최연석(Yeon-Seok Choi),강창구(Chang-Gu Kang),유정석(Jeong-Seok Yu)한국해양환경·에너지학회2002한국해양환경·에너지학회지Vol.5 No.2원문보기 3NDSLDBpiaDBpia본 연구는 수거된 해양폐기물을 원료로 이용하여 폐기물연료(RDF : Refuse Derived Fuel)를 제조하는 공정 개발을 목적으로 하고 있으며, 산업용 연료로서의 활용 가능성을 확인하기 제조된 RDF의 물성분석을 분석하였다. 해양폐기물은 로우프에서의 납제거와 파쇄 그리고 해저 슬러지의 세척과 같은 전처리, 그리고 가연성분의 입자화 공정을 통하여 해양폐기물이 에너지로 변환하는 것이 가능하게 되었다. 이와 같은 해양폐기물 자원화 공정은 수거 폐기물의 환경적 처리는 물론 대체에너지 확보의 측면에서 매우 유익한 것으로 사료된다. The purpose of this study is the RDF process development for the disposal of marine debris, and physical properties of RDF was analyzed for the reliability as a industrial fuel. By the separation of lead from the waste rope and the pelletizing of burning material, marine debris changes to fuel resources. The resource recycling process is effective in the clean treatment of waste and the secure of substitute energy.\n",
      "원문보기 3NDSLDBpiaDBpia\n",
      "NDSL\n",
      "DBpia\n",
      "DBpia\n",
      "3수거된 해양폐기물 자원화 기술 개발(Ⅱ)길상인(Sang-In Keel),김석준(Seock-Joon Kim),윤진한(Jin-Han Yun),강창구(Chang-Gu Kang),유정석(Jeong-Seok Yu)한국해양환경·에너지학회2002한국해양환경·에너지학회지Vol.5 No.2원문보기어구용 폐스티로폼을 자원으로 활용할 수 있도록 하기 위하여 세정과 건조공정을 도입하여 감용하는 장치를 자체 개발하였으며, 시스템을 폐스티로폼 발생 지자체에 설치하여 설비의 운전 신뢰성을 확인하였다. 굴피 및 홍합피 등의 이물질을 완벽하게 제거하고 해수에 포함된 염분을 육상에서 발생되는 스티로폼 수준까지 낮춤으로써 어구용 폐스티로폼은 플라스틱의 원료로 탈바꿈할 수 있었으며, 종래의 처리 방법에 비해 1/10수준에 불과한 비용으로 어구용 폐스티로폼의 처리가 가능하도록 함으로써 설비의 설치나 운영에 따르는 경제성 문제를 해결하고자 하였다. By the introduction of cleaning and drying processes, thermal extrusion system for the volume reduction of used polystyrene buoys was developed. It was tested in the costal area for the determination of operational reliability. By the removal of oyster shells and cleaning of salt, waste polystyrene buoys was changed to the raw material of plastics. The lower cost of one-tenth compared with that of the outer request treatment is promising the practical use of waste buoys' volume reduction system.\n",
      "원문보기\n",
      "4동아시아태평양 주요국가의 해양관리시스템 분석 : 해양질서관리와 해양자원관리를 중심으로주종광한국해양경찰학회2017한국해양경찰학회 학술대회Vol.2017 No.02원문보기\n",
      "원문보기\n",
      "5해양수산생명자원의 국내 접근 절차 관련 법제도의 문제점 및 개선방향 : 해양수산생명자원법과 유전자원법의 비교를 중심으로최석문,박수진해양환경안전학회2019해양환경안전학회 학술발표대회 논문집Vol.2019 No.11원문보기\n",
      "원문보기\n",
      "6우리나라 관할해역 내 해양자원 관리현황 및 정책 개선방향박수진(Su Jin Park)한국해양환경·에너지학회2014한국해양환경공학회 학술대회논문집Vol.2014 No.5원문보기\n",
      "원문보기\n",
      "7해양관광 활성화를 위한 해양문화관광자원 활용 방안 해양문화축제를 중심으로하경희(Ha, Kyoung-Hee)한국해양관광학회2018해양관광학연구Vol.11 No.1원문보기'스콜라' 이용 시 소속기관이 구독 중이 아닌 경우, 오후 4시부터 익일 오전 9시까지 원문보기가 가능합니다.Nowadays there are changes of types of tourism for experience tour and marine tour. And marine resources could satisfy the tourists  needs for software and contents of tour in the era of qualitative growth. Therefore this study is performed in order to propose the development directions of marine culture festivals for the sustainable tourism. The development directions of marine culture festivals are as follows. First, the development of the marine culture festivals should be considered the environmental and cultural value such as marine traditional culture and history. Second, the marine culture festivals programs should be focused on the educational contents on the basis of marine culture and environment. Third, regional brand strategy for marine cultural festivals is necessary with unique marine culture resources of region.\n",
      "원문보기\n",
      "8중국 해양자원개발의 해양강국 전략에 대한 함의주현희(Hyun-Hee Ju)한국해양비즈니스학회2014해양비즈니스Vol.- No.27원문보기'스콜라' 이용 시 소속기관이 구독 중이 아닌 경우, 오후 4시부터 익일 오전 9시까지 원문보기가 가능합니다.After “Reforming and Opening”, Chinese marine power strategy has been focusing on exploitation of marine resource. The goal of Chinese marine power strategy is to achieve domestic economic growth by developing ocean-based infrastructure including marine economy and promoting related industries. It is important for China to exploit and develop marine infrastructures which provide various strategic resources including foods, energy, minerals, water, and economic spaces in that the country lacks resources required for economic development.Success of Chinese marine power strategy may depend on how effectively they exploit and manage marine resources, as the share of Chinese marine industry has been consistently escalating. This study reviews the strategy of marine resources development strategy and provides implication and status of marine resources development to the marine power strategy in China.\n",
      "원문보기\n",
      "921세기 신성장동력원 해양유전자원의 개발박수진한국해양수산개발원2009해양국토21Vol.1 No.-원문보기\n",
      "원문보기\n",
      "10국가관할권 이원에서의 해양유전자원 개발에 관한 국제적 논의 동향김선화해양환경안전학회2012해양환경안전학회 학술발표대회 논문집Vol.2012 No.11원문보기\n",
      "원문보기\n",
      "검색하신 키워드 해양자원 (으)로 총 3,818 건의 학위논문이 검색되었습니다\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 중에서 몇 건을 수집하시겠습니까?: 3\n",
      "3 건의 데이터를 수집하기 위해 1 페이지의 게시물을 조회합니다.\n",
      "================================================================================\n",
      "1.번호: 1\n",
      "2.논문제목: 발표논문 / 해양생물자원으로서 해조류 : 생물활성물질의 정제와 분자적 응용\n",
      "3.저자: 홍용기(Yong Ki Hong)\n",
      "4.소속기관: 한국조류학회\n",
      "5.발표년도: 2000\n",
      "6.논문집/자료집: 국제심포지움 일정 및 발표논문집 - 21세기, 해양환경과 해양생물자원의 전망\n",
      "7.논문 URL: http://www.riss.kr/search/detail/DetailView.do?p_mat_type=1a0202e37d52c72d&control_no=508c47ca1906d5ecffe0bdc3ef48d419&keyword=해양자원\n",
      "\n",
      "\n",
      "1.번호: 2\n",
      "2.논문제목: 수거된 해양폐기물 자원화 기술 개발(Ⅰ)\n",
      "3.저자: 길상인(Sang-In Keel),윤진한(Jin-Han Yun),최연석(Yeon-Seok Choi),강창구(Chang-Gu Kang),유정석(Jeong-Seok Yu)\n",
      "4.소속기관: 한국해양환경·에너지학회\n",
      "5.발표년도: 2002\n",
      "6.논문집/자료집: 한국해양환경·에너지학회지\n",
      "7.논문 URL: http://www.riss.kr/search/detail/DetailView.do?p_mat_type=1a0202e37d52c72d&control_no=09ec1e3541fcc118ffe0bdc3ef48d419&keyword=해양자원\n",
      "\n",
      "\n",
      "1.번호: 3\n",
      "2.논문제목: 수거된 해양폐기물 자원화 기술 개발(Ⅱ)\n",
      "3.저자: 길상인(Sang-In Keel),김석준(Seock-Joon Kim),윤진한(Jin-Han Yun),강창구(Chang-Gu Kang),유정석(Jeong-Seok Yu)\n",
      "4.소속기관: 한국해양환경·에너지학회\n",
      "5.발표년도: 2002\n",
      "6.논문집/자료집: 한국해양환경·에너지학회지\n",
      "7.논문 URL: http://www.riss.kr/search/detail/DetailView.do?p_mat_type=1a0202e37d52c72d&control_no=6cbe992540bae11effe0bdc3ef48d419&keyword=해양자원\n",
      "\n",
      "\n",
      "요청하신 작업이 모두 완료되었습니다\n",
      "요청하신 데이터 수집 작업이 정상적으로 완료되었습니다\n"
     ]
    }
   ],
   "source": [
    "# 1. riss.kr 에서 특정 키워드로 논문 / 학술 자료 검색하기\n",
    "\n",
    "#Step 1. 필요한 모듈을 로딩합니다\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time          \n",
    "\n",
    "#Step 2. 사용자에게 검색 관련 정보들을 입력 받습니다.\n",
    "print(\"=\" *100)\n",
    "print(\" 이 크롤러는 RISS 사이트의 논문 및 학술자료 수집용 웹크롤러입니다.\")\n",
    "print(\"=\" *100)\n",
    "query_txt = input('1.수집할 자료의 키워드는 무엇입니까? : ')\n",
    "\n",
    "print(\"=\" *100)\n",
    "# input에 다중 문장을 넣을 때는 ''''''사용\n",
    "print_2_que = '''2. 위 키워드로 아래의 장르 중 어떤 장르의 정보를 수집할까요?\n",
    "\n",
    "1.학위논문\\t2.국내학술논문\\t3.해외학술논문\\t4.학술지\n",
    "5.단행본\\t6.공개강의\\t7.연구보고서\n",
    "\n",
    "위 장르 중 수집할 장르의 번호를 입력하세요 : '''\n",
    "print(\"=\" *100)\n",
    "query_number = int(input(print_2_que))\n",
    "\n",
    "#Step 3. 수집된 데이터를 저장할 파일 이름 입력받기\n",
    "fc_name = \"C:\\\\Users\\\\Windows\\\\Desktop\\\\대학교\\\\4학년 여름방학\\\\Big_AI\\\\크롤링\\\\코드\\\\Chap_3\\\\출력 파일\\\\riss_problem1.csv\"\n",
    "fx_name = \"C:\\\\Users\\\\Windows\\\\Desktop\\\\대학교\\\\4학년 여름방학\\\\Big_AI\\\\크롤링\\\\코드\\\\Chap_3\\\\출력 파일\\\\riss_problem1.xls\"\n",
    "\n",
    "#Step 4. 크롬 드라이버 설정 및 웹 페이지 열기\n",
    "s = Service(\"C:\\\\chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "url = 'https://www.riss.kr/'\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "driver.maximize_window()\n",
    "\n",
    "#Step 5. 자동으로 검색어 입력 후 조회하기\n",
    "element = driver.find_element(By.ID,'query')\n",
    "driver.find_element(By.ID,'query').click( )\n",
    "element.send_keys(query_txt)\n",
    "element.send_keys(\"\\n\")\n",
    "\n",
    "#Step 6.입력 받은 장르 선택하기\n",
    "input_genre = \"\"\n",
    "\n",
    "if query_number == 1:\n",
    "    input_genre = \"학위논문\" #  searchBox pd\n",
    "elif query_number == 2:\n",
    "    input_genre = \"국내학술논문\" # searchBox\n",
    "elif query_number == 3:\n",
    "    input_genre = \"해외학술논문\" # searchBox\n",
    "elif query_number == 4:\n",
    "    input_genre = \"학술지\" # searchBox pd\n",
    "elif query_number == 5:\n",
    "    input_genre = \"단행본\" # searchBox pd\n",
    "elif query_number == 6:\n",
    "    input_genre = \"공개강의\" # searchBox pd\n",
    "elif query_number == 7:\n",
    "    input_genre = \"연구보고서\" # searchBox pd\n",
    "\n",
    "driver.find_element(By.LINK_TEXT,input_genre).click()\n",
    "time.sleep(2)\n",
    "\n",
    "#Step 7.Beautiful Soup 로 본문 내용만 추출하기\n",
    "from bs4 import BeautifulSoup\n",
    "html_1 = driver.page_source\n",
    "soup_1 = BeautifulSoup(html_1, 'html.parser')\n",
    "\n",
    "content_1 = soup_1.find('div','srchResultListW').find_all('li')\n",
    "for i in content_1 :\n",
    "    print(i.get_text().replace(\"\\n\",\"\"))\n",
    "\n",
    "#Step 8. 총 검색 건수를 보여주고 수집할 건수 입력받기\n",
    "import math\n",
    "\n",
    "if (query_number == 2) or (query_number == 3):\n",
    "    total_cnt = soup_1.find('div','searchBox').find('span','num').get_text()\n",
    "else:\n",
    "    total_cnt = soup_1.find('div','searchBox pd').find('span','num').get_text()\n",
    "\n",
    "print('검색하신 키워드 %s (으)로 총 %s 건의 학위논문이 검색되었습니다' %(query_txt,total_cnt))\n",
    "collect_cnt = int(input('이 중에서 몇 건을 수집하시겠습니까?: '))\n",
    "collect_page_cnt = math.ceil(collect_cnt / 10) # 올림\n",
    "print('%s 건의 데이터를 수집하기 위해 %s 페이지의 게시물을 조회합니다.' %(collect_cnt,collect_page_cnt))\n",
    "print('=' *80)\n",
    "\n",
    "#Step 9. 각 항목별로 데이터를 추출하여 리스트에 저장하기\n",
    "no2 = []        #번호 저장\n",
    "title2 = []     #논문제목 저장\n",
    "writer2 = []    #논문저자 저장\n",
    "org2 = []       #소속기관 저장\n",
    "year2 = []       #발표년도 저장\n",
    "refer2 = []  #논문집/자료집 저장\n",
    "URL2 = []         #논문 URL 저장\n",
    "\n",
    "no = 1\n",
    "\n",
    "for a in range(1, collect_page_cnt + 1) :\n",
    "    html_2 = driver.page_source\n",
    "    soup_2 = BeautifulSoup(html_2, 'html.parser')\n",
    "\n",
    "    content_2 = soup_2.find('div','srchResultListW').find_all('li')\n",
    "    \n",
    "    for b in content_2 :    \n",
    "        #1. 논문제목 있을 경우만\n",
    "        try :\n",
    "            title = b.find('div','cont').find('p','title').get_text()\n",
    "        except :\n",
    "            continue\n",
    "        else :\n",
    "            print('1.번호:',no)\n",
    "            no2.append(no)\n",
    "\n",
    "            print('2.논문제목:',title)\n",
    "            title2.append(title)\n",
    "            \n",
    "            writer = b.find('span','writer').get_text()\n",
    "            print('3.저자:',writer)\n",
    "            writer2.append(writer)\n",
    "\n",
    "            org = b.find('span','assigned').get_text()\n",
    "            print('4.소속기관:' , org)\n",
    "            org2.append(org)\n",
    "            \n",
    "            etc_value = b.find('p','etc').select(\"p > span\")\n",
    "            year = etc_value[2].get_text()\n",
    "            print('5.발표년도:' , year)\n",
    "            year2.append(year)\n",
    "            \n",
    "            refer = etc_value[3].get_text()\n",
    "            print('6.논문집/자료집:' , refer)\n",
    "            refer2.append(refer)\n",
    "            \n",
    "            # a tag에 href라는 속성값을 뽑아야 url을 뽑을 수 있다. soup.find('a')['href']\n",
    "            URL = \"http://www.riss.kr\" + b.find('div','cont').find('p','title').find(\"a\")[\"href\"]\n",
    "            print('7.논문 URL:' , URL)\n",
    "            URL2.append(URL)\n",
    "            \n",
    "            no += 1\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            if no > collect_cnt :\n",
    "                break\n",
    "\n",
    "            time.sleep(1)        # 페이지 변경 전 1초 대기 \n",
    "\n",
    "    a += 1 \n",
    "    b = str(a)\n",
    "\n",
    "    try :\n",
    "        driver.find_element(By.LINK_TEXT ,'%s' %b).click() \n",
    "    except :\n",
    "        driver.find_element(By.LINK_TEXT, '다음 페이지로').click()\n",
    "        \n",
    "print(\"요청하신 작업이 모두 완료되었습니다\")\n",
    "\n",
    "# Step 10. 수집된 데이터를 xls와 csv 형태로 저장하기\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['번호']=no2\n",
    "df['제목']=pd.Series(title2) # 결측값이 있으면 df[\"제목\"] = title2가 작동 x\n",
    "df['저자']=pd.Series(writer2)\n",
    "df['소속(발행)기관']=pd.Series(org2)\n",
    "df[\"발표년도\"] = pd.Series(year2)\n",
    "df[\"논문집/자료집\"] = pd.Series(refer2)\n",
    "df[\"논문 URL\"] = pd.Series(URL2)\n",
    "\n",
    "# xls 형태로 저장하기\n",
    "df.to_excel(fx_name,index=False, encoding=\"utf-8\" , engine='openpyxl')\n",
    "\n",
    "# csv 형태로 저장하기\n",
    "df.to_csv(fc_name,index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print('요청하신 데이터 수집 작업이 정상적으로 완료되었습니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "이 크롤러는 네이버 블로그 수집용 웹크롤러입니다.\n",
      "====================================================================================================\n",
      "1. 크롤링할 키워드는 무엇입니까? : 서진수 빅데이터\n",
      "2. 수집할 데이터는 몇 건입니까? : 15\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 2. 네이버에서 특정 키워드로 논문 / 학술 자료 검색하기\n",
    "\n",
    "#Step 1. 필요한 모듈을 로딩합니다\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time          \n",
    "\n",
    "#Step 2. 사용자에게 검색 관련 정보들을 입력 받습니다.\n",
    "print(\"=\" *100)\n",
    "print(\"이 크롤러는 네이버 블로그 수집용 웹크롤러입니다.\")\n",
    "print(\"=\" *100)\n",
    "query_txt = input('1. 크롤링할 키워드는 무엇입니까? : ')\n",
    "collect_cnt = int(input('2. 수집할 데이터는 몇 건입니까? : '))\n",
    "print(\"=\" *100)\n",
    "\n",
    "#Step 3. 수집된 데이터를 저장할 파일 이름 입력받기\n",
    "fc_name = \"C:\\\\Users\\\\Windows\\\\Desktop\\\\대학교\\\\4학년 여름방학\\\\Big_AI\\\\크롤링\\\\코드\\\\Chap_3\\\\출력 파일\\\\naver_problem2.csv\"\n",
    "fx_name = \"C:\\\\Users\\\\Windows\\\\Desktop\\\\대학교\\\\4학년 여름방학\\\\Big_AI\\\\크롤링\\\\코드\\\\Chap_3\\\\출력 파일\\\\naver_problem2.xls\"\n",
    "\n",
    "#Step 4. 크롬 드라이버 설정 및 웹 페이지 열기\n",
    "s = Service(\"C:\\\\chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "url = 'https://www.naver.com/'\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "driver.maximize_window()\n",
    "\n",
    "#Step 5. 자동으로 검색어 입력 후 조회하기\n",
    "element = driver.find_element(By.ID,'query')\n",
    "driver.find_element(By.ID,'query').click( )\n",
    "element.send_keys(query_txt)\n",
    "element.send_keys(\"\\n\")\n",
    "\n",
    "#Step 6.View-Blog 선택하기\n",
    "driver.find_element(By.LINK_TEXT,\"VIEW\").click()\n",
    "time.sleep(1)\n",
    "driver.find_element(By.LINK_TEXT,\"블로그\").click()\n",
    "time.sleep(1)\n",
    "\n",
    "#Step 7.Beautiful Soup 로 본문 내용만 추출하기\n",
    "from bs4 import BeautifulSoup\n",
    "html_1 = driver.page_source\n",
    "soup_1 = BeautifulSoup(html_1, 'html.parser')\n",
    "\n",
    "content_1 = soup_1.find('div','_more_contents_event_base').find_all('li')\n",
    "for i in content_1:\n",
    "    print(i.get_text().replace(\"\\n\",\"\"))\n",
    "\n",
    "#Step 8. 각 항목별로 데이터를 추출하여 리스트에 저장하기\n",
    "no2 = []        #번호 저장\n",
    "title2 = []     #제목 저장\n",
    "mean2 = []    #요약내용 저장\n",
    "day2 = []       #작성일자 저장\n",
    "nickname2 = []       #블로그 닉네임 저장\n",
    "\n",
    "no = 1\n",
    "\n",
    "for i in content_1:   \n",
    "    try :\n",
    "        title = i.find('div','total_area').find('a','api_txt_lines total_tit').get_text()\n",
    "    except :\n",
    "        continue\n",
    "    else :\n",
    "            print('1.번호:',no)\n",
    "            no2.append(no)\n",
    "\n",
    "            print('2.제목:',title)\n",
    "            title2.append(title)\n",
    "            \n",
    "            mean = i.find('div','total_area').find('a','total_dsc').get_text()\n",
    "            print('3.요약내용:',mean)\n",
    "            mean2.append(mean)\n",
    "\n",
    "            day = i.find('div','total_area').find('span', 'sub_time sub_txt').get_text()\n",
    "            print('4.작성일자:' , day)\n",
    "            day2.append(day)\n",
    "            \n",
    "            nickname = i.find('div','total_area').find('a', 'sub_txt sub_name').get_text()\n",
    "            print('5.블로그닉네임:' , nickname)\n",
    "            nickname2.append(nickname)\n",
    "            \n",
    "            no += 1\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            if no > collect_cnt :\n",
    "                break\n",
    "        \n",
    "print(\"요청하신 작업이 모두 완료되었습니다\")\n",
    "\n",
    "# Step 10. 수집된 데이터를 xls와 csv 형태로 저장하기\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['번호']=no2\n",
    "df['제목']=pd.Series(title2)\n",
    "df['요약내용']=pd.Series(mean2)\n",
    "df['작성일자']=pd.Series(day2)\n",
    "df[\"블로그닉네임\"] = pd.Series(nickname2)\n",
    "\n",
    "# xls 형태로 저장하기\n",
    "df.to_excel(fx_name,index=False, encoding=\"utf-8\" , engine='openpyxl')\n",
    "\n",
    "# csv 형태로 저장하기\n",
    "df.to_csv(fc_name,index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print('요청하신 데이터 수집 작업이 정상적으로 완료되었습니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
