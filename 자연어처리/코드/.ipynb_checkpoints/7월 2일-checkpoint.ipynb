{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KrAmsm8WG80"
   },
   "source": [
    "# 자연어 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1656944124109,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "LPAD-FPiUZFi",
    "outputId": "e1d44c5f-8158-46e1-bd9b-52a8da670999"
   },
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\Windows\\\\Desktop\\\\대학교\\\\4학년 여름방학\\\\Big_AI\\\\자연어처리\\\\pytest\\\\data\\\\eng-kor\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1321,
     "status": "ok",
     "timestamp": 1656944125419,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "YiSyGUcgq8wo",
    "outputId": "eeb84825-0f2a-43f0-8659-d4c28f223306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length : \n",
      " 1000\n",
      "data type : \n",
      " <class 'pandas.core.frame.DataFrame'>\n",
      "data shape : \n",
      " (1000, 2)\n",
      "data sample : \n",
      "                   source         target\n",
      "906  Drink to my health.     건강을 위해 건배.\n",
      "75             How cute!          귀엽잖아!\n",
      "836   I sent Tom a text.  난 톰한테 문자 보냈어.\n",
      "802   Champagne, please.       샴폐인 부탁해.\n",
      "110            What fun!          재밌잖아!\n",
      "data.target length : \n",
      " 1000\n",
      "data.target type : \n",
      " <class 'pandas.core.series.Series'>\n",
      "data.target shape : \n",
      " (1000,)\n",
      "data.target sample : \n",
      " 45             저리 가.\n",
      "598         담배 냄새 나.\n",
      "847    톰이 먹는 걸 지켜봤어.\n",
      "168             고마워.\n",
      "190           누가 알아?\n",
      "Name: target, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(path+\"eng-kor_small.txt\", names=[\"source\", \"target\"], sep = \"\\t\", encoding = \"utf-8\")\n",
    "print(\"data length : \\n\", len(data))\n",
    "print(\"data type : \\n\", type(data))\n",
    "print(\"data shape : \\n\", data.shape)\n",
    "print(\"data sample : \\n\", data.sample(5))\n",
    "\n",
    "print(\"data.target length : \\n\", len(data.target))\n",
    "print(\"data.target type : \\n\", type(data.target))\n",
    "print(\"data.target shape : \\n\", data.target.shape)\n",
    "print(\"data.target sample : \\n\", data.target.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBWY-X12WRR9"
   },
   "source": [
    "## 시작부호와 종료부호 부착"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1656944125419,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "b4bQ9D_GV2dr",
    "outputId": "e825a736-bcfb-42ab-aa49-a33dc0a4510b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data.target_input : \n",
      " 0                 \\t가.\\n\n",
      "1                \\t안녕.\\n\n",
      "2                \\t뛰어!\\n\n",
      "3                \\t뛰어.\\n\n",
      "4                \\t누구?\\n\n",
      "             ...        \n",
      "995     \\t노래하는 거 좋아해요?\\n\n",
      "996      \\t노래하는 거 좋아해?\\n\n",
      "997    \\t고양이를 좋아하지 않아?\\n\n",
      "998      \\t꿈은 이루어질 거야.\\n\n",
      "999     \\t모두 그녀를 사랑한다.\\n\n",
      "Name: target, Length: 1000, dtype: object\n",
      "\n",
      "data.target_target : \n",
      " 0                 가.\\n\n",
      "1                안녕.\\n\n",
      "2                뛰어!\\n\n",
      "3                뛰어.\\n\n",
      "4                누구?\\n\n",
      "            ...       \n",
      "995     노래하는 거 좋아해요?\\n\n",
      "996      노래하는 거 좋아해?\\n\n",
      "997    고양이를 좋아하지 않아?\\n\n",
      "998      꿈은 이루어질 거야.\\n\n",
      "999     모두 그녀를 사랑한다.\\n\n",
      "Name: target, Length: 1000, dtype: object\n",
      "data.target_input length : \n",
      " 1000\n",
      "data.target_input type : \n",
      " <class 'pandas.core.series.Series'>\n",
      "data.target_input shape : \n",
      " (1000,)\n",
      "data.target_input sample : \n",
      " 298             \\t다시 확인해.\\n\n",
      "216            \\t톰은 잊어버려.\\n\n",
      "897    \\t너는 내 것을 가질 수 있다.\\n\n",
      "612           \\t톰은 정말 멋져.\\n\n",
      "410             \\t나는 몰라요.\\n\n",
      "Name: target, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-953c8a28e506>:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  data.target_input = data.target.apply(lambda x : '\\t' + x + '\\n')\n",
      "<ipython-input-5-953c8a28e506>:2: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  data.target_target = data.target.apply(lambda x : x + \"\\n\")\n"
     ]
    }
   ],
   "source": [
    "data.target_input = data.target.apply(lambda x : '\\t' + x + '\\n')\n",
    "data.target_target = data.target.apply(lambda x : x + \"\\n\")\n",
    "print(\"\\ndata.target_input : \\n\", data.target_input)\n",
    "print(\"\\ndata.target_target : \\n\", data.target_target)\n",
    "\n",
    "print(\"data.target_input length : \\n\", len(data.target_input))\n",
    "print(\"data.target_input type : \\n\", type(data.target_input))\n",
    "print(\"data.target_input shape : \\n\", data.target_input.shape)\n",
    "print(\"data.target_input sample : \\n\", data.target_input.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oX02BH36XBi_"
   },
   "source": [
    "## 문장의 길이 maxlen 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1656944125420,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "jYS5OikJWxJk",
    "outputId": "3a50518c-3263-4ff6-8d48-aa8247ee396b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source sentence max length :  20\n",
      "target sentence max length :  19\n"
     ]
    }
   ],
   "source": [
    "# source 문장의 최대 길이\n",
    "max_src_len = data.source.apply(lambda x : len(x)).max()\n",
    "print(\"source sentence max length : \", max_src_len)\n",
    "\n",
    "# target_input 문장의 최대 길이\n",
    "max_tar_len = data.target_input.apply(lambda x : len(x)).max() - 2 # \"\\t\"와 \"\\n\"의 길이는 제외\n",
    "print(\"target sentence max length : \", max_tar_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWh4u4lQXsic"
   },
   "source": [
    "## 데이터 Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5646,
     "status": "ok",
     "timestamp": 1656944131061,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "bUorYWMdWxaZ",
    "outputId": "32dcdb64-aeec-4f24-bbec-f43e888a9a0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체에서 64개의 고유한 토큰을 찾았습니다.\n",
      "word_index_source :  {' ': 1, 'e': 2, 'o': 3, '.': 4, 'a': 5, 't': 6, 'i': 7, 's': 8, 'n': 9, 'r': 10, 'l': 11, 'd': 12, 'm': 13, 'h': 14, 'y': 15, 'u': 16, 'T': 17, 'g': 18, 'I': 19, 'c': 20, 'p': 21, 'w': 22, 'k': 23, \"'\": 24, 'v': 25, 'b': 26, 'f': 27, '?': 28, 'S': 29, '!': 30, 'W': 31, 'H': 32, 'C': 33, 'D': 34, 'E': 35, 'K': 36, 'A': 37, 'G': 38, 'Y': 39, 'N': 40, 'x': 41, 'F': 42, 'B': 43, 'L': 44, 'M': 45, 'q': 46, ',': 47, 'P': 48, 'R': 49, 'O': 50, 'z': 51, 'J': 52, 'j': 53, 'Q': 54, '-': 55, '7': 56, ':': 57, '4': 58, '5': 59, 'U': 60, '2': 61, '0': 62, '1': 63, '3': 64}\n"
     ]
    }
   ],
   "source": [
    "# source 언어 Tokenizing\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer_source = Tokenizer(num_words=None, char_level=True, lower = False) # 음절 기반, 문장부호를 제거하진 않음\n",
    "tokenizer_source.fit_on_texts(data.source)\n",
    "word_index_source = tokenizer_source.word_index\n",
    "\n",
    "print(\"전체에서 %s개의 고유한 토큰을 찾았습니다.\" % len(word_index_source))\n",
    "print(\"word_index_source : \", word_index_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1656944131064,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "OlaCcdLpYRW8",
    "outputId": "6cddb64b-849f-4d3c-9402-077830c020b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체에서 558개의 고유한 토큰을 찾았습니다.\n",
      "word_index_target :  {' ': 1, '\\t': 2, '\\n': 3, '.': 4, '어': 5, '이': 6, '톰': 7, '해': 8, '아': 9, '은': 10, '다': 11, '그': 12, '가': 13, '는': 14, '?': 15, '나': 16, '거': 17, '!': 18, '고': 19, '하': 20, '을': 21, '했': 22, '요': 23, '있': 24, '야': 25, '지': 26, '었': 27, '사': 28, '난': 29, '말': 30, '들': 31, '기': 32, '게': 33, '리': 34, '를': 35, '니': 36, '도': 37, '우': 38, '좋': 39, '와': 40, '내': 41, '에': 42, '람': 43, '무': 44, '자': 45, '마': 46, '서': 47, '봐': 48, '한': 49, '계': 50, '안': 51, '네': 52, '시': 53, '속': 54, '너': 55, '수': 56, '모': 57, '만': 58, '짓': 59, '라': 60, '두': 61, '누': 62, '일': 63, '세': 64, '정': 65, '웃': 66, '로': 67, '않': 68, '줘': 69, '았': 70, '제': 71, '렸': 72, '걸': 73, '없': 74, '려': 75, '물': 76, '미': 77, '저': 78, '여': 79, '건': 80, '죽': 81, '으': 82, '워': 83, '조': 84, '주': 85, '린': 86, '신': 87, '것': 88, '의': 89, '져': 90, '심': 91, '좀': 92, '운': 93, '인': 94, '구': 95, '진': 96, '렇': 97, '날': 98, '입': 99, '래': 100, '울': 101, '전': 102, '빨': 103, '피': 104, '히': 105, '할': 106, '보': 107, '러': 108, '파': 109, '소': 110, '양': 111, '대': 112, '발': 113, '질': 114, '용': 115, '부': 116, '생': 117, '졌': 118, '실': 119, '빠': 120, '녀': 121, '공': 122, '테': 123, '났': 124, '새': 125, '쳤': 126, '노': 127, '까': 128, '직': 129, '적': 130, '프': 131, '겼': 132, '왔': 133, '동': 134, '합': 135, '잘': 136, '버': 137, '떠': 138, '불': 139, '먹': 140, '비': 141, '군': 142, '혼': 143, '남': 144, '습': 145, '살': 146, '잠': 147, '작': 148, '알': 149, '절': 150, '장': 151, '되': 152, '늦': 153, '집': 154, '개': 155, '행': 156, '바': 157, '간': 158, '봤': 159, '열': 160, '오': 161, '돼': 162, '잊': 163, '눈': 164, '재': 165, '감': 166, '상': 167, '원': 168, '문': 169, '과': 170, '중': 171, '책': 172, '언': 173, '길': 174, '움': 175, '당': 176, '읽': 177, '경': 178, '참': 179, '스': 180, '쳐': 181, '웠': 182, '렀': 183, '복': 184, '성': 185, '짜': 186, '싫': 187, '름': 188, '화': 189, '늙': 190, '음': 191, '색': 192, '르': 193, '락': 194, '벽': 195, '얼': 196, '앉': 197, '믿': 198, '답': 199, '따': 200, '약': 201, '죄': 202, '번': 203, '타': 204, '천': 205, '선': 206, '키': 207, '랑': 208, '엄': 209, '춤': 210, '였': 211, '숨': 212, '쉬': 213, '차': 214, '넌': 215, '증': 216, '쟁': 217, '필': 218, '폐': 219, '머': 220, '명': 221, '디': 222, '녕': 223, '왜': 224, '연': 225, '슬': 226, '퍼': 227, '완': 228, '영': 229, '잡': 230, '잖': 231, '둘': 232, '둬': 233, '끝': 234, '켜': 235, '끔': 236, '찍': 237, '유': 238, '냄': 239, '분': 240, '침': 241, '돌': 242, '회': 243, '멋': 244, '골': 245, '괜': 246, '찮': 247, '킬': 248, '췄': 249, '싸': 250, '외': 251, '잤': 252, '평': 253, '돈': 254, '식': 255, '탁': 256, '굴': 257, '박': 258, '방': 259, '몰': 260, '독': 261, '배': 262, '느': 263, '싶': 264, '학': 265, '교': 266, '력': 267, '변': 268, '꿈': 269, '호': 270, '갈': 271, '뛰': 272, '쏴': 273, '점': 274, '격': 275, '때': 276, '끄': 277, '꺼': 278, '귀': 279, '못': 280, '크': 281, '뭐': 282, '밌': 283, '금': 284, '멈': 285, '춰': 286, '졸': 287, '쪽': 288, '반': 289, '걔': 290, '억': 291, '더': 292, '냈': 293, '줬': 294, '른': 295, '애': 296, '퇴': 297, '흥': 298, '놓': 299, '포': 300, '임': 301, '담': 302, '투': 303, '순': 304, '걱': 305, '든': 306, '강': 307, '많': 308, '받': 309, '찾': 310, '축': 311, '희': 312, '루': 313, '매': 314, '결': 315, '친': 316, '검': 317, '달': 318, '텐': 319, '후': 320, '샴': 321, '위': 322, '깐': 323, '볼': 324, '짝': 325, '설': 326, '환': 327, '엽': 328, '깊': 329, '빌': 330, '손': 331, '송': 332, '론': 333, '코': 334, '써': 335, '확': 336, '릴': 337, '존': 338, ',': 339, '빴': 340, '예': 341, '패': 342, '끙': 343, '국': 344, '데': 345, '떨': 346, '밍': 347, '옹': 348, '썼': 349, '익': 350, '갔': 351, '낄': 352, '치': 353, '틀': 354, '현': 355, '각': 356, '냥': 357, '추': 358, '땅': 359, '껴': 360, '덜': 361, '깨': 362, '엇': 363, '뭔': 364, '꽃': 365, '향': 366, '폭': 367, '드': 368, '섰': 369, '냐': 370, '단': 371, '큰': 372, '똑': 373, '끓': 374, '랩': 375, '꿔': 376, '초': 377, '꼈': 378, '례': 379, '판': 380, '창': 381, '같': 382, '찼': 383, '즉': 384, '뀌': 385, '카': 386, '샀': 387, '럽': 388, '밖': 389, '술': 390, '꽤': 391, '허': 392, '면': 393, '케': 394, '랐': 395, '런': 396, '꼼': 397, '겠': 398, '럴': 399, '굉': 400, '힘': 401, '흘': 402, '짖': 403, '준': 404, '쥐': 405, '7': 406, '4': 407, '5': 408, '죠': 409, '착': 410, '꾸': 411, '벅': 412, '뜨': 413, '왼': 414, '좌': 415, '얘': 416, '커': 417, '찌': 418, '뜻': 419, '쓰': 420, '옆': 421, '헉': 422, '댔': 423, '덕': 424, '품': 425, '활': 426, '맥': 427, '긴': 428, '충': 429, '최': 430, '란': 431, '별': 432, '종': 433, '찡': 434, '득': 435, '씩': 436, '씨': 437, '앓': 438, '셨': 439, '릎': 440, '꿇': 441, '쇠': 442, '붉': 443, '앞': 444, '령': 445, '숟': 446, '낡': 447, '흐': 448, '망': 449, '승': 450, '낙': 451, '싱': 452, '긋': 453, '빙': 454, '레': 455, '찔': 456, '궁': 457, '렁': 458, '렵': 459, '멀': 460, '쩡': 461, '듣': 462, '쓱': 463, '휘': 464, '병': 465, '겨': 466, '납': 467, '앙': 468, '긍': 469, '총': 470, '쨌': 471, '백': 472, '업': 473, '뭇': 474, '쪼': 475, '편': 476, '녹': 477, '티': 478, '켓': 479, '항': 480, '섭': 481, '처': 482, '랬': 483, '록': 484, '롭': 485, '목': 486, '삐': 487, '윤': 488, '잔': 489, '황': 490, '될': 491, '육': 492, '멍': 493, '청': 494, '꽉': 495, '표': 496, '겐': 497, '몇': 498, '십': 499, '셔': 500, '산': 501, '갰': 502, '갑': 503, '잃': 504, '농': 505, '체': 506, '관': 507, '뭘': 508, '딨': 509, '획': 510, '료': 511, '줄': 512, '낼': 513, '톱': 514, '곱': 515, '걷': 516, '흔': 517, '훔': 518, '널': 519, '취': 520, '철': 521, '극': 522, '낯': 523, '뿐': 524, '삶': 525, '짧': 526, '께': 527, '악': 528, '민': 529, '응': 530, '메': 531, '토': 532, '슨': 533, '욕': 534, '끼': 535, '앵': 536, '2': 537, '0': 538, '1': 539, '3': 540, '년': 541, '태': 542, '곧': 543, '닮': 544, '림': 545, '탓': 546, '옮': 547, '쉽': 548, '등': 549, '밤': 550, '끈': 551, '묶': 552, '월': 553, '깜': 554, '놀': 555, '쯤': 556, '올': 557, '컵': 558}\n"
     ]
    }
   ],
   "source": [
    "# target 언어 Tokenizing\n",
    "tokenizer_target = Tokenizer(num_words=None, char_level=True, lower = False) # 음절 기반, 문장부호를 제거하진 않음\n",
    "tokenizer_target.fit_on_texts(data.target_input)\n",
    "word_index_target = tokenizer_target.word_index\n",
    "\n",
    "print(\"전체에서 %s개의 고유한 토큰을 찾았습니다.\" % len(word_index_target))\n",
    "print(\"word_index_target : \", word_index_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyqDHrHJZDVQ"
   },
   "source": [
    "## 데이터 Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 287,
     "status": "ok",
     "timestamp": 1656944131065,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "VqQ_dG_6ZBGz",
    "outputId": "7bbac4cc-0839-4a80-da62-0bf174ed5382"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of encoder_input sequencing : \n",
      "Go. [38, 3, 4]\n",
      "Hi. [32, 7, 4]\n",
      "Run! [49, 16, 9, 30]\n",
      "Run. [49, 16, 9, 4]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = tokenizer_source.texts_to_sequences(data.source)\n",
    "\n",
    "print(\"Result of encoder_input sequencing : \")\n",
    "print(data.source[0], encoder_input[0])\n",
    "print(data.source[1], encoder_input[1])\n",
    "print(data.source[2], encoder_input[2])\n",
    "print(data.source[3], encoder_input[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1656944131067,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "POmP79SlZBdK",
    "outputId": "1f874be6-7c0c-4d4b-ef1e-307ebcf22cde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of decoder_input sequencing : \n",
      "\t가.\n",
      " [2, 13, 4, 3]\n",
      "\t안녕.\n",
      " [2, 51, 223, 4, 3]\n",
      "\t뛰어!\n",
      " [2, 272, 5, 18, 3]\n",
      "Result of decoder_target sequencing : \n",
      "가.\n",
      " [13, 4, 3]\n",
      "안녕.\n",
      " [51, 223, 4, 3]\n",
      "뛰어!\n",
      " [272, 5, 18, 3]\n",
      "Result of decoder_target sequencing : \n",
      "가.\n",
      " [13, 4, 3]\n",
      "안녕.\n",
      " [51, 223, 4, 3]\n",
      "뛰어!\n",
      " [272, 5, 18, 3]\n"
     ]
    }
   ],
   "source": [
    "decoder_input = tokenizer_target.texts_to_sequences(data.target_input)\n",
    "decoder_target = tokenizer_target.texts_to_sequences(data.target_target)\n",
    "\n",
    "print('Result of decoder_input sequencing : ')\n",
    "print(data.target_input[0], decoder_input[0])\n",
    "print(data.target_input[1], decoder_input[1])\n",
    "print(data.target_input[2], decoder_input[2])\n",
    "\n",
    "print('Result of decoder_target sequencing : ')\n",
    "print(data.target_target[0], decoder_target[0])\n",
    "print(data.target_target[1], decoder_target[1])\n",
    "print(data.target_target[2], decoder_target[2])\n",
    "\n",
    "print('Result of decoder_target sequencing : ')\n",
    "print(data.target_target[0], decoder_target[0])\n",
    "print(data.target_target[1], decoder_target[1])\n",
    "print(data.target_target[2], decoder_target[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeoBNAjadSJS"
   },
   "source": [
    "## 데이터 Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1656944131549,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "fgAHjaxaZBoa",
    "outputId": "5277411b-4a3e-432b-adc0-a65f3ccce841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding result sample : \n",
      "\t가.\n",
      " [ 2 13  4  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "decoder_input length :  1000\n",
      "decoder_input type :  <class 'numpy.ndarray'>\n",
      "decoder_input shape :  (1000, 19)\n"
     ]
    }
   ],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "encoder_input = pad_sequences(encoder_input, maxlen = max_src_len, padding = \"post\")\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_tar_len, padding = \"post\")\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_tar_len, padding = \"post\")\n",
    "\n",
    "print(\"Padding result sample : \")\n",
    "print(data.target_input[0], decoder_input[0])\n",
    "\n",
    "print(\"decoder_input length : \", len(decoder_input))\n",
    "print(\"decoder_input type : \", type(decoder_input))\n",
    "print(\"decoder_input shape : \", decoder_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1656944131550,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "FTldK2xkd1dE",
    "outputId": "c322bba6-e021-46eb-9bf0-f757e5474a9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of One-Hot Encodded decoder_input sequencing : \n",
      "(1000, 19, 559)\n",
      "\t가.\n",
      " [[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "encoder_input = to_categorical(encoder_input, num_classes = len(word_index_source) + 1)\n",
    "decoder_input = to_categorical(decoder_input, num_classes = len(word_index_target) + 1)\n",
    "decoder_target = to_categorical(decoder_target, num_classes = len(word_index_target) + 1)\n",
    "\n",
    "print(\"Result of One-Hot Encodded decoder_input sequencing : \")\n",
    "print(decoder_input.shape)\n",
    "print(data.target_input[0], decoder_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Yx1JKvYhKhTv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-0\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "0-1\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "0-2\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "0-3\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "0-18\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"0-0\\n\", decoder_input[0][0])\n",
    "print(\"0-1\\n\", decoder_input[0][1])\n",
    "print(\"0-2\\n\", decoder_input[0][2])\n",
    "print(\"0-3\\n\", decoder_input[0][3])\n",
    "print(\"0-18\\n\", decoder_input[0][18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1656944131557,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "hWaAyVrFKh2m",
    "outputId": "30b526a5-0ca3-42a1-9cf0-41fabd0fdea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input length :  1000\n",
      "decoder_input type :  <class 'numpy.ndarray'>\n",
      "decoder_input shape :  (1000, 19, 559)\n"
     ]
    }
   ],
   "source": [
    "print(\"decoder_input length : \", len(decoder_input))\n",
    "print(\"decoder_input type : \", type(decoder_input))\n",
    "print(\"decoder_input shape : \", decoder_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4VaheQrJnCg"
   },
   "source": [
    "## 훈련용 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "AygZLqrbyUMf"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input,LSTM, Dense\n",
    "\n",
    "encoder_inputs = Input(shape = (None, len(word_index_source) + 1))\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "_, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XMygloYMYO_"
   },
   "source": [
    "## 훈련용 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2ffQ-i04MEj6"
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, len(word_index_target) + 1))\n",
    "decoder_lstm = LSTM(256, return_sequences = True, return_state = True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(word_index_target) + 1, activation = \"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DEjosH2O34h"
   },
   "source": [
    "## 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 205839,
     "status": "ok",
     "timestamp": 1656944338063,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "JU7wXA57O4H_",
    "outputId": "3175bf6f-81ba-4c04-d8fe-93a36836443a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "13/13 [==============================] - 10s 423ms/step - loss: 3.4260 - val_loss: 3.1424\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 4s 297ms/step - loss: 2.2710 - val_loss: 3.2148\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 4s 318ms/step - loss: 2.2310 - val_loss: 3.2592\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 4s 318ms/step - loss: 2.1923 - val_loss: 2.9773\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 4s 289ms/step - loss: 2.1488 - val_loss: 2.9591\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 4s 306ms/step - loss: 2.1103 - val_loss: 2.9520\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 5s 369ms/step - loss: 2.0740 - val_loss: 2.9217\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 4s 322ms/step - loss: 2.0451 - val_loss: 3.1573\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 4s 298ms/step - loss: 2.0319 - val_loss: 2.9319\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 4s 323ms/step - loss: 1.9797 - val_loss: 2.9903\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 4s 311ms/step - loss: 1.9121 - val_loss: 3.1626\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 4s 318ms/step - loss: 1.8586 - val_loss: 2.9092\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 4s 289ms/step - loss: 1.8427 - val_loss: 2.7097\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 4s 296ms/step - loss: 1.7617 - val_loss: 2.8247\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 4s 314ms/step - loss: 1.6833 - val_loss: 2.6371\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 4s 304ms/step - loss: 1.7307 - val_loss: 2.7142\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 4s 301ms/step - loss: 1.6059 - val_loss: 2.5840\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 4s 291ms/step - loss: 1.5828 - val_loss: 2.5463\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 4s 327ms/step - loss: 1.5470 - val_loss: 2.5217\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 4s 317ms/step - loss: 1.5052 - val_loss: 2.5986\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 4s 312ms/step - loss: 1.5065 - val_loss: 2.4688\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 4s 298ms/step - loss: 1.4321 - val_loss: 2.5577\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 4s 334ms/step - loss: 1.4226 - val_loss: 2.4752\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 5s 351ms/step - loss: 1.3829 - val_loss: 2.4415\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 4s 337ms/step - loss: 1.3627 - val_loss: 2.5036\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 5s 393ms/step - loss: 1.3273 - val_loss: 2.4066\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 4s 340ms/step - loss: 1.3154 - val_loss: 2.3685\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 4s 303ms/step - loss: 1.2822 - val_loss: 2.4518\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 4s 319ms/step - loss: 1.2576 - val_loss: 2.4546\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 4s 302ms/step - loss: 1.2363 - val_loss: 2.3493\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 5s 383ms/step - loss: 1.2074 - val_loss: 2.3558\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 4s 321ms/step - loss: 1.1887 - val_loss: 2.3453\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 4s 344ms/step - loss: 1.1647 - val_loss: 2.3855\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 5s 339ms/step - loss: 1.1488 - val_loss: 2.3327\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 4s 326ms/step - loss: 1.1194 - val_loss: 2.4063\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 4s 298ms/step - loss: 1.1027 - val_loss: 2.3097\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 4s 289ms/step - loss: 1.0821 - val_loss: 2.3450\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 4s 305ms/step - loss: 1.0627 - val_loss: 2.3428\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 4s 301ms/step - loss: 1.0432 - val_loss: 2.3565\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 4s 295ms/step - loss: 1.0226 - val_loss: 2.3397\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 4s 292ms/step - loss: 1.0008 - val_loss: 2.3828\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 4s 312ms/step - loss: 0.9824 - val_loss: 2.4262\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 4s 302ms/step - loss: 0.9713 - val_loss: 2.3194\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 4s 297ms/step - loss: 0.9520 - val_loss: 2.3550\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 4s 282ms/step - loss: 0.9235 - val_loss: 2.3528\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 4s 314ms/step - loss: 0.9167 - val_loss: 2.3498\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 4s 303ms/step - loss: 0.8929 - val_loss: 2.2967\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 4s 305ms/step - loss: 0.8697 - val_loss: 2.3515\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 4s 280ms/step - loss: 0.8637 - val_loss: 2.3499\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 4s 301ms/step - loss: 0.8430 - val_loss: 2.3823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14518ba7c10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer = \"rmsprop\", loss = \"categorical_crossentropy\")\n",
    "model.fit(x=[encoder_input, decoder_input], y = decoder_target, batch_size = 64, epochs = 50, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mqi78uVzPhtE"
   },
   "source": [
    "## 예측용 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "I20wLoPvPbNO"
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKO92WgTPuh-"
   },
   "source": [
    "## 예측용 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WrjKfDncPrQJ"
   },
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape = (256,)) # Encoder vector를 받기 위함\n",
    "decoder_state_input_c = Input(shape = (256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(inputs = [decoder_inputs] + decoder_states_inputs, outputs = [decoder_outputs] + decoder_states) # input으론 target_input과 현재의 문장상태, output으론 예측 결과와 은닉상태와 셀상태가 나오게 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b18i1n9tS-V4"
   },
   "source": [
    "## Word Dictionary 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1656944338066,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "TTFVMwd4Sakg",
    "outputId": "41b13b03-70a6-4d46-8f5e-265f1e86f40f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ' ', 2: '\\t', 3: '\\n', 4: '.', 5: '어', 6: '이', 7: '톰', 8: '해', 9: '아', 10: '은', 11: '다', 12: '그', 13: '가', 14: '는', 15: '?', 16: '나', 17: '거', 18: '!', 19: '고', 20: '하', 21: '을', 22: '했', 23: '요', 24: '있', 25: '야', 26: '지', 27: '었', 28: '사', 29: '난', 30: '말', 31: '들', 32: '기', 33: '게', 34: '리', 35: '를', 36: '니', 37: '도', 38: '우', 39: '좋', 40: '와', 41: '내', 42: '에', 43: '람', 44: '무', 45: '자', 46: '마', 47: '서', 48: '봐', 49: '한', 50: '계', 51: '안', 52: '네', 53: '시', 54: '속', 55: '너', 56: '수', 57: '모', 58: '만', 59: '짓', 60: '라', 61: '두', 62: '누', 63: '일', 64: '세', 65: '정', 66: '웃', 67: '로', 68: '않', 69: '줘', 70: '았', 71: '제', 72: '렸', 73: '걸', 74: '없', 75: '려', 76: '물', 77: '미', 78: '저', 79: '여', 80: '건', 81: '죽', 82: '으', 83: '워', 84: '조', 85: '주', 86: '린', 87: '신', 88: '것', 89: '의', 90: '져', 91: '심', 92: '좀', 93: '운', 94: '인', 95: '구', 96: '진', 97: '렇', 98: '날', 99: '입', 100: '래', 101: '울', 102: '전', 103: '빨', 104: '피', 105: '히', 106: '할', 107: '보', 108: '러', 109: '파', 110: '소', 111: '양', 112: '대', 113: '발', 114: '질', 115: '용', 116: '부', 117: '생', 118: '졌', 119: '실', 120: '빠', 121: '녀', 122: '공', 123: '테', 124: '났', 125: '새', 126: '쳤', 127: '노', 128: '까', 129: '직', 130: '적', 131: '프', 132: '겼', 133: '왔', 134: '동', 135: '합', 136: '잘', 137: '버', 138: '떠', 139: '불', 140: '먹', 141: '비', 142: '군', 143: '혼', 144: '남', 145: '습', 146: '살', 147: '잠', 148: '작', 149: '알', 150: '절', 151: '장', 152: '되', 153: '늦', 154: '집', 155: '개', 156: '행', 157: '바', 158: '간', 159: '봤', 160: '열', 161: '오', 162: '돼', 163: '잊', 164: '눈', 165: '재', 166: '감', 167: '상', 168: '원', 169: '문', 170: '과', 171: '중', 172: '책', 173: '언', 174: '길', 175: '움', 176: '당', 177: '읽', 178: '경', 179: '참', 180: '스', 181: '쳐', 182: '웠', 183: '렀', 184: '복', 185: '성', 186: '짜', 187: '싫', 188: '름', 189: '화', 190: '늙', 191: '음', 192: '색', 193: '르', 194: '락', 195: '벽', 196: '얼', 197: '앉', 198: '믿', 199: '답', 200: '따', 201: '약', 202: '죄', 203: '번', 204: '타', 205: '천', 206: '선', 207: '키', 208: '랑', 209: '엄', 210: '춤', 211: '였', 212: '숨', 213: '쉬', 214: '차', 215: '넌', 216: '증', 217: '쟁', 218: '필', 219: '폐', 220: '머', 221: '명', 222: '디', 223: '녕', 224: '왜', 225: '연', 226: '슬', 227: '퍼', 228: '완', 229: '영', 230: '잡', 231: '잖', 232: '둘', 233: '둬', 234: '끝', 235: '켜', 236: '끔', 237: '찍', 238: '유', 239: '냄', 240: '분', 241: '침', 242: '돌', 243: '회', 244: '멋', 245: '골', 246: '괜', 247: '찮', 248: '킬', 249: '췄', 250: '싸', 251: '외', 252: '잤', 253: '평', 254: '돈', 255: '식', 256: '탁', 257: '굴', 258: '박', 259: '방', 260: '몰', 261: '독', 262: '배', 263: '느', 264: '싶', 265: '학', 266: '교', 267: '력', 268: '변', 269: '꿈', 270: '호', 271: '갈', 272: '뛰', 273: '쏴', 274: '점', 275: '격', 276: '때', 277: '끄', 278: '꺼', 279: '귀', 280: '못', 281: '크', 282: '뭐', 283: '밌', 284: '금', 285: '멈', 286: '춰', 287: '졸', 288: '쪽', 289: '반', 290: '걔', 291: '억', 292: '더', 293: '냈', 294: '줬', 295: '른', 296: '애', 297: '퇴', 298: '흥', 299: '놓', 300: '포', 301: '임', 302: '담', 303: '투', 304: '순', 305: '걱', 306: '든', 307: '강', 308: '많', 309: '받', 310: '찾', 311: '축', 312: '희', 313: '루', 314: '매', 315: '결', 316: '친', 317: '검', 318: '달', 319: '텐', 320: '후', 321: '샴', 322: '위', 323: '깐', 324: '볼', 325: '짝', 326: '설', 327: '환', 328: '엽', 329: '깊', 330: '빌', 331: '손', 332: '송', 333: '론', 334: '코', 335: '써', 336: '확', 337: '릴', 338: '존', 339: ',', 340: '빴', 341: '예', 342: '패', 343: '끙', 344: '국', 345: '데', 346: '떨', 347: '밍', 348: '옹', 349: '썼', 350: '익', 351: '갔', 352: '낄', 353: '치', 354: '틀', 355: '현', 356: '각', 357: '냥', 358: '추', 359: '땅', 360: '껴', 361: '덜', 362: '깨', 363: '엇', 364: '뭔', 365: '꽃', 366: '향', 367: '폭', 368: '드', 369: '섰', 370: '냐', 371: '단', 372: '큰', 373: '똑', 374: '끓', 375: '랩', 376: '꿔', 377: '초', 378: '꼈', 379: '례', 380: '판', 381: '창', 382: '같', 383: '찼', 384: '즉', 385: '뀌', 386: '카', 387: '샀', 388: '럽', 389: '밖', 390: '술', 391: '꽤', 392: '허', 393: '면', 394: '케', 395: '랐', 396: '런', 397: '꼼', 398: '겠', 399: '럴', 400: '굉', 401: '힘', 402: '흘', 403: '짖', 404: '준', 405: '쥐', 406: '7', 407: '4', 408: '5', 409: '죠', 410: '착', 411: '꾸', 412: '벅', 413: '뜨', 414: '왼', 415: '좌', 416: '얘', 417: '커', 418: '찌', 419: '뜻', 420: '쓰', 421: '옆', 422: '헉', 423: '댔', 424: '덕', 425: '품', 426: '활', 427: '맥', 428: '긴', 429: '충', 430: '최', 431: '란', 432: '별', 433: '종', 434: '찡', 435: '득', 436: '씩', 437: '씨', 438: '앓', 439: '셨', 440: '릎', 441: '꿇', 442: '쇠', 443: '붉', 444: '앞', 445: '령', 446: '숟', 447: '낡', 448: '흐', 449: '망', 450: '승', 451: '낙', 452: '싱', 453: '긋', 454: '빙', 455: '레', 456: '찔', 457: '궁', 458: '렁', 459: '렵', 460: '멀', 461: '쩡', 462: '듣', 463: '쓱', 464: '휘', 465: '병', 466: '겨', 467: '납', 468: '앙', 469: '긍', 470: '총', 471: '쨌', 472: '백', 473: '업', 474: '뭇', 475: '쪼', 476: '편', 477: '녹', 478: '티', 479: '켓', 480: '항', 481: '섭', 482: '처', 483: '랬', 484: '록', 485: '롭', 486: '목', 487: '삐', 488: '윤', 489: '잔', 490: '황', 491: '될', 492: '육', 493: '멍', 494: '청', 495: '꽉', 496: '표', 497: '겐', 498: '몇', 499: '십', 500: '셔', 501: '산', 502: '갰', 503: '갑', 504: '잃', 505: '농', 506: '체', 507: '관', 508: '뭘', 509: '딨', 510: '획', 511: '료', 512: '줄', 513: '낼', 514: '톱', 515: '곱', 516: '걷', 517: '흔', 518: '훔', 519: '널', 520: '취', 521: '철', 522: '극', 523: '낯', 524: '뿐', 525: '삶', 526: '짧', 527: '께', 528: '악', 529: '민', 530: '응', 531: '메', 532: '토', 533: '슨', 534: '욕', 535: '끼', 536: '앵', 537: '2', 538: '0', 539: '1', 540: '3', 541: '년', 542: '태', 543: '곧', 544: '닮', 545: '림', 546: '탓', 547: '옮', 548: '쉽', 549: '등', 550: '밤', 551: '끈', 552: '묶', 553: '월', 554: '깜', 555: '놀', 556: '쯤', 557: '올', 558: '컵'}\n"
     ]
    }
   ],
   "source": [
    "index_to_src = dict((i, char) for char, i in word_index_source.items())\n",
    "index_to_tar = dict((i, char) for char, i in word_index_target.items())\n",
    "print(index_to_tar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gORgCa3lS5me"
   },
   "source": [
    "## 예측함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ySxza7niS4_f"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "  states_value = encoder_model.predict(input_seq) # 초기 문장상태벡터를 얻음\n",
    "  target_seq = np.zeros((1, 1, len(word_index_target) + 1)) # 디코더 초기화\n",
    "  target_seq[0, 0, word_index_target[\"\\t\"]] = 1. # 디코더의 첫 시작은 \"\\t\"이므로 원-핫 인코딩으로 기록\n",
    "  stop_condition = False\n",
    "  decoded_sentence = \"\"\n",
    "\n",
    "  while not stop_condition:\n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value) # target_input과 문장상태벡터 입력\n",
    "    sampled_token_index = np.argmax(output_tokens)\n",
    "\n",
    "    if (sampled_token_index == 0):\n",
    "      sampled_token_index = 1 # 1은 공백 음절\n",
    "\n",
    "    sampled_char = index_to_tar[sampled_token_index]\n",
    "    decoded_sentence += sampled_char\n",
    "\n",
    "    if (sampled_char == \"\\n\" or len(decoded_sentence) > max_tar_len):\n",
    "      stop_condition = True\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, len(word_index_target) + 1))\n",
    "    target_seq[0, 0, sampled_token_index] = 1.\n",
    "    states_value = [h, c]\n",
    "\n",
    "  return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1602,
     "status": "ok",
     "timestamp": 1656944339651,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "Zn1VpXWJV1Qe",
    "outputId": "f06f1e97-c5dc-4143-cbf6-ec1a6e0f1ade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 924ms/step\n",
      "1/1 [==============================] - 1s 854ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "-----------------------------------\n",
      "입력 문장 :  Hi.\n",
      "정답 문장 :  안녕.\n",
      "번역기가 번역한 문장 :  안녕.\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "-----------------------------------\n",
      "입력 문장 :  Run!\n",
      "정답 문장 :  뛰어!\n",
      "번역기가 번역한 문장 :  어.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "-----------------------------------\n",
      "입력 문장 :  Run.\n",
      "정답 문장 :  뛰어.\n",
      "번역기가 번역한 문장 :  어.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for seq_index in [1, 2, 3]:\n",
    "  input_seq = encoder_input[seq_index:seq_index+1] # 3차원 배열에선 [n:n+1] 형태로 출력해야 3차원이 유지되면서 n번째가 출력된다.\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(35 * \"-\")\n",
    "  print(\"입력 문장 : \", data.source[seq_index])\n",
    "  print(\"정답 문장 : \", data.target[seq_index][:len(data.target[seq_index])])\n",
    "  print(\"번역기가 번역한 문장 : \", decoded_sentence[:len(decoded_sentence) - 1]) # \"\\n\"은 빼고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1656944340080,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "l56GzYoHXxjy",
    "outputId": "41ac949f-4059-4d31-d81a-9ed2f9c44914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "이렇게 귀엽니다니!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_seq_1 = tokenizer_source.texts_to_sequences([list(\"wait\")])\n",
    "input_seq_1 = pad_sequences(input_seq_1, maxlen = max_src_len, padding = \"post\")\n",
    "input_seq_1 = to_categorical(input_seq_1, num_classes = len(word_index_source) + 1)\n",
    "\n",
    "decoded_sentence = decode_sequence(input_seq_1)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8PqJIkWY5Zx"
   },
   "source": [
    "# 자연어생성 연습문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5404,
     "status": "ok",
     "timestamp": 1657068159523,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "2rV0Q1UEY4EP",
    "outputId": "cbd72c41-367f-4256-b8a9-0dcc445aecef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length : \n",
      " 30000\n",
      "data type : \n",
      " <class 'pandas.core.frame.DataFrame'>\n",
      "data shape : \n",
      " (30000, 2)\n",
      "data sample : \n",
      "                                source  \\\n",
      "14390          뭐 그렇다한들 이 잘생김이 어딜 가겠어?   \n",
      "3196               나는 교육 제도가 바뀌기를 바래.   \n",
      "19736             빠른 시일내에 정리하여 보내겠어요.   \n",
      "13028           검사를 위하여 나는 부산에 가야 해요.   \n",
      "19945  죄송한 말씀이지만 다른 숙소를 알아보시는 게 좋겠어요.   \n",
      "\n",
      "                                                  target  \n",
      "14390  Even so, where would this handsomeness go anyw...  \n",
      "3196          I want the education system to be changed.  \n",
      "19736        We will send it to you as soon as possible.  \n",
      "13028             I have to go to Busan for examination.  \n",
      "19945  I am very sorry, but you should look for a dif...  \n",
      "data.target length : \n",
      " 30000\n",
      "data.target type : \n",
      " <class 'pandas.core.series.Series'>\n",
      "data.target shape : \n",
      " (30000,)\n",
      "data.target sample : \n",
      " 24227    It has been a long time since I played the gui...\n",
      "28474         They sell Saewookkang for seagulls on board.\n",
      "2633                    I was just joking don't get angry.\n",
      "7064     The family can feel a stronger bond making a C...\n",
      "5985     My thought that I had at that time hasn't chan...\n",
      "Name: target, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Windows\\\\Desktop\\\\대학교\\\\4학년 여름방학\\\\Big_AI\\\\자연어처리\\\\pytest\\\\data\\\\kor-eng\\\\kor-eng.txt\", names=[\"source\", \"target\"], sep = \"\\t\", encoding = \"utf-8\")\n",
    "data = data.iloc[:30000]\n",
    "print(\"data length : \\n\", len(data))\n",
    "print(\"data type : \\n\", type(data))\n",
    "print(\"data shape : \\n\", data.shape)\n",
    "print(\"data sample : \\n\", data.sample(5))\n",
    "\n",
    "print(\"data.target length : \\n\", len(data.target))\n",
    "print(\"data.target type : \\n\", type(data.target))\n",
    "print(\"data.target shape : \\n\", data.target.shape)\n",
    "print(\"data.target sample : \\n\", data.target.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1657068161248,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "gzAfLquyZ9RD",
    "outputId": "a99b9743-5598-4c90-fea5-dc5d3a85c62b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data.target_input : \n",
      " 0         \\tI go to the attic every evening to meet Bat.\\n\n",
      "1          \\tSir, I don't understand this sentence here.\\n\n",
      "2        \\tTime flies when you start using the computer.\\n\n",
      "3           \\tI'm going back to Korea today at midnight.\\n\n",
      "4               \\tI go to bathroom as soon as I wake up.\\n\n",
      "                               ...                        \n",
      "29995    \\tPlease cancel your order after you're logged...\n",
      "29996    \\tPlease check if the attached photo is the co...\n",
      "29997      \\tPlease check the attached transfer receipt.\\n\n",
      "29998    \\tPlease check the meeting's details, and cont...\n",
      "29999    \\tPlease check the sticker attached to the pro...\n",
      "Name: target, Length: 30000, dtype: object\n",
      "\n",
      "data.target_target : \n",
      " 0           I go to the attic every evening to meet Bat.\\n\n",
      "1            Sir, I don't understand this sentence here.\\n\n",
      "2          Time flies when you start using the computer.\\n\n",
      "3             I'm going back to Korea today at midnight.\\n\n",
      "4                 I go to bathroom as soon as I wake up.\\n\n",
      "                               ...                        \n",
      "29995    Please cancel your order after you're logged i...\n",
      "29996    Please check if the attached photo is the corr...\n",
      "29997        Please check the attached transfer receipt.\\n\n",
      "29998    Please check the meeting's details, and contac...\n",
      "29999    Please check the sticker attached to the produ...\n",
      "Name: target, Length: 30000, dtype: object\n",
      "data.target_input length : \n",
      " 30000\n",
      "data.target_input type : \n",
      " <class 'pandas.core.series.Series'>\n",
      "data.target_input shape : \n",
      " (30000,)\n",
      "data.target_input sample : \n",
      " 14677    \\tWe think that Iphone needs to innovate at th...\n",
      "5873          \\tI cannot take the elevator with couples.\\n\n",
      "16147       \\tI think you've sent a wrong message to us.\\n\n",
      "14130           \\tThe sight of him gives me goose bumps.\\n\n",
      "24294    \\tThe lost refugees are taken hostage of enemi...\n",
      "Name: target, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-953c8a28e506>:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  data.target_input = data.target.apply(lambda x : '\\t' + x + '\\n')\n",
      "<ipython-input-43-953c8a28e506>:2: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  data.target_target = data.target.apply(lambda x : x + \"\\n\")\n"
     ]
    }
   ],
   "source": [
    "data.target_input = data.target.apply(lambda x : '\\t' + x + '\\n')\n",
    "data.target_target = data.target.apply(lambda x : x + \"\\n\")\n",
    "print(\"\\ndata.target_input : \\n\", data.target_input)\n",
    "print(\"\\ndata.target_target : \\n\", data.target_target)\n",
    "\n",
    "print(\"data.target_input length : \\n\", len(data.target_input))\n",
    "print(\"data.target_input type : \\n\", type(data.target_input))\n",
    "print(\"data.target_input shape : \\n\", data.target_input.shape)\n",
    "print(\"data.target_input sample : \\n\", data.target_input.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1657068164068,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "7CvRFohbmWfU",
    "outputId": "cb79ebb8-ec69-44bb-8456-aabea9e390c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source sentence max length :  47\n",
      "target sentence max length :  62\n"
     ]
    }
   ],
   "source": [
    "# source 문장의 최대 길이\n",
    "max_src_len = data.source.apply(lambda x : len(x)).max()\n",
    "print(\"source sentence max length : \", max_src_len)\n",
    "\n",
    "# target_input 문장의 최대 길이\n",
    "max_tar_len = data.target_input.apply(lambda x : len(x)).max() - 2 # \"\\t\"와 \"\\n\"의 길이는 제외\n",
    "print(\"target sentence max length : \", max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4030,
     "status": "ok",
     "timestamp": 1657068168093,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "Dt2xTijEmak2",
    "outputId": "eba6119a-edea-471f-dd37-b878472b9cc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체에서 1411개의 고유한 토큰을 찾았습니다.\n",
      "word_index_source :  {' ': 1, '.': 2, '요': 3, '이': 4, '어': 5, '는': 6, '에': 7, '해': 8, '을': 9, '가': 10, '하': 11, '고': 12, '지': 13, '나': 14, '아': 15, '있': 16, '은': 17, '서': 18, '그': 19, '를': 20, '한': 21, '리': 22, '의': 23, '다': 24, '로': 25, '기': 26, '도': 27, '사': 28, '게': 29, '일': 30, '시': 31, '수': 32, '들': 33, '면': 34, '것': 35, '?': 36, '신': 37, '야': 38, '내': 39, '주': 40, '으': 41, '만': 42, '인': 43, '보': 44, '제': 45, '거': 46, '자': 47, '당': 48, '우': 49, '라': 50, '겠': 51, '같': 52, '려': 53, '드': 54, '대': 55, '생': 56, '할': 57, '여': 58, '부': 59, '마': 60, '되': 61, '래': 62, '정': 63, '오': 64, '스': 65, '구': 66, '너': 67, '니': 68, '국': 69, '바': 70, '좋': 71, '과': 72, '까': 73, ',': 74, '와': 75, '전': 76, '않': 77, '장': 78, '각': 79, '문': 80, '음': 81, '말': 82, '안': 83, '저': 84, '무': 85, '금': 86, '없': 87, '러': 88, '싶': 89, '상': 90, '적': 91, '데': 92, '모': 93, '많': 94, '간': 95, '행': 96, '었': 97, '화': 98, '예': 99, '용': 100, '영': 101, '중': 102, '비': 103, '진': 104, '번': 105, '소': 106, '때': 107, '친': 108, '공': 109, '람': 110, '동': 111, '미': 112, '트': 113, '알': 114, '더': 115, '세': 116, '건': 117, '운': 118, '원': 119, '잘': 120, '학': 121, '못': 122, '연': 123, '위': 124, '유': 125, '식': 126, '실': 127, '늘': 128, '회': 129, '분': 130, '길': 131, '업': 132, '물': 133, '먹': 134, '선': 135, '경': 136, '계': 137, '치': 138, '르': 139, '방': 140, '관': 141, '터': 142, '했': 143, '교': 144, '매': 145, '히': 146, '네': 147, '품': 148, '조': 149, '심': 150, '집': 151, '성': 152, '결': 153, '능': 154, '난': 155, '두': 156, '남': 157, '확': 158, '워': 159, '날': 160, '명': 161, '랑': 162, '될': 163, '약': 164, '받': 165, '른': 166, '직': 167, '점': 168, '개': 169, '돼': 170, '든': 171, '차': 172, '후': 173, '갔': 174, '재': 175, '녀': 176, '배': 177, '타': 178, '필': 179, '달': 180, '걸': 181, '발': 182, '호': 183, '감': 184, '죠': 185, '크': 186, '파': 187, '통': 188, '께': 189, '작': 190, '항': 191, '함': 192, '프': 193, '디': 194, '력': 195, '런': 196, '갈': 197, '노': 198, '곳': 199, '탁': 200, '록': 201, '입': 202, '피': 203, '언': 204, '처': 205, '줄': 206, '카': 207, '불': 208, '테': 209, '락': 210, '편': 211, '준': 212, '메': 213, '청': 214, '떻': 215, '본': 216, '단': 217, '된': 218, '속': 219, '버': 220, '체': 221, '송': 222, '얼': 223, '추': 224, '님': 225, '복': 226, '강': 227, '렇': 228, '희': 229, '역': 230, '살': 231, '던': 232, '양': 233, '외': 234, '산': 235, '루': 236, '출': 237, '올': 238, '울': 239, '현': 240, '름': 241, '포': 242, '릴': 243, '토': 244, '반': 245, '레': 246, '또': 247, '변': 248, '줘': 249, '열': 250, '족': 251, '근': 252, '누': 253, '럼': 254, '절': 255, '느': 256, '험': 257, '질': 258, '져': 259, '격': 260, '혼': 261, '임': 262, '표': 263, '책': 264, '떤': 265, '료': 266, '최': 267, '몰': 268, '설': 269, '따': 270, '찾': 271, '새': 272, '월': 273, '법': 274, '목': 275, '머': 276, '천': 277, '종': 278, '움': 279, '힘': 280, '좀': 281, '등': 282, '답': 283, '맞': 284, '볼': 285, '군': 286, '빠': 287, '키': 288, '돌': 289, '침': 290, '돈': 291, '즐': 292, '셔': 293, '참': 294, '린': 295, '궁': 296, '왜': 297, '티': 298, '몇': 299, '활': 300, '겨': 301, '맛': 302, '태': 303, '봐': 304, '판': 305, '빨': 306, '착': 307, '취': 308, '케': 309, '았': 310, '온': 311, '환': 312, '년': 313, '앞': 314, '술': 315, '녁': 316, '끝': 317, '급': 318, '잠': 319, '습': 320, '페': 321, '코': 322, '쓰': 323, '담': 324, '텔': 325, '!': 326, '별': 327, '휴': 328, '째': 329, '폰': 330, '견': 331, '류': 332, '억': 333, '망': 334, '꼭': 335, '객': 336, '플': 337, '귀': 338, '찍': 339, '특': 340, '놀': 341, '1': 342, '향': 343, '며': 344, '먼': 345, '즘': 346, '커': 347, '엄': 348, '접': 349, '석': 350, '큰': 351, '민': 352, '손': 353, '증': 354, '색': 355, '왔': 356, '막': 357, '뭐': 358, '림': 359, '존': 360, '초': 361, '렸': 362, '형': 363, '검': 364, '충': 365, '량': 366, '엔': 367, '합': 368, '핀': 369, '애': 370, '육': 371, '엇': 372, '쳐': 373, '황': 374, '쪽': 375, '평': 376, '얘': 377, '완': 378, '택': 379, '란': 380, '축': 381, '옷': 382, '눈': 383, '떠': 384, '났': 385, '숙': 386, '2': 387, '권': 388, '밤': 389, '브': 390, '악': 391, '늦': 392, '독': 393, '광': 394, '순': 395, '씨': 396, '곤': 397, '끼': 398, '됐': 399, '갖': 400, '꿈': 401, '즈': 402, '꺼': 403, '낼': 404, '닌': 405, '혹': 406, '박': 407, '글': 408, '짜': 409, '베': 410, '응': 411, '찮': 412, '였': 413, '걱': 414, '씀': 415, '쉽': 416, '쁘': 417, '첨': 418, '꾸': 419, '겁': 420, '팅': 421, '써': 422, '련': 423, '승': 424, '0': 425, '잡': 426, '창': 427, '셨': 428, '읽': 429, '병': 430, '괜': 431, '럽': 432, '핑': 433, '벌': 434, '슨': 435, '밖': 436, '믿': 437, '둘': 438, '떨': 439, '밥': 440, '랜': 441, '욕': 442, '철': 443, '팀': 444, '션': 445, '첫': 446, '쉬': 447, '긴': 448, '채': 449, '죽': 450, '뜻': 451, '됩': 452, '패': 453, '멋': 454, '죄': 455, '칠': 456, '켜': 457, '몸': 458, '허': 459, '클': 460, '램': 461, '듣': 462, '획': 463, '잊': 464, '싫': 465, '액': 466, '투': 467, '넷': 468, '뉴': 469, '큼': 470, '퓨': 471, '3': 472, '론': 473, '딸': 474, '갑': 475, '냈': 476, '냥': 477, '쁜': 478, '웃': 479, '골': 480, '례': 481, '블': 482, '끔': 483, '싸': 484, '놓': 485, '극': 486, '김': 487, '쇼': 488, '컴': 489, '북': 490, '잔': 491, '익': 492, '텐': 493, '득': 494, '얻': 495, '협': 496, '팔': 497, '틀': 498, '핸': 499, '삶': 500, '뒤': 501, '졌': 502, '봤': 503, '랐': 504, '샘': 505, '높': 506, '씩': 507, '켓': 508, '밀': 509, '넌': 510, '깨': 511, '풍': 512, '넣': 513, '똑': 514, '홍': 515, '퍼': 516, '효': 517, '립': 518, '템': 519, '백': 520, '듯': 521, '널': 522, '염': 523, '뭔': 524, '곧': 525, '좌': 526, '캐': 527, '젠': 528, '봉': 529, '짐': 530, '턴': 531, '탈': 532, '풀': 533, '벽': 534, '삼': 535, '옆': 536, '츠': 537, '5': 538, '닐': 539, '젝': 540, '컨': 541, '꽃': 542, '앉': 543, '슬': 544, '납': 545, '욱': 546, '콘': 547, '렌': 548, '헤': 549, '혀': 550, '넘': 551, '덕': 552, '퇴': 553, '빌': 554, '낸': 555, '쯤': 556, '쟁': 557, '롭': 558, '규': 559, '킹': 560, '찰': 561, '굴': 562, '탕': 563, '율': 564, '겼': 565, '짓': 566, '싱': 567, '낮': 568, '톡': 569, '킨': 570, '맡': 571, '념': 572, '멀': 573, '흥': 574, '슷': 575, '척': 576, '웠': 577, '콩': 578, '범': 579, '링': 580, '깐': 581, '센': 582, '졸': 583, '측': 584, '뻐': 585, '뷰': 586, '슈': 587, '챙': 588, '홈': 589, '긍': 590, '왕': 591, '렵': 592, '논': 593, '콜': 594, '픽': 595, '찬': 596, '꿔': 597, '줬': 598, '랫': 599, '깊': 600, '탄': 601, '징': 602, '킬': 603, '샤': 604, '잃': 605, '숨': 606, '럴': 607, '빛': 608, '솔': 609, '뭘': 610, '령': 611, '푸': 612, '낌': 613, '냐': 614, '뢰': 615, '므': 616, '짧': 617, '픈': 618, '춰': 619, '튼': 620, '맘': 621, '4': 622, '끄': 623, '랬': 624, '옮': 625, '팬': 626, '맥': 627, '략': 628, '폭': 629, '쿠': 630, '벤': 631, '밌': 632, '웨': 633, '및': 634, '총': 635, '찌': 636, '촬': 637, '즉': 638, '맙': 639, '값': 640, '쩔': 641, '닥': 642, '룸': 643, '뜨': 644, '흔': 645, '렴': 646, '끊': 647, '벨': 648, '컬': 649, '곡': 650, '섯': 651, '껴': 652, '깔': 653, '층': 654, '칙': 655, '엘': 656, '빵': 657, '왼': 658, '쓸': 659, '샌': 660, '끗': 661, '붙': 662, '빈': 663, '잖': 664, '뮤': 665, '쿄': 666, '춤': 667, '칼': 668, '꽤': 669, '딩': 670, '맨': 671, '뛰': 672, '끌': 673, '촌': 674, '짝': 675, '섬': 676, '굉': 677, '컵': 678, '섭': 679, '닫': 680, '칭': 681, '딱': 682, '델': 683, '딘': 684, '넓': 685, '폐': 686, '’': 687, '쳤': 688, '릭': 689, '씬': 690, '냉': 691, '낄': 692, '폴': 693, '훈': 694, '윈': 695, '땐': 696, '탑': 697, '젊': 698, '옳': 699, '벗': 700, '겐': 701, '낫': 702, '땅': 703, '렛': 704, '농': 705, '흘': 706, '돕': 707, '뀌': 708, '녹': 709, '흐': 710, '훌': 711, '륭': 712, '‘': 713, '깝': 714, '꼼': 715, '닝': 716, '혈': 717, '쾌': 718, '낙': 719, '릿': 720, '웹': 721, '옛': 722, '눠': 723, '홀': 724, '펜': 725, '봄': 726, '혜': 727, '압': 728, '앱': 729, '떡': 730, '룰': 731, '뿐': 732, '밝': 733, '웅': 734, '빼': 735, '샵': 736, '캠': 737, '럭': 738, '싼': 739, '쥴': 740, '멈': 741, '랄': 742, '멘': 743, '훨': 744, '암': 745, '뇨': 746, '둥': 747, '냄': 748, '쌓': 749, '묵': 750, '삭': 751, '꿀': 752, '튜': 753, '쇠': 754, '겪': 755, '6': 756, '혁': 757, '9': 758, '쩌': 759, '묻': 760, '폼': 761, '껏': 762, '엉': 763, '팩': 764, '녕': 765, '멤': 766, '펌': 767, '샀': 768, '뿌': 769, '뚱': 770, '뵙': 771, '덜': 772, '꼬': 773, '닮': 774, '젤': 775, '옥': 776, '헬': 777, '딜': 778, '펴': 779, '섹': 780, '쎄': 781, '균': 782, '멍': 783, '롤': 784, '뒷': 785, '7': 786, '샐': 787, '땠': 788, '빙': 789, '8': 790, '쓴': 791, '털': 792, '셋': 793, '흰': 794, '랍': 795, '괴': 796, '팝': 797, '닭': 798, '십': 799, '촉': 800, '앤': 801, '큐': 802, '셀': 803, '휘': 804, '닉': 805, '볶': 806, '빅': 807, '곁': 808, '틱': 809, '밴': 810, '텍': 811, '캘': 812, '쌀': 813, '곰': 814, '쁠': 815, '낭': 816, '렀': 817, '률': 818, '랙': 819, '겹': 820, '찜': 821, '햄': 822, '흡': 823, '낚': 824, '걷': 825, '칸': 826, '픔': 827, '틴': 828, '쇄': 829, '앙': 830, '뤄': 831, '힐': 832, '튀': 833, '닿': 834, '밑': 835, '끓': 836, '낳': 837, '몬': 838, '숫': 839, '녔': 840, '룹': 841, '듭': 842, '닙': 843, '롯': 844, '잉': 845, '슴': 846, '헌': 847, '랩': 848, '꾼': 849, '춘': 850, '놔': 851, '눌': 852, '쏟': 853, '힌': 854, '뇌': 855, '끈': 856, '벼': 857, '캔': 858, '멜': 859, '펼': 860, '쉴': 861, '빔': 862, '봇': 863, '콤': 864, '꼈': 865, '뷔': 866, '몽': 867, '쨌': 868, '잇': 869, '햇': 870, '잎': 871, '틈': 872, '팁': 873, '퀴': 874, '엽': 875, '긋': 876, '윤': 877, '씻': 878, '푹': 879, '댁': 880, '멕': 881, '푼': 882, '겸': 883, '왠': 884, '낀': 885, '겉': 886, '밍': 887, '곱': 888, '깜': 889, '꿨': 890, '펙': 891, '톰': 892, '샷': 893, '섞': 894, '탐': 895, '늙': 896, '뽀': 897, '뱅': 898, '헷': 899, '깎': 900, '깅': 901, '닷': 902, '캄': 903, '낯': 904, '퀘': 905, '묘': 906, '릎': 907, '앨': 908, '렁': 909, '킷': 910, '숍': 911, '룩': 912, '붕': 913, '잭': 914, '짬': 915, '뽕': 916, '톤': 917, '릇': 918, '엌': 919, '잤': 920, '융': 921, '쌍': 922, '헨': 923, '얀': 924, '덤': 925, '쿨': 926, '톱': 927, '맹': 928, '띄': 929, '둬': 930, '셉': 931, '썼': 932, '땀': 933, '쥐': 934, '깃': 935, '삐': 936, '쥬': 937, '쭤': 938, '뚜': 939, '듬': 940, '룻': 941, '윗': 942, '넥': 943, '젖': 944, '맑': 945, '볍': 946, '겟': 947, '즌': 948, '~': 949, '퉁': 950, '댄': 951, '옹': 952, '엑': 953, '갤': 954, '굳': 955, '헐': 956, '쌈': 957, '렐': 958, '셜': 959, '놈': 960, '켈': 961, '룬': 962, '둔': 963, '덩': 964, '넉': 965, '짚': 966, '쉐': 967, '뒀': 968, '땄': 969, '걀': 970, '덮': 971, '뀔': 972, '뵈': 973, '샴': 974, '밭': 975, '찢': 976, '%': 977, '쁨': 978, '렉': 979, '콕': 980, '흑': 981, '늬': 982, '핵': 983, '숭': 984, '뼈': 985, '뀐': 986, '숲': 987, '쐬': 988, '펠': 989, '벚': 990, '펀': 991, '옵': 992, '칵': 993, '탠': 994, '컷': 995, '껍': 996, '놨': 997, '벳': 998, '밸': 999, '덧': 1000, '댓': 1001, '껑': 1002, '좁': 1003, '몹': 1004, '뜯': 1005, '맵': 1006, '짖': 1007, '첸': 1008, '뽑': 1009, '괄': 1010, '룡': 1011, '됬': 1012, '얽': 1013, '덥': 1014, '렬': 1015, '탓': 1016, '칩': 1017, '롬': 1018, '켰': 1019, '줍': 1020, '륙': 1021, '뭇': 1022, '헛': 1023, '핫': 1024, '젓': 1025, '퀄': 1026, '붐': 1027, '빡': 1028, '덴': 1029, '캡': 1030, '덟': 1031, '쿤': 1032, '핼': 1033, '훔': 1034, '롱': 1035, '빽': 1036, '갓': 1037, '턱': 1038, '깼': 1039, '툴': 1040, '횡': 1041, '핏': 1042, '윷': 1043, '뱉': 1044, '쿼': 1045, '갚': 1046, '둑': 1047, '얹': 1048, '귄': 1049, '빗': 1050, '쭈': 1051, '쭉': 1052, '딴': 1053, '땡': 1054, '븐': 1055, '랭': 1056, '밟': 1057, '씹': 1058, '썰': 1059, '뚫': 1060, '뉘': 1061, '팽': 1062, '뚝': 1063, '굽': 1064, '삽': 1065, '릉': 1066, '첩': 1067, '둠': 1068, '씌': 1069, '쫄': 1070, '얄': 1071, '꽉': 1072, '붓': 1073, '녘': 1074, '탔': 1075, '벙': 1076, '쩍': 1077, '굼': 1078, '맷': 1079, '뤘': 1080, '갯': 1081, '촛': 1082, '팥': 1083, '톨': 1084, '흠': 1085, '팍': 1086, '솥': 1087, '묶': 1088, '딪': 1089, '뮌': 1090, '맺': 1091, '냅': 1092, '*': 1093, '켄': 1094, '뭡': 1095, '돋': 1096, '횟': 1097, '떼': 1098, '돗': 1099, '텝': 1100, '젯': 1101, '찼': 1102, '얗': 1103, '갇': 1104, '걔': 1105, '흉': 1106, '찻': 1107, '셈': 1108, '줌': 1109, '럿': 1110, '닦': 1111, '엣': 1112, '찔': 1113, '삿': 1114, '뜩': 1115, '웬': 1116, '잌': 1117, '꼴': 1118, '굶': 1119, '눔': 1120, '돔': 1121, '뱀': 1122, '툼': 1123, '꼽': 1124, '짢': 1125, '앰': 1126, '왈': 1127, '귤': 1128, '홉': 1129, '젼': 1130, '찹': 1131, '츄': 1132, '곽': 1133, '싣': 1134, '딛': 1135, '똥': 1136, '깰': 1137, '흙': 1138, '팸': 1139, '킴': 1140, '혐': 1141, '짊': 1142, '둡': 1143, '돛': 1144, '갱': 1145, '“': 1146, '”': 1147, '뺄': 1148, '칫': 1149, '뺏': 1150, '셰': 1151, '멧': 1152, '밧': 1153, '틸': 1154, '덱': 1155, '멸': 1156, '썩': 1157, '쩐': 1158, '꽁': 1159, '찐': 1160, '팠': 1161, '쪼': 1162, '쫓': 1163, '껀': 1164, '벅': 1165, '핌': 1166, '짤': 1167, '쟤': 1168, '낵': 1169, '꽂': 1170, '꺾': 1171, '뜰': 1172, '헝': 1173, '얕': 1174, '팡': 1175, '댑': 1176, '슘': 1177, '쫀': 1178, '&': 1179, '펭': 1180, '깍': 1181, '짠': 1182, '냇': 1183, '엥': 1184, '쉰': 1185, '쏴': 1186, '캣': 1187, '챔': 1188, '췄': 1189, '앗': 1190, '궐': 1191, '썬': 1192, '잼': 1193, '듈': 1194, '넛': 1195, '귈': 1196, '눴': 1197, '툰': 1198, '숯': 1199, '텀': 1200, '흩': 1201, '됨': 1202, '첼': 1203, '랗': 1204, '뭄': 1205, '룽': 1206, '펍': 1207, '엎': 1208, '탱': 1209, '궈': 1210, '빚': 1211, '쨍': 1212, '쬐': 1213, '닛': 1214, '텅': 1215, '쏠': 1216, '렷': 1217, '깥': 1218, '볕': 1219, '늑': 1220, '갛': 1221, '낡': 1222, '춧': 1223, '갉': 1224, '벛': 1225, '뻔': 1226, '퀸': 1227, '꺽': 1228, '섰': 1229, '됀': 1230, '끽': 1231, '맟': 1232, '뵌': 1233, '벡': 1234, '곯': 1235, '텨': 1236, '넨': 1237, '썸': 1238, '딤': 1239, '숟': 1240, '윌': 1241, '뎌': 1242, '쨋': 1243, '뻗': 1244, '헹': 1245, '궜': 1246, '훤': 1247, '+': 1248, '컥': 1249, '괌': 1250, '쯧': 1251, '컸': 1252, '딧': 1253, '챤': 1254, '챗': 1255, '잣': 1256, 'Z': 1257, '짱': 1258, '딥': 1259, '랠': 1260, '·': 1261, '밋': 1262, '쯔': 1263, '쿵': 1264, '슐': 1265, '쾰': 1266, '\\\\': 1267, '탤': 1268, '얌': 1269, '킥': 1270, '싯': 1271, '엿': 1272, '싹': 1273, '꾹': 1274, '…': 1275, '팜': 1276, '뭣': 1277, '펫': 1278, '뎅': 1279, '곗': 1280, '쌔': 1281, '퀵': 1282, '텃': 1283, '홋': 1284, '랖': 1285, '껌': 1286, '챈': 1287, '풋': 1288, '뻬': 1289, '댜': 1290, '갭': 1291, '큘': 1292, '챠': 1293, '즙': 1294, '뚤': 1295, '눗': 1296, '빴': 1297, '늪': 1298, '뭉': 1299, '뛸': 1300, '팟': 1301, '퐁': 1302, '솜': 1303, '핍': 1304, '휠': 1305, '킵': 1306, '뿜': 1307, '낱': 1308, '펩': 1309, '쩜': 1310, '쪄': 1311, '떄': 1312, '휼': 1313, '얇': 1314, '탭': 1315, '짭': 1316, '썹': 1317, '앵': 1318, '렙': 1319, '옐': 1320, '댐': 1321, '챡': 1322, '몫': 1323, '욘': 1324, '갸': 1325, '띡': 1326, '섀': 1327, '꾳': 1328, '슝': 1329, '[': 1330, ']': 1331, '솝': 1332, '믹': 1333, '댕': 1334, '넋': 1335, '앓': 1336, '셧': 1337, '쟈': 1338, '쟀': 1339, '쑥': 1340, '썽': 1341, '콰': 1342, '텟': 1343, '띠': 1344, '샬': 1345, '멩': 1346, '펄': 1347, '묽': 1348, '귓': 1349, '욧': 1350, '윔': 1351, ':': 1352, '멎': 1353, '혔': 1354, '딕': 1355, '뎁': 1356, '셸': 1357, '핥': 1358, '갠': 1359, '콧': 1360, '뻤': 1361, '웰': 1362, '줏': 1363, '맴': 1364, '츰': 1365, '넜': 1366, '넸': 1367, '벵': 1368, '눕': 1369, '륨': 1370, '휜': 1371, '깬': 1372, '뜬': 1373, '퉜': 1374, '꿇': 1375, '챦': 1376, '댈': 1377, '뎠': 1378, '붉': 1379, '늄': 1380, '짂': 1381, '됫': 1382, '뵐': 1383, '굵': 1384, '봅': 1385, '땃': 1386, '뗐': 1387, '꿰': 1388, '찡': 1389, '겔': 1390, '뻑': 1391, '긔': 1392, '쏙': 1393, '뽐': 1394, '숱': 1395, '땜': 1396, '릅': 1397, '훗': 1398, '뷸': 1399, '넬': 1400, '긁': 1401, '힙': 1402, '꾀': 1403, '굿': 1404, '깡': 1405, '찿': 1406, '맸': 1407, '멉': 1408, '춥': 1409, '젬': 1410, '촣': 1411}\n"
     ]
    }
   ],
   "source": [
    "# source 언어 Tokenizing\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer_source = Tokenizer(num_words=None, char_level=True, lower = False) # 음절 기반, 문장부호를 제거하진 않음\n",
    "tokenizer_source.fit_on_texts(data.source)\n",
    "word_index_source = tokenizer_source.word_index\n",
    "\n",
    "print(\"전체에서 %s개의 고유한 토큰을 찾았습니다.\" % len(word_index_source))\n",
    "print(\"word_index_source : \", word_index_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1657068168094,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "opCaU8I9mm3a",
    "outputId": "5b9def6b-4441-4a58-cbeb-bceb9779849b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체에서 76개의 고유한 토큰을 찾았습니다.\n",
      "word_index_target :  {' ': 1, 'e': 2, 't': 3, 'o': 4, 'a': 5, 'n': 6, 'i': 7, 's': 8, 'r': 9, 'h': 10, 'l': 11, 'u': 12, 'd': 13, 'y': 14, 'm': 15, '\\t': 16, '\\n': 17, 'c': 18, '.': 19, 'w': 20, 'g': 21, 'f': 22, 'p': 23, 'I': 24, 'b': 25, 'k': 26, 'v': 27, \"'\": 28, ',': 29, 'T': 30, '?': 31, 'W': 32, 'x': 33, 'S': 34, 'A': 35, 'Y': 36, 'H': 37, 'j': 38, 'K': 39, 'P': 40, 'M': 41, 'C': 42, 'B': 43, 'q': 44, 'E': 45, 'D': 46, 'F': 47, 'z': 48, 'O': 49, 'N': 50, 'J': 51, 'L': 52, 'G': 53, '!': 54, 'R': 55, 'U': 56, 'V': 57, '’': 58, '\"': 59, '`': 60, ';': 61, 'Z': 62, 'Q': 63, '&': 64, '~': 65, 'X': 66, '*': 67, '‘': 68, ':': 69, '´': 70, '[': 71, ']': 72, '…': 73, '“': 74, '\\\\': 75, '+': 76}\n"
     ]
    }
   ],
   "source": [
    "# target 언어 Tokenizing\n",
    "tokenizer_target = Tokenizer(num_words=None, char_level=True, lower = False) # 음절 기반, 문장부호를 제거하진 않음\n",
    "tokenizer_target.fit_on_texts(data.target_input)\n",
    "word_index_target = tokenizer_target.word_index\n",
    "\n",
    "print(\"전체에서 %s개의 고유한 토큰을 찾았습니다.\" % len(word_index_target))\n",
    "print(\"word_index_target : \", word_index_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1657068168095,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "9AclJlAYmwRI",
    "outputId": "8b2b533d-4b35-4b49-ae4d-3059fe57fd79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of encoder_input sequencing : \n",
      "나는 매일 저녁 배트를 만나러 다락방으로 가요. [14, 6, 1, 145, 30, 1, 84, 316, 1, 177, 113, 20, 1, 42, 14, 88, 1, 24, 210, 140, 41, 25, 1, 10, 3, 2]\n",
      "선생님 이문장이 이해가 안 가요. [135, 56, 225, 1, 4, 80, 78, 4, 1, 4, 8, 10, 1, 83, 1, 10, 3, 2]\n",
      "컴퓨터를 시작하면 시간이 너무 빠르게 가요. [489, 471, 142, 20, 1, 31, 190, 11, 34, 1, 31, 95, 4, 1, 67, 85, 1, 287, 139, 29, 1, 10, 3, 2]\n",
      "나는 오늘 자정에 한국으로 돌아 가요. [14, 6, 1, 64, 128, 1, 47, 63, 7, 1, 21, 69, 41, 25, 1, 289, 15, 1, 10, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = tokenizer_source.texts_to_sequences(data.source)\n",
    "\n",
    "print(\"Result of encoder_input sequencing : \")\n",
    "print(data.source[0], encoder_input[0])\n",
    "print(data.source[1], encoder_input[1])\n",
    "print(data.source[2], encoder_input[2])\n",
    "print(data.source[3], encoder_input[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1657068168095,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "zcEftPLvm0lI",
    "outputId": "5252fe03-96e0-4109-f68f-7a678136cf94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of decoder_input sequencing : \n",
      "\tI go to the attic every evening to meet Bat.\n",
      " [16, 24, 1, 21, 4, 1, 3, 4, 1, 3, 10, 2, 1, 5, 3, 3, 7, 18, 1, 2, 27, 2, 9, 14, 1, 2, 27, 2, 6, 7, 6, 21, 1, 3, 4, 1, 15, 2, 2, 3, 1, 43, 5, 3, 19, 17]\n",
      "\tSir, I don't understand this sentence here.\n",
      " [16, 34, 7, 9, 29, 1, 24, 1, 13, 4, 6, 28, 3, 1, 12, 6, 13, 2, 9, 8, 3, 5, 6, 13, 1, 3, 10, 7, 8, 1, 8, 2, 6, 3, 2, 6, 18, 2, 1, 10, 2, 9, 2, 19, 17]\n",
      "\tTime flies when you start using the computer.\n",
      " [16, 30, 7, 15, 2, 1, 22, 11, 7, 2, 8, 1, 20, 10, 2, 6, 1, 14, 4, 12, 1, 8, 3, 5, 9, 3, 1, 12, 8, 7, 6, 21, 1, 3, 10, 2, 1, 18, 4, 15, 23, 12, 3, 2, 9, 19, 17]\n",
      "Result of decoder_target sequencing : \n",
      "I go to the attic every evening to meet Bat.\n",
      " [24, 1, 21, 4, 1, 3, 4, 1, 3, 10, 2, 1, 5, 3, 3, 7, 18, 1, 2, 27, 2, 9, 14, 1, 2, 27, 2, 6, 7, 6, 21, 1, 3, 4, 1, 15, 2, 2, 3, 1, 43, 5, 3, 19, 17]\n",
      "Sir, I don't understand this sentence here.\n",
      " [34, 7, 9, 29, 1, 24, 1, 13, 4, 6, 28, 3, 1, 12, 6, 13, 2, 9, 8, 3, 5, 6, 13, 1, 3, 10, 7, 8, 1, 8, 2, 6, 3, 2, 6, 18, 2, 1, 10, 2, 9, 2, 19, 17]\n",
      "Time flies when you start using the computer.\n",
      " [30, 7, 15, 2, 1, 22, 11, 7, 2, 8, 1, 20, 10, 2, 6, 1, 14, 4, 12, 1, 8, 3, 5, 9, 3, 1, 12, 8, 7, 6, 21, 1, 3, 10, 2, 1, 18, 4, 15, 23, 12, 3, 2, 9, 19, 17]\n",
      "Result of decoder_target sequencing : \n",
      "I go to the attic every evening to meet Bat.\n",
      " [24, 1, 21, 4, 1, 3, 4, 1, 3, 10, 2, 1, 5, 3, 3, 7, 18, 1, 2, 27, 2, 9, 14, 1, 2, 27, 2, 6, 7, 6, 21, 1, 3, 4, 1, 15, 2, 2, 3, 1, 43, 5, 3, 19, 17]\n",
      "Sir, I don't understand this sentence here.\n",
      " [34, 7, 9, 29, 1, 24, 1, 13, 4, 6, 28, 3, 1, 12, 6, 13, 2, 9, 8, 3, 5, 6, 13, 1, 3, 10, 7, 8, 1, 8, 2, 6, 3, 2, 6, 18, 2, 1, 10, 2, 9, 2, 19, 17]\n",
      "Time flies when you start using the computer.\n",
      " [30, 7, 15, 2, 1, 22, 11, 7, 2, 8, 1, 20, 10, 2, 6, 1, 14, 4, 12, 1, 8, 3, 5, 9, 3, 1, 12, 8, 7, 6, 21, 1, 3, 10, 2, 1, 18, 4, 15, 23, 12, 3, 2, 9, 19, 17]\n"
     ]
    }
   ],
   "source": [
    "decoder_input = tokenizer_target.texts_to_sequences(data.target_input)\n",
    "decoder_target = tokenizer_target.texts_to_sequences(data.target_target)\n",
    "\n",
    "print('Result of decoder_input sequencing : ')\n",
    "print(data.target_input[0], decoder_input[0])\n",
    "print(data.target_input[1], decoder_input[1])\n",
    "print(data.target_input[2], decoder_input[2])\n",
    "\n",
    "print('Result of decoder_target sequencing : ')\n",
    "print(data.target_target[0], decoder_target[0])\n",
    "print(data.target_target[1], decoder_target[1])\n",
    "print(data.target_target[2], decoder_target[2])\n",
    "\n",
    "print('Result of decoder_target sequencing : ')\n",
    "print(data.target_target[0], decoder_target[0])\n",
    "print(data.target_target[1], decoder_target[1])\n",
    "print(data.target_target[2], decoder_target[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1657068170465,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "FHY1vhdWm5_f",
    "outputId": "74ef57d6-53bf-4611-c2c6-2b94527d95b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding result sample : \n",
      "\tI go to the attic every evening to meet Bat.\n",
      " [16 24  1 21  4  1  3  4  1  3 10  2  1  5  3  3  7 18  1  2 27  2  9 14\n",
      "  1  2 27  2  6  7  6 21  1  3  4  1 15  2  2  3  1 43  5  3 19 17  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "decoder_input length :  30000\n",
      "decoder_input type :  <class 'numpy.ndarray'>\n",
      "decoder_input shape :  (30000, 62)\n"
     ]
    }
   ],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "encoder_input = pad_sequences(encoder_input, maxlen = max_src_len, padding = \"post\")\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_tar_len, padding = \"post\")\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_tar_len, padding = \"post\")\n",
    "\n",
    "print(\"Padding result sample : \")\n",
    "print(data.target_input[0], decoder_input[0])\n",
    "\n",
    "print(\"decoder_input length : \", len(decoder_input))\n",
    "print(\"decoder_input type : \", type(decoder_input))\n",
    "print(\"decoder_input shape : \", decoder_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1672,
     "status": "ok",
     "timestamp": 1657068174009,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "VBo66NaknBF_",
    "outputId": "b1b1a3f7-ef83-4157-88ce-cde46bdec6de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of One-Hot Encodded decoder_input sequencing : \n",
      "(30000, 62, 77)\n",
      "\tI go to the attic every evening to meet Bat.\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "encoder_input = to_categorical(encoder_input, num_classes = len(word_index_source) + 1)\n",
    "decoder_input = to_categorical(decoder_input, num_classes = len(word_index_target) + 1)\n",
    "decoder_target = to_categorical(decoder_target, num_classes = len(word_index_target) + 1)\n",
    "\n",
    "print(\"Result of One-Hot Encodded decoder_input sequencing : \")\n",
    "print(decoder_input.shape)\n",
    "print(data.target_input[0], decoder_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1657068174010,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "v1gc6tDanM9a",
    "outputId": "c09f1e0a-c7b1-4a17-a69f-1580f1ef9f18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-0\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0.]\n",
      "0-1\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0.]\n",
      "0-2\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0.]\n",
      "0-3\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0.]\n",
      "0-18\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"0-0\\n\", decoder_input[0][0])\n",
    "print(\"0-1\\n\", decoder_input[0][1])\n",
    "print(\"0-2\\n\", decoder_input[0][2])\n",
    "print(\"0-3\\n\", decoder_input[0][3])\n",
    "print(\"0-18\\n\", decoder_input[0][18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1657068175921,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "HOE4CL7xnTUH",
    "outputId": "b441f560-e7ee-4f95-d02a-91d9aaa59cc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input length :  30000\n",
      "decoder_input type :  <class 'numpy.ndarray'>\n",
      "decoder_input shape :  (30000, 62, 77)\n"
     ]
    }
   ],
   "source": [
    "print(\"decoder_input length : \", len(decoder_input))\n",
    "print(\"decoder_input type : \", type(decoder_input))\n",
    "print(\"decoder_input shape : \", decoder_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 5447,
     "status": "ok",
     "timestamp": 1657068181361,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "HVCJaokZopo3"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input,LSTM, Dense\n",
    "\n",
    "encoder_inputs = Input(shape = (None, len(word_index_source) + 1))\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "_, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 1297,
     "status": "ok",
     "timestamp": 1657068182640,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "kfxmCEl1o3lx"
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, len(word_index_target) + 1))\n",
    "decoder_lstm = LSTM(256, return_sequences = True, return_state = True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(word_index_target) + 1, activation = \"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 174273,
     "status": "ok",
     "timestamp": 1657068356897,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "crsCPm_So8Fd",
    "outputId": "8cc79679-b72f-47b7-dea1-af4cda900209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "375/375 [==============================] - 446s 1s/step - loss: 2.0202 - val_loss: 1.7552\n",
      "Epoch 2/2\n",
      "375/375 [==============================] - 418s 1s/step - loss: 1.6115 - val_loss: 1.5589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1452251ec10>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer = \"rmsprop\", loss = \"categorical_crossentropy\")\n",
    "model.fit(x=[encoder_input, decoder_input], y = decoder_target, batch_size = 64, epochs = 2, validation_split = 0.2) # epochs 많으면 터진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1657068366379,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "1qtlaYuNpBdI"
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "executionInfo": {
     "elapsed": 1100,
     "status": "ok",
     "timestamp": 1657068370476,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "zaTM0SZvpBpt"
   },
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape = (256,)) # Encoder vector를 받기 위함\n",
    "decoder_state_input_c = Input(shape = (256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(inputs = [decoder_inputs] + decoder_states_inputs, outputs = [decoder_outputs] + decoder_states) # input으론 target_input과 현재의 문장상태, output으론 예측 결과와 은닉상태와 셀상태가 나오게 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 524,
     "status": "ok",
     "timestamp": 1657068373552,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "NC16yd7vpOg1",
    "outputId": "4450de6f-cc40-4e9f-b43a-0b137a554b6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ' ', 2: 'e', 3: 't', 4: 'o', 5: 'a', 6: 'n', 7: 'i', 8: 's', 9: 'r', 10: 'h', 11: 'l', 12: 'u', 13: 'd', 14: 'y', 15: 'm', 16: '\\t', 17: '\\n', 18: 'c', 19: '.', 20: 'w', 21: 'g', 22: 'f', 23: 'p', 24: 'I', 25: 'b', 26: 'k', 27: 'v', 28: \"'\", 29: ',', 30: 'T', 31: '?', 32: 'W', 33: 'x', 34: 'S', 35: 'A', 36: 'Y', 37: 'H', 38: 'j', 39: 'K', 40: 'P', 41: 'M', 42: 'C', 43: 'B', 44: 'q', 45: 'E', 46: 'D', 47: 'F', 48: 'z', 49: 'O', 50: 'N', 51: 'J', 52: 'L', 53: 'G', 54: '!', 55: 'R', 56: 'U', 57: 'V', 58: '’', 59: '\"', 60: '`', 61: ';', 62: 'Z', 63: 'Q', 64: '&', 65: '~', 66: 'X', 67: '*', 68: '‘', 69: ':', 70: '´', 71: '[', 72: ']', 73: '…', 74: '“', 75: '\\\\', 76: '+'}\n"
     ]
    }
   ],
   "source": [
    "index_to_src = dict((i, char) for char, i in word_index_source.items())\n",
    "index_to_tar = dict((i, char) for char, i in word_index_target.items())\n",
    "print(index_to_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "executionInfo": {
     "elapsed": 838,
     "status": "ok",
     "timestamp": 1657068376717,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "nbJGejZyrN5O"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "  states_value = encoder_model.predict(input_seq) # 초기 문장상태벡터를 얻음\n",
    "  target_seq = np.zeros((1, 1, len(word_index_target) + 1)) # 디코더 초기화\n",
    "  target_seq[0, 0, word_index_target[\"\\t\"]] = 1. # 디코더의 첫 시작은 \"\\t\"이므로 원-핫 인코딩으로 기록\n",
    "  stop_condition = False\n",
    "  decoded_sentence = \"\"\n",
    "\n",
    "  while not stop_condition:\n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value) # target_input과 문장상태벡터 입력\n",
    "    sampled_token_index = np.argmax(output_tokens)\n",
    "\n",
    "    if (sampled_token_index == 0):\n",
    "      sampled_token_index = 1 # 1은 공백 음절\n",
    "\n",
    "    sampled_char = index_to_tar[sampled_token_index]\n",
    "    decoded_sentence += sampled_char\n",
    "\n",
    "    if (sampled_char == \"\\n\" or len(decoded_sentence) > max_tar_len):\n",
    "      stop_condition = True\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, len(word_index_target) + 1))\n",
    "    target_seq[0, 0, sampled_token_index] = 1.\n",
    "    states_value = [h, c]\n",
    "\n",
    "  return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7480,
     "status": "ok",
     "timestamp": 1657068386791,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "JPdaMfzppSy_",
    "outputId": "9458a547-7c0d-47c9-d017-2b4c8ff4c608"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 872ms/step\n",
      "1/1 [==============================] - 1s 576ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "-----------------------------------\n",
      "입력 문장 :  선생님 이문장이 이해가 안 가요.\n",
      "정답 문장 :  Sir, I don't understand this sentence here.\n",
      "번역기가 번역한 문장 :  I want to ke with the will be the seating the  and and the  am\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "-----------------------------------\n",
      "입력 문장 :  컴퓨터를 시작하면 시간이 너무 빠르게 가요.\n",
      "정답 문장 :  Time flies when you start using the computer.\n",
      "번역기가 번역한 문장 :  I want to ke with the will be the seating the  and and the sse\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "-----------------------------------\n",
      "입력 문장 :  나는 오늘 자정에 한국으로 돌아 가요.\n",
      "정답 문장 :  I'm going back to Korea today at midnight.\n",
      "번역기가 번역한 문장 :  I want to ke with the will be the seating the  and and the sse\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for seq_index in [1, 2, 3]:\n",
    "  input_seq = encoder_input[seq_index:seq_index+1] # 3차원 배열에선 [n:n+1] 형태로 출력해야 3차원이 유지되면서 n번째가 출력된다.\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(35 * \"-\")\n",
    "  print(\"입력 문장 : \", data.source[seq_index])\n",
    "  print(\"정답 문장 : \", data.target[seq_index][:len(data.target[seq_index])])\n",
    "  print(\"번역기가 번역한 문장 : \", decoded_sentence[:len(decoded_sentence) - 1]) # \"\\n\"은 빼고 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqOBbrwVphIa"
   },
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tb-Rn5fy7a87"
   },
   "source": [
    "## 환경 설정 & 하이퍼 파라미터 설정 & 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "executionInfo": {
     "elapsed": 13394,
     "status": "ok",
     "timestamp": 1657067085721,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "bNDD09Jz7W31"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import *\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1657067085726,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "_ipIPdD577S9"
   },
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history[\"val_\" + string], \"\")\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, \"val_\"+string])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1657067085738,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "GOy4lgK48WCF"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1111)\n",
    "np.random.seed(1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1657067139290,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "30DwXwfd8boH"
   },
   "outputs": [],
   "source": [
    "CLASS_NUMBER = 2\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 2\n",
    "VALID_SPLIT = 0.2\n",
    "MAX_LEN = 40\n",
    "BERT_CKPT = \"C:\\\\Users\\\\Windows\\\\Desktop\\\\대학교\\\\4학년 여름방학\\\\Big_AI\\\\자연어처리\\\\pytest\\\\data\\\\KOR\\\\naver_movie\\\\bert_ckpt\\\\\"\n",
    "DATA_IN_PATH = \"C:\\\\Users\\\\Windows\\\\Desktop\\\\대학교\\\\4학년 여름방학\\\\Big_AI\\\\자연어처리\\\\pytest\\\\data\\\\KOR\\\\naver_movie\\\\data_in\\\\\"\n",
    "DATA_OUT_PATH = \"C:\\\\Users\\\\Windows\\\\Desktop\\\\대학교\\\\4학년 여름방학\\\\Big_AI\\\\자연어처리\\\\pytest\\\\data\\\\KOR\\\\naver_movie\\\\data_out\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "executionInfo": {
     "elapsed": 2949,
     "status": "ok",
     "timestamp": 1657067144901,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "BdXiVU4V84gM"
   },
   "outputs": [],
   "source": [
    "def listToString(listdata):\n",
    "  result = \"id\\tdocument\\tlabel\\n\"\n",
    "  for data_each in listdata:\n",
    "    if data_each:\n",
    "      result += data_each[0] + \"\\t\" + data_each[1] + \"\\t\" + data_each[2] + \"\\n\"\n",
    "  return result\n",
    "\n",
    "def read_data(filename, encoding=\"cp949\", start = 0):\n",
    "  with open(filename, \"r\", encoding=encoding) as f:\n",
    "    data = [line.split(\"\\t\") for line in f.read().splitlines()]\n",
    "    data = data[start:]\n",
    "  return data\n",
    "\n",
    "def write_data(data, filename, encoding=\"cp949\"):\n",
    "  with open(filename, \"w\", encoding=encoding) as f:\n",
    "    f.write(data)\n",
    "\n",
    "data_ratings = read_data(os.path.join(DATA_IN_PATH, \"ratings_utf8_small.txt\"), encoding=\"utf-8\", start = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bx2XGgFh94ZV"
   },
   "source": [
    "## 훈련데이터와 테스트데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "executionInfo": {
     "elapsed": 2058,
     "status": "ok",
     "timestamp": 1657067150209,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "7AvS0tWq91GP"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ratings_train, ratings_test = train_test_split(data_ratings)\n",
    "\n",
    "ratings_train = listToString(ratings_train)\n",
    "ratings_test = listToString(ratings_test)\n",
    "\n",
    "write_data(ratings_train, os.path.join(DATA_IN_PATH, \"ratings_train.txt\"), encoding=\"utf-8\")\n",
    "write_data(ratings_test, os.path.join(DATA_IN_PATH, \"ratings_test.txt\"), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvleTc8C-gfk"
   },
   "source": [
    "## BERT 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6968,
     "status": "ok",
     "timestamp": 1657067159825,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "0v73uaVK-aMk",
    "outputId": "71133c65-37a3-411c-ce21-145c10284df7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\bert_ckpt\\tokenizer\\tmpwygiwhgy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ebd820a94bf4bbb9e237914c21e6c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=995526.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt in cache at C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\bert_ckpt\\tokenizer\\eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
      "creating metadata file for C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\bert_ckpt\\tokenizer\\eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\bert_ckpt\\tokenizer\\tmp5kawznl1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22937a4ba0054c308106ce29276859b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=29.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json in cache at C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\bert_ckpt\\tokenizer\\f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "creating metadata file for C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\bert_ckpt\\tokenizer\\f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\bert_ckpt\\tokenizer\\eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\bert_ckpt\\tokenizer\\f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\bert_ckpt\\tokenizer\\tmpcq9rji9k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0e1ed790ee45c6b1388cd6cbcaba40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json in cache at C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\bert_ckpt\\tokenizer\\6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "creating metadata file for C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\bert_ckpt\\tokenizer\\6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\bert_ckpt\\tokenizer\\6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", cache_dir=os.path.join(BERT_CKPT, \"tokenizer\"), do_lower_case = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1657067159826,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "JhyJGXrP-7y8",
    "outputId": "a1b6a658-4388-40bb-cad2-645c064ac25e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 9521, 118741, 35506, 24982, 48549, 117, 9321, 118610, 119081, 48345, 119, 102]\n",
      "['[ C L S ]', '안', '# # 녕', '# # 하', '# # 세', '# # 요', ',', '반', '# # 갑', '# # 습', '# # 니 다', '.', '[ S E P ]']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"안녕하세요, 반갑습니다.\"\n",
    "\n",
    "encode = tokenizer.encode(test_sentence)\n",
    "token_point = [tokenizer.decode(token) for token in encode]\n",
    "\n",
    "print(encode)\n",
    "print(token_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1657067159826,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "NiqHUZvx_XjO",
    "outputId": "dc80d997-4658-4960-8af8-d96c8f84829a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 9521, 118741, 35506, 24982, 48549, 117, 9321, 118610, 119081, 48345, 119, 102]\n",
      "[101, 31178, 11356, 102]\n",
      "[CLS] 안녕하세요, 반갑습니다. [SEP]\n",
      "[CLS] Hello world [SEP]\n"
     ]
    }
   ],
   "source": [
    "kor_encode = tokenizer.encode(\"안녕하세요, 반갑습니다.\")\n",
    "eng_encode = tokenizer.encode(\"Hello world\")\n",
    "kor_decode = tokenizer.decode(kor_encode)\n",
    "eng_decode = tokenizer.decode(eng_encode)\n",
    "\n",
    "print(kor_encode)\n",
    "print(eng_encode)\n",
    "print(kor_decode)\n",
    "print(eng_decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wr5MBAqq_vQ6"
   },
   "source": [
    "## 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1657067165811,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "NLQ1ZjBb_tBk",
    "outputId": "a9e1d7d9-1157-472b-9ee9-e9b13c06483f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7743368</td>\n",
       "      <td>박흥식영화 사랑해 말순씨 ***보면 장애우를 배려하지않는 저질 영화장애우가 성희롱이...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4254270</td>\n",
       "      <td>성형부작용 같은 주인공 얼굴 때문에 집중이 안됨..;;;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>530171</td>\n",
       "      <td>맥티어난의 최고 졸작품</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9462634</td>\n",
       "      <td>어린 외계인역 하신분 귀엽네요..ㅎ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   7743368  박흥식영화 사랑해 말순씨 ***보면 장애우를 배려하지않는 저질 영화장애우가 성희롱이...      0\n",
       "1  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1\n",
       "2   4254270                    성형부작용 같은 주인공 얼굴 때문에 집중이 안됨..;;;      0\n",
       "3    530171                                       맥티어난의 최고 졸작품      0\n",
       "4   9462634                                어린 외계인역 하신분 귀엽네요..ㅎ      1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_TRAIN_PATH = os.path.join(DATA_IN_PATH, \"ratings_train.txt\")\n",
    "DATA_TEST_PATH = os.path.join(DATA_IN_PATH, \"ratings_test.txt\")\n",
    "\n",
    "train_data = pd.read_csv(DATA_TRAIN_PATH, header = 0, delimiter = \"\\t\", quoting = 3)\n",
    "train_data = train_data.dropna()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEm5lRCOALr_"
   },
   "source": [
    "## 스페셜 토큰 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1657067168835,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "wecNndeuAHw9",
    "outputId": "c9363d21-5cec-47fe-9d94-27a249daf00c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]'] \n",
      " [100, 102, 0, 101, 103]\n",
      "[101, 9521, 118741, 35506, 24982, 48549, 117, 9321, 118610, 119081, 48345, 119, 102]\n",
      "[101, 31178, 11356, 102]\n",
      "[CLS] 안녕하세요, 반갑습니다. [SEP]\n",
      "[CLS] Hello world [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.all_special_tokens, \"\\n\", tokenizer.all_special_ids)\n",
    "\n",
    "kor_encode = tokenizer.encode(\"안녕하세요, 반갑습니다.\")\n",
    "eng_encode = tokenizer.encode(\"Hello world\")\n",
    "\n",
    "kor_decode = tokenizer.decode(kor_encode)\n",
    "eng_decode = tokenizer.decode(eng_encode)\n",
    "\n",
    "print(kor_encode)\n",
    "print(eng_encode)\n",
    "print(kor_decode)\n",
    "print(eng_decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jovsZlfiApir"
   },
   "source": [
    "## 사용자 정의 BERT 토크나이저 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "executionInfo": {
     "elapsed": 605,
     "status": "ok",
     "timestamp": 1657067172343,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "S8YvRpP8Amd1"
   },
   "outputs": [],
   "source": [
    "def bert_tokenizer(sent, MAX_LEN):\n",
    "  encoded_dict = tokenizer.encode_plus(\n",
    "      text = sent,\n",
    "      add_special_tokens = True,\n",
    "      max_length = MAX_LEN,\n",
    "      pad_to_max_length = True,\n",
    "      return_attention_mask = True\n",
    "  )\n",
    "\n",
    "  input_id = encoded_dict[\"input_ids\"]\n",
    "  attention_mask = encoded_dict[\"attention_mask\"]\n",
    "  token_type_id = encoded_dict[\"token_type_ids\"]\n",
    "\n",
    "  return input_id, attention_mask, token_type_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdI2CxNcBlNa"
   },
   "source": [
    "## 훈련데이터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1657067175362,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "6JJchvu8BU6M",
    "outputId": "3a787c93-ff21-4b99-8f61-7add0bb1239f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/375 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 375/375 [00:00<00:00, 1656.40it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "token_type_ids = []\n",
    "train_data_labels = []\n",
    "\n",
    "for train_sent, train_label in tqdm(zip(train_data[\"document\"], train_data[\"label\"]), total = len(train_data)):\n",
    "  input_id, attention_mask, token_type_id = bert_tokenizer(train_sent, MAX_LEN)\n",
    "\n",
    "  input_ids.append(input_id)\n",
    "  attention_masks.append(attention_mask)\n",
    "  token_type_ids.append(token_type_id)\n",
    "  train_data_labels.append(train_label)\n",
    "\n",
    "train_movie_input_ids = np.array(input_ids, dtype = int)\n",
    "train_movie_attention_masks = np.array(attention_masks, dtype = int)\n",
    "train_movie_type_ids = np.array(token_type_ids, dtype = int)\n",
    "train_movie_inputs = (train_movie_input_ids, train_movie_attention_masks, train_movie_type_ids)\n",
    "\n",
    "train_data_labels = np.asarray(train_data_labels, dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 564,
     "status": "ok",
     "timestamp": 1657067179557,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "qtTlQIUzC-xb",
    "outputId": "09c4b93e-2860-448c-9e31-9a3674cf9461"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   101   9521  21789   9651 119168  11102   9326  35506 118762  10530\n",
      "   9138  13767   9757  48210  89851  18589  42428    119    102      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "[CLS] 안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "input_id = train_movie_input_ids[1]\n",
    "attention_mask = train_movie_attention_masks[1]\n",
    "token_type_id = train_movie_type_ids[1]\n",
    "\n",
    "print(input_id)\n",
    "print(attention_mask)\n",
    "print(token_type_id)\n",
    "print(tokenizer.decode(input_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJKt3c-iDTdv"
   },
   "source": [
    "## BERT 사전학습모델을 이용한 분류기 정의 & 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1657067183251,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "DOWXzCPdDQ5v"
   },
   "outputs": [],
   "source": [
    "class TFBertClassifier(tf.keras.Model):\n",
    "  def __init__(self, model_name, dir_path, num_class):\n",
    "    super(TFBertClassifier, self).__init__()\n",
    "\n",
    "    self.bert = TFBertModel.from_pretrained(model_name, cache_dir = dir_path)\n",
    "    self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "    self.classifier = tf.keras.layers.Dense(num_class, name = \"classifier\")\n",
    "  \n",
    "  def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
    "    outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    pooled_output = outputs[1]\n",
    "    pooled_output = self.dropout(pooled_output, training = training)\n",
    "    logits = self.classifier(pooled_output)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21153,
     "status": "ok",
     "timestamp": 1657067207651,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "QUgJLCDaD9_U",
    "outputId": "10bd1b19-6bf5-4f05-a138-0a22d6b9c743"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\bert_ckpt\\model\\tmphvf83e19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db604e036b04d6a85ee0f26d960788b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json in cache at C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\bert_ckpt\\model\\6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "creating metadata file for C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\bert_ckpt\\model\\6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\bert_ckpt\\model\\6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/bert-base-multilingual-cased/resolve/main/tf_model.h5 not found in cache or force_download set to True, downloading to C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\bert_ckpt\\model\\tmpltwl4kfj\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83d2e4fb65d4a4ab4f6042c3baf8e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1083389348.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/tf_model.h5 in cache at C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\bert_ckpt\\model\\879ba3c37de5c396bee982a9383f64cf08c9cc966d66742254a3904e6719357f.53d9b251a2a9d5d86139b64555b5e8deb0a20fc53f8a5ee958ddbb4506125a0b.h5\n",
      "creating metadata file for C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\bert_ckpt\\model\\879ba3c37de5c396bee982a9383f64cf08c9cc966d66742254a3904e6719357f.53d9b251a2a9d5d86139b64555b5e8deb0a20fc53f8a5ee958ddbb4506125a0b.h5\n",
      "loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tf_model.h5 from cache at C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\bert_ckpt\\model\\879ba3c37de5c396bee982a9383f64cf08c9cc966d66742254a3904e6719357f.53d9b251a2a9d5d86139b64555b5e8deb0a20fc53f8a5ee958ddbb4506125a0b.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "cls_model = TFBertClassifier(model_name=\"bert-base-multilingual-cased\", dir_path = os.path.join(BERT_CKPT, \"model\"), num_class=CLASS_NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhP2H43bFUCd"
   },
   "source": [
    "## 모델 컴파일 & 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "executionInfo": {
     "elapsed": 1471,
     "status": "ok",
     "timestamp": 1657067531738,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "iiigMqT8FGJs"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n",
    "cls_model.compile(optimizer=optimizer, loss = loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1657067537723,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "BFigVTvwFpn_",
    "outputId": "7aed51fc-7951-4a5b-9824-d0ff63196467"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\data_out\\tf2_bert_naver_movie -- Folder create complete \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tf2_bert_naver_movie\"\n",
    "earlystop_callback = EarlyStopping(monitor=\"val_accuracy\", min_delta=0.0001, patience=2)\n",
    "\n",
    "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, \"weights.h5\")\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "  print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "  os.makedirs(checkpoint_dir, exist_ok = True)\n",
    "  print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "\n",
    "cp_callback = ModelCheckpoint(checkpoint_path, monitor = \"val_accuracy\", verbose = 1, save_best_only = True, save_weights_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58258,
     "status": "ok",
     "timestamp": 1657067599919,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "V91V3APrGhXV",
    "outputId": "74111595-de2f-4a39-d76f-5934fd76b616"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7148 - accuracy: 0.4833 \n",
      "Epoch 1: val_accuracy improved from -inf to 0.53333, saving model to C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\data_out\\tf2_bert_naver_movie\\weights.h5\n",
      "10/10 [==============================] - 237s 21s/step - loss: 0.7148 - accuracy: 0.4833 - val_loss: 0.6826 - val_accuracy: 0.5333\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6636 - accuracy: 0.6367 \n",
      "Epoch 2: val_accuracy improved from 0.53333 to 0.65333, saving model to C:\\Users\\Windows\\Desktop\\대학교\\4학년 여름방학\\Big_AI\\자연어처리\\pytest\\data\\KOR\\naver_movie\\data_out\\tf2_bert_naver_movie\\weights.h5\n",
      "10/10 [==============================] - 223s 22s/step - loss: 0.6636 - accuracy: 0.6367 - val_loss: 0.6459 - val_accuracy: 0.6533\n",
      "{'loss': [0.7147948145866394, 0.6635547876358032], 'accuracy': [0.4833333194255829, 0.6366666555404663], 'val_loss': [0.6825587153434753, 0.6459322571754456], 'val_accuracy': [0.5333333611488342, 0.653333306312561]}\n"
     ]
    }
   ],
   "source": [
    "history = cls_model.fit(train_movie_inputs, train_data_labels, epochs = NUM_EPOCHS, batch_size = BATCH_SIZE, validation_split = VALID_SPLIT, callbacks = [earlystop_callback, cp_callback]) # epoch 늘리면 터짐\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nQA0eHUHUPY"
   },
   "source": [
    "## 테스트 데이터 확인 & 변환 & 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1657067601059,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "_1mi3GbVHGAM",
    "outputId": "013d987a-7b82-44b8-e161-cb731ab57ca6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9904263</td>\n",
       "      <td>그 반지의 제왕을 만든 피터 잭슨 ㅋㅋ 이런 영화을 찍었다는거 부터가 웃겼다. ㅋㅋ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>307000</td>\n",
       "      <td>삐질삐질;;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8620641</td>\n",
       "      <td>내가 없어질 내인생. 제목을 참 잘 지었다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7553915</td>\n",
       "      <td>굿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9339372</td>\n",
       "      <td>중3인데 일단 2까지봣다. 뭐 말할것도없다장국영 자살이 안타깝다</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           document  label\n",
       "0  9904263  그 반지의 제왕을 만든 피터 잭슨 ㅋㅋ 이런 영화을 찍었다는거 부터가 웃겼다. ㅋㅋ...      1\n",
       "1   307000                                             삐질삐질;;      0\n",
       "2  8620641                           내가 없어질 내인생. 제목을 참 잘 지었다.      1\n",
       "3  7553915                                                  굿      0\n",
       "4  9339372                중3인데 일단 2까지봣다. 뭐 말할것도없다장국영 자살이 안타깝다      1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(DATA_TEST_PATH, header = 0, delimiter = \"\\t\", quoting = 3)\n",
    "test_data = test_data.dropna()\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1657067602135,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "4fMgZ0EvHGRA",
    "outputId": "9ddb3ef7-be8d-419f-b84c-b7c751355529"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "125it [00:00, 864.37it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "token_type_ids = []\n",
    "test_data_labels = []\n",
    "\n",
    "for test_sent, test_label in tqdm(zip(test_data[\"document\"], test_data[\"label\"])):\n",
    "  input_id, attention_mask, token_type_id = bert_tokenizer(test_sent, MAX_LEN)\n",
    "\n",
    "  input_ids.append(input_id)\n",
    "  attention_masks.append(attention_mask)\n",
    "  token_type_ids.append(token_type_id)\n",
    "  test_data_labels.append(test_label)\n",
    "\n",
    "test_movie_input_ids = np.array(input_ids, dtype = int)\n",
    "test_movie_attention_masks = np.array(attention_masks, dtype = int)\n",
    "test_movie_type_ids = np.array(token_type_ids, dtype = int)\n",
    "test_movie_inputs = (test_movie_input_ids, test_movie_attention_masks, test_movie_type_ids)\n",
    "\n",
    "test_data_labels = np.asarray(test_data_labels, dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 594,
     "status": "ok",
     "timestamp": 1657067608451,
     "user": {
      "displayName": "진창호",
      "userId": "09608549926967536417"
     },
     "user_tz": -540
    },
    "id": "SIUcy_anIQOz",
    "outputId": "5edb065b-00f9-40d3-8b7e-0ca062ef6310"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 16s 4s/step - loss: 0.6757 - accuracy: 0.5840\n",
      "test loss, test acc :  [0.6757106184959412, 0.5839999914169312]\n"
     ]
    }
   ],
   "source": [
    "results = cls_model.evaluate(test_movie_inputs, test_data_labels, batch_size = BATCH_SIZE)\n",
    "print(\"test loss, test acc : \", results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMi2p4cXQg1aqxLA+iY2sxc",
   "collapsed_sections": [
    "kqOBbrwVphIa"
   ],
   "name": "7월 2일.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
