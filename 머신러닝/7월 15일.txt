비지도 학습(Clustering)
비슷한 객체들을 하나의 그룹으로 묶는 과정

유사도(거리) 정보 기반
그룹끼리는 거리가 촘촘하게 뭉쳐야 하고 해당 그룹은 다른 그룹과는 거리가 멀어야 한다.

K-means 클러스터링
유클리디안 거리를 사용한 클러스터링(피타고라스 공식)
임의로 k개 던짐 -> 모든 점에 대한 유클리디안 거리 구하기 -> 중심점으로 k 이동 -> 모든 점에 대한 유클리디안 거리 구하기 -> 3, 4 과정을 중심점이 안 움직일 때까지 반복

K-means 클러스터링 문제
1. 초기값이 군집에 큰 영향을 미친다.
2. 원형 형태의 클러스터이다.
3. 이상치에 약하다.

K-means의 문제 해결법
1. n_init : 초기값을 많이 설정해본다.
2. init = 'k-means++' : 초기값을 많이 퍼지게 만든다.
3. K를 설정해 이상치 민감성을 줄인다.

K-means의 평가 지표
Silhouette score : 값이 클수록 군집끼리 잘 나눠져 있다.
Elbow method : inertia_값을 가져와서 그래프를 그렸을 때 팔꿈치 부분이 가장 이상적인 K이다.

계층 클러스터링
거리에 따라 계층적으로 군집을 나눈다.

유사도
Min Distance : 군집간의 최소거리
Max Distance : 군집간의 최대거리
Group Average Distance : 군집간의 거리의 평균
Between Centroids Distance : 군집의 대푯값의 거리

Min Distance를 기준으로 삼는 계층 클러스터링
가장 짧은 거리의 객체를 군집으로 잡는다. 그러면 군집으로 묶인 객체는 하나의 객체로 생각한다. -> 모두가 하나의 군집이 될 때까지 반복한다. -> 최근에 합쳐진 위치를 자르며 군집을 나눈다.

계층 클러스터링의 장, 단점
장점 : 군집 구조 정보가 매우 풍부, 군집화의 결과가 유동적, 임의의 모양을 갖는 군집 생성, 이상치 판단 가능
단점 : 계산 복잡도가 높고 메모리 확보가 필요

DBSCAN
특정 범위에 자신을 제외한 객체를 센다. -> MinPts를 넘으면 가장 먼 객체를 찾아간다.
-> 다시 특정 범위에 자신을 제외한 객체를 센다. -> MinPts를 넘지 않으면 그룹화를 멈추고 MinPts를 넘으면 첫 찾아간 객체의 방향에서 가장 먼 객체를 찾아간다.
-> 모든 객체가 그룹화될 때까지 반복한다.

파라미터 최적화 방법
EPS 늘림, 줄임 & MinPts 늘림, 줄임으로 어떻게 되는지 파악

DBSCAN 장, 단점
장점 : 노이즈에 매우 둔감한 군집화, 임의의 모양을 갖는 군집 생성가능
단점 : 파라미터에 민감, 계산 비용이 높음

회귀 분석 - y = w0x
RSS는 2차함수로 그려진다. 이 때 기울기가 0이 되는 점을 찾으면 최적의 w가 된다.
RSS 기울기가 양수면 weight(x값)가 작아지고, RSS 기울기가 음수면 weight(x값)이 커진다.
이 때 Learning rate를 곱해줌으로써 최적점을 널뛰기하는 현상을 막는다.

회귀 분석 - y = w0x + w1
RSS는 3차함수로 그려진다.

사이킷런 - Linear Regression
정규화는 자동으로 이뤄짐

회귀평가지표
MAE, MSE, RMSE, R^2
 