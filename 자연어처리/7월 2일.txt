모델의 정확성을 높이는 방법 
1. DTM 컬럼을 늘려준다 : 훈련 데이터의 성능은 좋아지나, 테스트 데이터의 성능은 떨어진다.
2. Dense 층을 늘려주거나, Cell 수를 늘려준다.
3. L1, L2 규제를 활용한다 : 손실에 비용을 추가하여 가중치를 낮춘다. 따라서 이상치를 설명하는 가중치를 줄일 수 있다. 
규제를 적용하면 가중치가 작아진다. 이는 비용을 낮추는 역할을 하는 데 비용이 높았다가 비용이 낮아지면 해당 값에 대한 훈련을 멈추기 때문에 이상치 예측을 막을 수 있다.
일반적으론 L1보다 L2가 효과가 좋다. (why? 독립 변수가 L2보다 L1에서 가중치가 0이 되기 싶고, 이는 독립 변수를 없애는 역할을 하기 때문이다.)
훈련이 진행될수록 validation loss가 높아지면 과적합을 의심해봐야한다.
4. Dropout : 노드의 일부분을 사용하지 않는 기법이다. 이를 통해 중요하지 않은 우연한 패턴이 깨지게 된다.
일반적으로 0.5 이하로 많이 사용한다.
L1, L2 규제는 어느정도 바꿀 지 설정할 수 없지만, Dropout은 어느정도 바꿀 지 설정할 수 있다.
Dropout + L2 규제는 오히려 손실값이 높아진다. 따라서 우선순위는 Dropout 단독 -> L2 단독 순이다.
5. 파라미터 조정
상황에 따른 마지막 층의 활성화 함수, 손실 함수는 상황에 따라 외워두기
Optimizer : RMSprop(처음에는 크게 변하다가 수정량을 줄여가며 최적의 가중치를 찾는 기법), Adam(RMSprop보다 더 부드럽게 수정량을 줄이는 기법)
모델을 복잡하게 만들거나 epoch을 늘려준다.

순환신경망 모델의 정확성을 높이는 추가적인 방법
1. recurrent_dropout : 히든층이 다른 히든층에 끼치는 영향 일부를 건너뛴다. 성능 개선은 크지 않을 때가 많다.
2. return_sequences = True : RNN의 층을 늘려준다.
3. Bidirectional : 양방향 순환층, input -> output 뿐만 아니라 output -> input도 고려해본다. 이 또한 층을 쌓을 수 있다.

One-Hot Encoding 함수
1. keras.utils.to_categorical() : 차원을 하나 늘림
2. texts_to_matrix(mode = "binary") : 차원을 안 늘림

함수형 API
Sequential 모델 : 네트워크의 입력과 출력이 하나라고 가정
다중 입력, 다중 출력, 그래프, 잔차 연결 등 다양한 구조를 지원

번역 모델
Encoder(원문) -> Context Vector -> Decoder(번역문)
 
문장 생성
1. I go to a school, I go to a library 같은 많은 문장을 훈련시킨다.
2. 앞 타임스텝의 출력 결과로 다음 타임스텝에 발생할 단어를 예측한다.
3. 기존에 입력된 문장과 유사하게 자동 생성한다.
캐릭터 -> 계산된 숫자(캐릭터랑 다를 수 있다.) -> so, 착각이 생길 수 있다.
<sos> : start of sentense, <eos> : end of sentense

seq2seq
음절 기반의 코드이기 때문에 word embedding은 사용하지 않는다. (점심 시간 : "점" -> "심" -> " " -> "시" -> "간")
embedding의 역할인 차원을 늘리기 위해 to_categorical을 사용한다.

seq2seq 데이터가 작은 데 훈련이 잘 되는 이유
1. 데이터가 작으면 반복해서 훈련해서 관계를 파악하기 쉽다.
2. 대부분 뜻이 구분되있기 때문에 훈련이 헷갈릴 일이 없다.

시작부호와 종료부호 부착
<sos>가 5음절("<", "s", "o", "s", ">")이기 때문에 \t로 대체해서 넣어준다. <eos>도 동일한 원리로 \n으로 대체해서 넣어준다.

문장의 길이
어차피 <eos>가 있기 때문에 최대 길이로 정한다.

Data Tokenizing
음절 기반으로 하기 위해서 char_level = True로 설정한다.
문장 부호를 제거하지 않는다.
decoder input이 한국어, decoder output이 한국어면 하나의 Tokenizer만 사용, decoder input이 영어, decoder output이 한국어면 두개의 Tokenizer 사용
한글의 음절 : 11172개

Data Padding
그 전에 Pre Padding을 한 이유 : 순환 신경망은 뒤로 갈수록 영향력이 크기 때문이다.
Pre Padding을 하면 0부터 나오다가 max_len에 도달해서 단어의 의미가 없어질 수 있다. 따라서 Post Padding을 수행한다.

Teacher Forcing
decoder input을 예측해서 나온 decoder output(정답과 다름)을 정답과 비교해서 정답을 낼 수 있도록 가중치를 조절하는 단계
그런데 위처럼 하면 순서대로 전부 다 예측할 때까지 시간이 너무 오래 걸리므로 각 step마다 정답을 밀어넣어준다.

훈련용 Encoder
입력문의 길이는 문장이 다르므로 None 설정
RNN과 RNN을 연결할 때 : return_sequences = True
은닉상태와 셀상태를 죽이지 않고 다음으로 넘겨줄 때 : return_state = True
encoder_outputs는 사용하지 않으므로 _로 교체한다. 

훈련용 Decoder
력문의 길이는 문장이 다르므로 None 설정
모든 타임스텝의 결과를 출력, 내부상태를 받음 : return_sequences = True, return_state = True
초기 상태는 encoder_states를 사용한다.

모델 훈련
Model(Input, Output) : Input과 Output 사이의 연결 관계를 다 이어준다.

예측용 Encoder
훈련용 Encoder에서 사용한 encoder를 그대로 사용 : 학습된 걸 가져온다.

예측용 Decoder (훈련용 Decoder에는 그냥 강제로 밀어넣어줬지만, 이제는 순차적인 단계가 필요하다.)
decoder_states_inputs : 위 쪽의 states_value
위 쪽의 states_value : 아래 쪽의 states_value
아래 쪽의 states_value는 다음 타임스텝의 input으로 들어간다.

예측 함수
target_seq : 한 문장, 한 음절 씩 처리할 예정 
stop_condition까지 반복
우선 시작 value + 문장 상태 벡터
가장 큰 인덱스 사용
padding은 1(공백)로 치환 / 훈련이 덜 되면 빈도가 높은 padding이 많이 나올 것이다.
dict을 이용해 decoded_sentence에 넣어준다.
종료 조건을 확인한다.
무언가가 다시 전달되어 반복된다.

seq2seq
정확한 해석은 아니더라도 비슷한 단어를 뽑아준다. 즉, 번역기에 사용해 볼 만하다.

Attention
seq2seq의 한계 : 
1. 고정된 길이의 벡터를 사용하므로 문장이 길어질수록 문장의 차이를 구별하기 어려워진다. 
2. 순환신경망은 문장이 길어질수록 그래디언트 소실 문제가 발생한다.
Encoder의 출력 결과를 적층시킨다. -> hs행렬
seq2seq에서 제거한 단어 간 상관관계를 다시 접목

Decoder 기본 구조
hs * a의 결과를 다 더해(가중합) 컨텍스트 벡터를 만들고 이와 가장 유사한 단어를 찾아 단어 유사도를 정의한다.

가중치 벡터 a를 구하는 방법(핵심은 임베딩)
아메리카노의 encoding 결과인 hs에다가 americano의 LSTM 수행 결과와 내적을 사용한다.
위의 결과에 softmax를 적용하면 가중치 벡터 a가 도출된다.
미리 사용된 임베딩이 아닌 keras 기본 임베딩을 사용해도 첫 번째 hidden layer의 대분류 역할을 잘 수행하므로 성능이 나쁘지 않다.

컨텍스트 벡터 c를 구하는 방법
hs에다가 가중치 벡터 a를 곱하면 컨텍스트 벡터 c가 도출된다.

Attention
가중치 벡터 a를 구하는 과정 + 컨텍스트 벡터 c를 구하는 과정

Decoder 구조
Dense 층에 LSTM의 출력값을 전하는 이유 : 해당 언어의 의미를 보강해준다.

Transformer
순환신경망의 문제 : 앞 time step을 계산해야 뒤 time step을 계산할 수 있고, 그래디언트 소실 문제도 있다.
그래서 Transformer는 인코더-디코더의 구조는 가지고 있지만 순환신경망을 사용하지 않는다.
순환신경망 대신 현재 어휘의 위치 정보를 추가해준다.

Self-Attention
입력된 문장 사이의 관계만을 이용하여 context vector를 구한다.

Scaled Dot-Product Attention
1. 단어의 임베딩 값이랑 서로 다른 가중치 행렬(초기는 랜덤) Wq, Wk, Wv를 행렬곱하여 Query, Key, Value를 만든다.
Query : 찾고자 하는 단어
Key : 사전에 등록된 단어
Value : 사전에 등록된 단어의 의미값
2. Q 행렬의 특정 단어와 K 행렬의 비교하고자 하는 단어를 내적하여 Attention Score를 얻는다.
3. 숫자를 줄이고자 Key 벡터의 차원수의 제곱근으로 나눈다. 그리고 softmax를 적용해 확률 정보로 만든다.
4. 단어마다 Value 벡터를 곱한다.
5. 나온 결과를 다 더해서 Context Vector를 만든다.

Multi Head Attention
Self Attention을 동시에 여러 번 수행한다.
Z : Query + Key + Value
초기값이 다른 3개의 Z를 만들어 다양한 상황에 잘 대응할 수 있게 한다.
실제 논문에서는 8개의 Z를 연결한다.

Positional Encoding : 현재 어휘의 위치 정보를 추가하는 방법
홀수 위치에는 코사인, 짝수 위치에는 사인 함수를 사용해 구분한다.
해당 정보를 더해준다.

Residual Connection(전차 연결)
특정 레이어를 거친 정보를 후에 다시 더하는 과정
레이어를 거치기 전의 정보를 보존하려는 목적

Subsequent Maskted Attention(순방향 마스크 어텐션)
자신보다 뒤에 있는 단어를 참고하지 않게 하는 마스킹 기법(순환 신경망의 개념을 활용한다.)

BERT(분류 알고리즘)
Transformer의 인코더만 가지고 있는 구조

BERT의 Transformer와의 차이점
마스크 언어 모델 : Dropout처럼 빼는 게 아니라 해당 단어를 맞추게 한다.
다음 문장 예측 : 그 다음 문장이 어떤 문장이 나올 지 예측하게 한다. - CLS : 입력 내용의 첫 부분, SEP : 문장이 구분되는 곳을 넣어준다.

BERT는 활성화함수로 ReLU가 아닌 GELU를 사용한다.

Subword Tokenizer
Transformer는 형태소 분석기를 사용한다. 하지만 BERT는 형태소 분석기를 사용하지 않고 wordpiece를 사용한다.

Wordpiece
같이 나타날 확률이 높은 문자열을 토큰으로 삼는 기법

Fine-Tuning(미세 조정)
BERT는 만들기에 굉장히 많은 자원이 필요하기 때문에 BERT를 가져오는 형태로 사용한다. 
입력층 - BERT - 추가 layer(분류 label 결정) - 출력층

Transformer & BERT를 위한 환경
Python 3.6+ (3.7 확인)
Tensorflow 2.3+ (2.9 확인)
최신버전 CUDA(11.7 확인), cuDNN(11.x 확인)
왠만하면 Anaconda Prompt에서
pip install transformers
pip install sacremoses
pip install tensorflow-gpu
pip install sentencepiece

Transformer & BERT는 이진 분류라도 마지막 Layer의 Cell이 2개여야 한다.
































































































