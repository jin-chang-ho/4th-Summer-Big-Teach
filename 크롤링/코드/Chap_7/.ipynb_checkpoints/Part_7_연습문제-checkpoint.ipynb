{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7445d313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 블로그주소:  https://blog.naver.com/hy820715/221514204265\n",
      "2. 작성자 닉네임:  가치랩장입니다\n",
      "3. 작성일자:  2019. 4. 15. 16:27\n",
      "4. 블로그내용: \n",
      " 웹 크롤링 진짜 많이 재미있죠?웹 크롤링에 대한 다양한 예제는 바로 이 책에 들어 있어요~   이 책안에는 다양한 유형의 웹사이트를 파이썬과 셀레니움을 활용하여 수집하는 노하우들이 다 들어 있습니다~하나씩 하나씩 따라하다 보면 금방 실력자가 되어 있으실 거예요~~^^그리고 수집된 데이터를 분석할 때  파이썬도 많이 사용하지만 R 프로그램도 많이 사용합니다~R 프로그램을 공부하시려면 아래의 책을 추천해드려요~~   위의 책에는 R 프로그램을 사용하여 텍스트 데이터를 분석하는 다양한 방법들부터 R 프로그램을 활용한 시각화 ,  지도 작업 , 정형 데이터를 핸들링하는 다양한 패키지 설명까지 제공되고 있어서 쉽고 빠르게 R 프로그램 사용 방법을 배우실 거예요~~​무엇보다도 절대로 포기하지 말고 열공하는 자세가 가장 중요합니다~~^^열공해 주세요~~^^​가치랩장 드림.​​​\n",
      "\n",
      "\n",
      "https://postfiles.pstatic.net/MjAxOTA3MjlfMTQ0/MDAxNTY0MzcwMDYxNzYy.3iQRsQbrL8btKFujR9tFXCT9_PKu3DsiM6up1YBFhAcg.Yj62iO9BekL9OY8AnBmP2Fkc9kkfWbHOcPZh2rqf3k4g.JPEG.hy820715/완친파_표지_최종.jpg?type=w966\n",
      "1 - 이미지 저장 완료\n",
      "https://postfiles.pstatic.net/MjAxOTA0MTVfNDgg/MDAxNTU1MzEzMDg3NTM1.biuF2sH30dCYq9ZITLLjl3rZNoanAYXM2dT4_0qS89sg.ttQz7rYhTwklVdSM_yO9UEbU5h35nO96W6DdR1OGXLQg.JPEG.hy820715/책표지_-_한국정보인재개발원_jpg.jpg?type=w80_blur\n",
      "2 - 이미지 저장 완료\n",
      "======================================================================\n",
      "총 소요시간은 11.110306978225708 초 입니다.\n"
     ]
    }
   ],
   "source": [
    "#1. 네이버 블로그에서 데이터 크롤링하기\n",
    "\n",
    "# Step 1. 필요한 모듈과 라이브러리를 로딩하고 검색어를 입력 받습니다\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request # 이미지 주소로 이미지를 다운받는 모듈\n",
    "import urllib.parse # 한글을 변환해주는 모듈\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "#Step 2. 사용자에게 검색 관련 정보들을 입력 받습니다.\n",
    "f_dir = \"C:\\\\Users\\\\Windows\\\\Desktop\\\\대학교\\\\4학년 여름방학\\\\Big_AI\\\\크롤링\\\\코드\\\\Chap_7\\\\출력 파일\\\\\"\n",
    "\n",
    "#Step 3. 크롬 드라이버 설정 및 웹 페이지 열기\n",
    "s = Service(\"C:\\\\chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "url = 'https://blog.naver.com/hy820715/221514204265'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(3)\n",
    "\n",
    "#Step 4. 텍스트 추출하여 저장하기\n",
    "start=time.time()\n",
    "driver.switch_to.frame(\"mainFrame\")\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "time.sleep(3)\n",
    "\n",
    "nickname = soup.find(\"div\", \"blog2_container\").find(\"span\", \"nick\").get_text()\n",
    "date = soup.find(\"div\", \"blog2_container\").find(\"span\", \"se_publishDate pcol2\").get_text()\n",
    "content = soup.find(\"div\", \"se-main-container\").get_text().replace('\\n', \"\").strip()\n",
    "\n",
    "print(\"1. 블로그주소: \", url)\n",
    "print(\"2. 작성자 닉네임: \", nickname)\n",
    "print(\"3. 작성일자: \", date)\n",
    "print(\"4. 블로그내용: \\n\", content)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Step 5. 이미지 추출하여 저장하기\n",
    "file_no = 0                                \n",
    "count = 1\n",
    "img_src2=[]  # 이미지 원본 URL 주소 저장할 리스트\n",
    "\n",
    "n = time.localtime()\n",
    "s = '%04d-%02d-%02d-%02d-%02d-%02d' % (n.tm_year, n.tm_mon, n.tm_mday, n.tm_hour, n.tm_min, n.tm_sec)\n",
    "f_dir = f_dir + s + \"-gachilap\\\\\"\n",
    "img_dir = f_dir+\"image\\\\\"\n",
    "os.makedirs(f_dir)\n",
    "os.makedirs(img_dir)\n",
    "\n",
    "ft_name = f_dir + s + \"-gachilap.txt\"\n",
    "\n",
    "time.sleep(4)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "img_src = soup.find('div','se-main-container').find_all('img')\n",
    "\n",
    "for i in img_src:\n",
    "    img_src1=i['src']\n",
    "    img_src2.append(img_src1)\n",
    "    \n",
    "for i in range(0,len(img_src2)) :\n",
    "    file_no += 1 \n",
    "    try :\n",
    "        urllib.request.urlretrieve(urllib.parse.quote(img_src2[i].encode('utf8'), '/:'), img_dir + str(file_no)+'.jpg')\n",
    "        print(img_src2[i])\n",
    "        print(\"%s - 이미지 저장 완료\" % file_no)\n",
    "    except TypeError:\n",
    "        continue\n",
    "    time.sleep(0.5)\n",
    "end = time.time()\n",
    "\n",
    "# Step 6. 요약 정보를 출력합니다                \n",
    "print(\"=\" *70)\n",
    "print(\"총 소요시간은 %s 초 입니다.\" % str(end - start))\n",
    "\n",
    "f = open(ft_name, \"w\", encoding=\"utf-8\")\n",
    "f.write(\"1. 블로그주소 : \" + url + \"\\n\")\n",
    "f.write(\"2. 작성자 닉네임 : \" + nickname + \"\\n\")\n",
    "f.write(\"3. 작성일자 : \" + date + \"\\n\")\n",
    "f.write(\"4. 블로그내용 : \" + content + \"\\n\")\n",
    "f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1682ec4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " 연습문제: 블로그  크롤러 : 네이버 view -> 블로그 정보 수집하기\n",
      "================================================================================\n",
      "1.정보를 수집할 키워드는 무엇입니까?: 경남대\n",
      "2.조회를 시작할 날짜를 입력하세요(예:2017-01-01) :2017-01-01\n",
      "3.조회를 종료할 날짜를 입력하세요(예:2017-12-31): 2017-01-31\n",
      "4.몇 건의 정보를 수집할까요? :5\n"
     ]
    }
   ],
   "source": [
    "# 구 주소 블로그 : https://blog.naver.com/junhyuk_abba/221286810022  - postViewArea 클래스명 사용\n",
    "# 신 주소 블로그 : https://blog.naver.com/suheeryu/221314766979 - se_component_wrap 클래스명 사용\n",
    "# 신 주소 블로그 :  https://blog.naver.com/ultrabat/222670803753 - se-main-container 클래스명 사용\n",
    "\n",
    "print(\"=\" *80)\n",
    "print(\" 연습문제: 블로그  크롤러 : 네이버 view -> 블로그 정보 수집하기\")\n",
    "print(\"=\" *80)\n",
    "\n",
    "#Step 1. 필요한 모듈과 라이브러리를 로딩합니다.\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import pandas  as pd    \n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import urllib.request\n",
    "import urllib\n",
    "\n",
    "#Step 2. 필요한 정보 입력받기\n",
    "query_txt = input('1.정보를 수집할 키워드는 무엇입니까?: ')\n",
    "start_date = input('2.조회를 시작할 날짜를 입력하세요(예:2017-01-01) :')\n",
    "end_date = input('3.조회를 종료할 날짜를 입력하세요(예:2017-12-31): ')\n",
    "cnt = int(input('4.몇 건의 정보를 수집할까요? :'))\n",
    "page_cnt = math.ceil( cnt / 30 )\n",
    "f_dir=\"C:\\\\Users\\\\Windows\\\\Desktop\\\\대학교\\\\4학년 여름방학\\\\Big_AI\\\\크롤링\\\\코드\\\\Chap_7\\\\출력 파일\\\\\"\n",
    "\n",
    "# Step 3. 결과를 저장할 파일명을 지정하기\n",
    "import os\n",
    "\n",
    "n = time.localtime()\n",
    "s1 = '%04d-%02d-%02d-%02d-%02d-%02d' %(n.tm_year, n.tm_mon, n.tm_mday, n.tm_hour, n.tm_min, n.tm_sec)   \n",
    "\n",
    "os.makedirs(f_dir+s1+'-'+query_txt)\n",
    " \n",
    "ft_name = f_dir+s1+'-'+query_txt+'\\\\'+s1+'-'+query_txt+'.txt'\n",
    "fc_name = f_dir+s1+'-'+query_txt+'\\\\'+s1+'-'+query_txt+'.csv'\n",
    "fx_name = f_dir+s1+'-'+query_txt+'\\\\'+s1+'-'+query_txt+'.xls'     \n",
    "\n",
    "s_time = time.time( )\n",
    "\n",
    "#Step 4. 검색어 입력한 후 검색하여 View로 이동하기\n",
    "s = Service(\"C:\\\\chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "driver.maximize_window()\n",
    "\n",
    "url = 'https://www.naver.com'\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "element = driver.find_element(By.NAME,\"query\")\n",
    "element.send_keys(query_txt)\n",
    "element.send_keys('\\n')\n",
    "\n",
    "#Step 5. 아래의 블로그 링크를 선택합니다\n",
    "driver.find_element(By.LINK_TEXT,\"VIEW\").click()\n",
    "driver.find_element(By.LINK_TEXT,\"블로그\").click()\n",
    "time.sleep(1)\n",
    "\n",
    "#Step 6. 검색 옵션 버튼 클릭\n",
    "driver.find_element(By.LINK_TEXT,'옵션').click()\n",
    "time.sleep(1)\n",
    "\n",
    "#Step 7. 정렬 버튼 클릭\n",
    "driver.find_element(By.LINK_TEXT,'최신순').click()\n",
    "time.sleep(1)\n",
    "\n",
    "#Step 8. 기간 버튼 클릭\n",
    "driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[1]/a[9]').click()\n",
    "time.sleep(2)\n",
    "\n",
    "# 조회 시작 날짜와 종료 날짜 선택\n",
    "start_date2 = start_date.split('-')\n",
    "start_year = start_date2[0]\n",
    "start_mon = str(int(start_date2[1]))\n",
    "start_day = str(int(start_date2[2]))\n",
    "\n",
    "end_date2 = end_date.split('-')\n",
    "end_year = end_date2[0]\n",
    "end_mon = str(int(end_date2[1]))\n",
    "end_day = str(int(end_date2[2]))\n",
    "\n",
    "#시작 날짜 부분 클릭\n",
    "driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[1]/span[1]/a').click()\n",
    "\n",
    "#시작 년도 클릭\n",
    "driver.find_element(By.LINK_TEXT,start_year).click()\n",
    "time.sleep(1)\n",
    "\n",
    "#시작 월의 값은 xpath 로 선택합니다. (1~9까진 XPATH, 나머지는 LINK_TEXT로 해야함)\n",
    "if start_mon == '1' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[1]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_mon == '2' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[2]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_mon == '3' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[3]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_mon == '4' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[4]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_mon == '5' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[5]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_mon == '6' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[6]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_mon == '7' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[7]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_mon == '8' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[8]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_mon == '9' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[9]/a').click()\n",
    "    time.sleep(1)\n",
    "else :\n",
    "    driver.find_element(By.LINK_TEXT,start_mon).click()\n",
    "    time.sleep(1)\n",
    "\n",
    "#시작일의 값도 xpath 값을 사용합니다.\n",
    "if start_day == '1' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[1]/a').click()\n",
    "    time.sleep(1)                 \n",
    "elif start_day == '2' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[2]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_day == '3' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[3]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_day == '4' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[4]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_day == '5' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[5]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_day == '6' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[6]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_day == '7' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[7]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_day == '8' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[8]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_day == '9' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[9]/a').click()\n",
    "    time.sleep(1)\n",
    "else :\n",
    "    driver.find_element(By.LINK_TEXT,start_day).click()\n",
    "    time.sleep(1)\n",
    "\n",
    "#종료 날짜 부분 클릭\n",
    "driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[1]/span[3]/a').click()\n",
    "\n",
    "#종료 년도 클릭\n",
    "driver.find_element(By.LINK_TEXT,end_year).click()\n",
    "time.sleep(1)  \n",
    "\n",
    "#종료 월의 값은 xpath 로 선택합니다. (1~9까진 XPATH, 나머지는 LINK_TEXT로 해야함)\n",
    "if end_mon == '1' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[1]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_mon == '2' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[2]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_mon == '3' :\n",
    "    driver.find_element_(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[3]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_mon == '4' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[4]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_mon == '5' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[5]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_mon == '6' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[6]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_mon == '7' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[7]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_mon == '8' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[8]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_mon == '9' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[9]/a').click()\n",
    "    time.sleep(1)\n",
    "else :\n",
    "    driver.find_element(By.LINK_TEXT,end_mon).click()\n",
    "    time.sleep(1)\n",
    "\n",
    "#종료일의 값도 xpath 값을 사용합니다.\n",
    "if end_day == '1' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[1]/a').click()\n",
    "    time.sleep(1)                 \n",
    "elif end_day == '2' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[2]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_day == '3' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[3]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_day == '4' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[4]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_day == '5' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[5]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_day == '6' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[6]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_day == '7' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[7]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_day == '8' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[8]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_day == '9' :\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[9]/a').click()\n",
    "    time.sleep(1)\n",
    "else :\n",
    "    driver.find_element(By.LINK_TEXT,end_day).click()\n",
    "    time.sleep(1)\n",
    "\n",
    "# 날짜 입력 후 적용 버튼 클릭        \n",
    "driver.find_element(By.XPATH,'//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[3]/button').click()\n",
    "time.sleep(1)\n",
    "\n",
    "# Step 9. 검색 요청 건수만큼 화면 스크롤링 하기\n",
    "# 자동 스크롤다운 함수\n",
    "def scroll_down(driver):\n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "    time.sleep(3)\n",
    "\n",
    "i = 1\n",
    "while (i <= page_cnt+2):\n",
    "    scroll_down(driver) \n",
    "    i += 1\n",
    "    print('%s 페이지 정보를 추출하고 있으니 잠시만 기다려 주세요~~^^' %i)\n",
    "\n",
    "# Step 10. 현재 조회된 목록에서 URL 주소를 추출하여 리스트 생성하기\n",
    "url_all_list=[]    #조회할 블로그의 URL 정보 저장용 리스트\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "url_list_1 = soup.find('ul','lst_total').find_all('li')\n",
    "\n",
    "for a in url_list_1 :\n",
    "    url_all_list.append( a.find('div','total_area').find_all('a') )\n",
    "\n",
    "url_detail=[]\n",
    "for b in range(0,len(url_all_list)) :\n",
    "    url_detail.append(url_all_list[b][5]['href'])\n",
    "    \n",
    "url_final_list=[]   # 수집할 블로그 리스트를 저장할 변수\n",
    "no = 1\n",
    "for c in url_detail :    \n",
    "    if c.split('/')[2] == 'blog.naver.com' : # 네이버 블로그만 가져옴\n",
    "        url_final_list.append(c)\n",
    "        no += 1\n",
    "\n",
    "        if no > cnt :\n",
    "            break\n",
    "            \n",
    "d_no = 1\n",
    "print('')\n",
    "print('정보를 수집할 블로그 URL 주소는 아래와 같습니다~~~')\n",
    "print('')\n",
    "for d in url_final_list :   \n",
    "    print(d_no,':',d)\n",
    "    d_no += 1\n",
    "\n",
    "driver.close()\n",
    "print('')\n",
    "            \n",
    "#Step 11. 수집된 URL 주소에 접속하여 데이터 추출하기\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "s = Service(\"C:\\\\chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "driver.maximize_window()\n",
    "time.sleep(2)\n",
    "\n",
    "blog_addr2 = []\n",
    "w_name2 = []\n",
    "w_date2 = []\n",
    "blog_txt2 = []\n",
    "\n",
    "no = 1   # 전체 게시글 번호용 변수\n",
    "\n",
    "f = open(ft_name, \"w\", encoding=\"UTF-8\")\n",
    "f.close()\n",
    "\n",
    "for blog_addr in tqdm(url_final_list) :            \n",
    "    driver.get(blog_addr)\n",
    "    time.sleep(3)\n",
    "\n",
    "    driver.switch_to.frame('mainFrame')\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    addr_1 = soup.select('#postViewArea')   \n",
    "    addr_2 = soup.select('div[class=\"se_component_wrap sect_dsc __se_component_area\"]')   \n",
    "    addr_3 = soup.select('div[class=\"se-main-container\"]')  \n",
    "    \n",
    "    if addr_1 :\n",
    "        img_no = 1\n",
    "        print()\n",
    "        print(\"%s번째 게시글 정보를 수집합니다~~~~~~~~~~\" %no)\n",
    "        f = open(ft_name, 'a',encoding='UTF-8')\n",
    "        \n",
    "        # 블로그 URL 주소\n",
    "        print(\"1.블로그주소: \",blog_addr)\n",
    "        blog_addr2.append(blog_addr)\n",
    "        f.write(\"1.블로그 주소:\"+ blog_addr + \"\\n\")\n",
    "        \n",
    "        # 작성자 닉네임\n",
    "        try :\n",
    "            writer = soup.find(\"div\", \"nick\").find(\"strong\", \"itemfont col\").get_text()  # 작성자 닉네임\n",
    "        except :\n",
    "            writer = \"작성자 닉네임이 없습니다\"\n",
    "        else :\n",
    "            writer = writer.replace(\"\\n\",\"\").strip()\n",
    "\n",
    "        print(\"2.작성자 닉네임: \",writer )\n",
    "        w_name2.append(writer)\n",
    "        f.write(\"2.작성자 닉네임:\" + writer + \"\\n\")\n",
    "        \n",
    "        # 작성일자\n",
    "        wdate = soup.select('p[class=\"date fil5 pcol2 _postAddDate\"]')\n",
    "        try : \n",
    "            wdate = wdate[0].get_text( )\n",
    "        except :\n",
    "            wdate = ' '\n",
    "\n",
    "        print(\"3.작성일자:\",wdate)\n",
    "        w_date2.append(wdate)\n",
    "        f.write(\"3.작성 일자:\" + wdate + \"\\n\")\n",
    "\n",
    "        # 블로그 본문 내용\n",
    "        for i in addr_1:\n",
    "                blog_txt = i.text.replace(\"\\n\",\"\").strip() # 엔터키를 제거하는 코드\n",
    "                print(\"4.블로그내용: \\n\",blog_txt) \n",
    "                print(\"\\n\")\n",
    "                blog_txt2.append(blog_txt)\n",
    "                f.write(\"4.블로그 내용:\" + blog_txt+\"\\n\"+\"\\n\")\n",
    "                \n",
    "        # 이미지 저장\n",
    "        img_dir = f_dir+s1+'-'+query_txt+'\\\\'+ writer\n",
    "        os.makedirs(img_dir)\n",
    "        \n",
    "        img_src1 = soup.find('div', id = 'postViewArea').find_all('img')\n",
    "        for img in img_src1 :\n",
    "            img_src2 = img['src']\n",
    "            print(img_src2)\n",
    "            urllib.request.urlretrieve(img_src2, img_dir + str(img_no)+'.jpg')\n",
    "            print(img_no , '-이미지 저장 완료')\n",
    "            img_no += 1\n",
    "            \n",
    "        f.close()\n",
    "        no += 1\n",
    "\n",
    "    elif addr_2 :\n",
    "        img_no =  1\n",
    "        print()\n",
    "        print(\"%s번째 게시글 정보를 수집합니다~~~~~~~~~~\" %no)\n",
    "        f = open(ft_name, 'a',encoding='UTF-8')\n",
    "        \n",
    "        # 블로그 주소\n",
    "        print(\"1.블로그주소: \",blog_addr)\n",
    "        blog_addr2.append(blog_addr)\n",
    "        f.write(\"1.블로그 주소:\"+ blog_addr + \"\\n\")\n",
    "        \n",
    "        # 작성자 닉네임\n",
    "        writer = soup.select(\"div.blog2_container > span.writer\")\n",
    "        try :\n",
    "                wname = writer[0].get_text( )   # 작성자 닉네임\n",
    "        except IndexError :\n",
    "                wname = \"작성자 닉네임이 없습니다\"\n",
    "        else :\n",
    "                wname = wname.replace(\"\\n\",\"\")\n",
    "\n",
    "        print(\"2.작성자 닉네임: \",wname )\n",
    "        w_name2.append(wname)\n",
    "        f.write(\"2.작성자 닉네임:\" + wname + \"\\n\")\n",
    "        \n",
    "        # 작성일자\n",
    "        wdate = soup.select(\"div.blog2_container > span.se_publishDate.pcol2\")\n",
    "        try : \n",
    "                wdate = wdate[0].get_text( )\n",
    "        except IndexError :\n",
    "                wdate = ' '\n",
    "\n",
    "        print(\"3.작성일자:\",wdate)\n",
    "        w_date2.append(wdate)\n",
    "        f.write(\"3.작성 일자:\" + wdate + \"\\n\")\n",
    "\n",
    "        # 블로그 본문 내용\n",
    "        for i in addr_2:\n",
    "                blog_txt = i.text.replace(\"\\n\",\"\")\n",
    "                print(\"4.블로그내용: \\n\",blog_txt) \n",
    "                print(\"\\n\")\n",
    "                blog_txt2.append(blog_txt)\n",
    "                f.write(\"4.블로그 내용:\" + blog_txt+\"\\n\"+\"\\n\")\n",
    "                \n",
    "        # 이미지 저장\n",
    "        img_dir = f_dir+s1+'-'+query_txt+'\\\\'+wname\n",
    "        os.makedirs(img_dir)\n",
    "        os.chdir(img_dir) \n",
    "        \n",
    "        img_src1 = soup.find('div',' pcol2 _param(1) _postViewArea221314766979').find_all('img')\n",
    "        for img in img_src1 :\n",
    "            img_src2 = img['src']\n",
    "            print(img_src2)\n",
    "            urllib.request.urlretrieve(img_src2, img_dir + str(img_no)+'.jpg')\n",
    "            print(img_no , '-이미지 저장 완료')\n",
    "            img_no += 1\n",
    "\n",
    "        f.close( )\n",
    "        no += 1\n",
    "            \n",
    "    elif addr_3 :\n",
    "        img_no = 1\n",
    "        print()\n",
    "        print(\"%s번째 게시글 정보를 수집합니다~~~~~~~~~~\" %no)\n",
    "        f = open(ft_name, 'a',encoding='UTF-8')\n",
    "        \n",
    "        # 블로그 주소\n",
    "        print(\"1.블로그주소: \",blog_addr)\n",
    "        blog_addr2.append(blog_addr)\n",
    "        f.write(\"1.블로그 주소:\"+ blog_addr + \"\\n\")\n",
    "        \n",
    "        # 작성자 닉네임\n",
    "        writer = soup.select(\"div.blog2_container > span.writer\")\n",
    "        try :\n",
    "                wname = writer[0].get_text( )   # 작성자 닉네임\n",
    "        except IndexError :\n",
    "                wname = \"작성자 닉네임이 없습니다\"\n",
    "        else :\n",
    "                wname = wname.replace(\"\\n\",\"\")\n",
    "\n",
    "        print(\"2.작성자 닉네임: \",wname )\n",
    "        w_name2.append(wname)\n",
    "        f.write(\"2.작성자 닉네임:\" + wname + \"\\n\")\n",
    "        \n",
    "        # 작성일자\n",
    "        wdate = soup.select(\"div.blog2_container > span.se_publishDate.pcol2\")\n",
    "        try : \n",
    "                wdate = wdate[0].get_text( )\n",
    "        except IndexError :\n",
    "                wdate = ' '\n",
    "\n",
    "        print(\"3.작성일자:\",wdate)\n",
    "        w_date2.append(wdate)\n",
    "        f.write(\"3.작성 일자:\" + wdate + \"\\n\")\n",
    "        \n",
    "        #블로그 본문 내용\n",
    "        for i in addr_3:\n",
    "                blog_txt = i.text.replace(\"\\n\",\"\")\n",
    "                print(\"4.블로그내용: \\n\",blog_txt) \n",
    "                print(\"\\n\")\n",
    "                blog_txt2.append(blog_txt)\n",
    "                f.write(\"4.블로그 내용:\" + blog_txt+\"\\n\"+\"\\n\")\n",
    "                \n",
    "        # 이미지 저장\n",
    "        img_dir = f_dir+s1+'-'+query_txt+'\\\\'+wname\n",
    "        os.makedirs(img_dir)\n",
    "        \n",
    "        img_src1 = soup.find('div','se-main-container').find_all('img')\n",
    "        for img in img_src1 :\n",
    "            img_src2 = img['src']\n",
    "            print(img_src2)\n",
    "            urllib.request.urlretrieve(img_src2, img_dir + str(img_no)+'.jpg')\n",
    "            print(img_no , '-이미지 저장 완료')\n",
    "            img_no += 1\n",
    "\n",
    "        f.close( )\n",
    "        no += 1\n",
    "                   \n",
    "#Step 8. xls 형태와 csv 형태로 저장하기\n",
    "naver_blog = pd.DataFrame()\n",
    "naver_blog['블로그주소']=blog_addr2\n",
    "naver_blog['작성자닉네임']=w_name2\n",
    "naver_blog['작성일자']=w_date2\n",
    "naver_blog['블로그내용']=blog_txt2\n",
    "\n",
    "# csv 형태로 저장하기\n",
    "naver_blog.to_csv(fc_name,encoding=\"utf-8-sig\",index=False)\n",
    "\n",
    "# 엑셀 형태로 저장하기\n",
    "naver_blog.to_excel(fx_name ,index=False,engine='openpyxl')\n",
    "\n",
    "e_time = time.time( )\n",
    "t_time = e_time - s_time\n",
    "\n",
    "\n",
    "print(\"\\n\") \n",
    "print(\"=\" *80)\n",
    "print(\"총 소요시간은 %s 초 입니다 \" %round(t_time,1))\n",
    "print(\"파일 저장 완료: txt 파일명 : %s \" %ft_name)\n",
    "print(\"파일 저장 완료: csv 파일명 : %s \" %fc_name)\n",
    "print(\"파일 저장 완료: xls 파일명 : %s \" %fx_name)\n",
    "print(\"=\" *80)\n",
    "\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
