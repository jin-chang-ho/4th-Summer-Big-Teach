인공지능 > 머신러닝 > 딥러닝
포함된 개념이지만 세부적으로는 다르다.

스팸
사람 : 인간 지능으론 알고리즘을 변경할 수 없는 지경이다.
머신러닝 : 데이터로 학습해서 발전을 꾀할 수 있다.

데이터와 머신러닝
데이터 : 독창성엔 데이터가 중요
머신러닝 : 모델이 어느정도 자동화 됨
Input이 똑같은데 Output이 다르면 학습이 들쑥날쑥한다.

R vs Python
Python이 R보다 사용량이 훨씬 많다.

지도학습 vs 비지도학습
지도학습 : 데이터와 데이터에 해당하는 라벨이 있어서 해당 데이터로 훈련을 하고, 훈련이 안된 데이터의 라벨을 예측한다.
비지도학습 : Input에 라벨은 없지만, Input을 몇 개의 그룹으로 나눈다.

지도학습 머신러닝 알고리즘
Naive Bayes, Logistic Regression, Decision Tree, SVM, Nearest Neighbor, Neural Network, Ensemble

Decision Tree Classification
계속 조건으로 경우를 나눠서 Tree를 깊게 만든다.
루트노드(처음) - 규칙노드 - 리프노드(마지막)
조건의 순서를 매기는 기준은 균일도(Gini, Entropy - 둘 다 낮을수록 데이터셋이 균일하다.)
균일도가 0이 될 때까지 계속 분기를 나누다 보니 트리가 훈련 데이터에 과적합될 가능성이 많다.
편향 - 분산 tradeoff : 과소적합 시 편향되고 과적합 시 분산된다. 따라서 적절한 훈련 파라미터가 중요
나무구조 제한 : 나무 깊이를 제한하거나 리프 노드의 수를 제한함으로써 과적합을 막는 기법
불순도 기준 : 분기를 발생시키는 최소 불순도를 정하거나 분기가 일어날 때 최소 불순도 감소폭을 정함으로써 과적합을 막는 기법

Decision Tree Regression
판별 기준이 SSE

Decision Tree의 장, 단점
장점 : 해석이 가능하다. 따라서 누군가에게 설명이 가능하다. 그래서 처음으로 해당 모델을 돌려 Feature의 중요도를 파악할 수 있다.
단점 : 성능이 높지 않다. 선형식으로 분류가 불가능하다.

Decision Tree Parameter
min_sample : 노드가 몇 개 이하면 분할을 수행하지 않음.
min_leaf : 노드가 몇 개 이하면 바로 leaf node로 만들어버림.
max_features : 피처의 최대 갯수(default : sqrt(피처 갯수))
max_depth : 트리의 최대 깊이
max_leaf_nodes : leaf node의 최대 갯수

ENSEMBLE
여러 머신러닝 기법을 합친 기법
BAGGING과 BOOSTING 존재
BAGGING : 데이터셋을 N개로 나눠서 각 데이터셋마다 M개의 Tree 적용, M개의 Tree 결과 중 가장 비율이 높은 결과를 최종 라벨로 결정 / 병렬로 훈련가능
BOOSTING : 순서가 있는 모델 / 병렬 처리가 불가 / 10000개 이하의 데이터가 있을 때는 XGBoost, 10000개 이상의 데이터가 있을 때는 LGBM

ENSEMBLE 기법
단순 / 가중 평균, 배깅(Random Forest), 부스팅(XGBoost, LGBM), 스택킹(=메타학습)

Voting
Hard Voting
0, 1로 출력해서 다수결을 따라감
Soft Voting
0, 1이 나온 퍼센트를 모두 더해서 큰 퍼센트를 따라감
So, 주로 Soft Voting을 따라감

단순 / 가중 평균
몇 개의 머신 러닝 기법으로 결과를 낸 후 가장 큰 비율을 차지하는 결과를 따라감 / 생각보다 효과없음

BAGGING(Bootstrap + Aggregating) 기법
트리는 균일도로 트리를 만들기 때문에 같은 데이터로 다양한 트리를 만들어도 분기가 똑같고 결과가 똑같다.
위 현상을 막기 위해 기본 데이터셋에서 랜덤으로 기본 데이터셋 크기만큼 데이터를 가져온다(중복 가능). 하지만 생각보다 트리 출력이 크게 변하지 않는다.
위 현상을 막기 위해 균일도 개념을 버리고 랜덤하게 규칙을 가져온다.

Random Forest의 장, 단점
장점 : 비교적 빠른 수행 속도, 괜찮은 성능
단점 : Bagging이 과소적합을 막기는 어렵다. 2개의 랜덤이 충분히 트리를 비연관적으로 만들 수 없다.

Boosting 방식
데이터셋을 N개로 나눠서 첫번쨰 데이터에서 에러가 작은 방향으로 weight 예측, 그 weight로 두번째 데이터에서 에러가 작은 방향으로 weight 변경, ...., N-1 weight로 N번째 데이터에서 에러가 작은 방향으로 weight 변경

Boosting 주요 파라미터
n_estimators, learning_rate, cv

Boosting의 장, 단점
장점 : 과적합에 강하다. 예측 성능이 뛰어나다.
단점 : 수행시간이 오래 걸린다.

XGBoost VS LightGBM
XGBoost는 균형있는 트리를 만들려고 하나, LightGBM은 불균형적인 트리가 만들어짐.
따라서 LightGBM이 시간이 더 빨리 걸림

Undersampling VS Oversampling
둘 다 한 라벨이 다른 라벨에 비해 양이 압도적으로 작을 때 사용하는 기법
Undersampling은 양이 압도적으로 많은 데이터를 줄이는 것, Oversampling은 양이 압도적으로 적은 데이터를 늘리는 것
다만.. 성능이 좋지 않다.

스태킹 앙상블
데이터 갯수가 m, 특징이 n개인 데이터가 있다.
이 데이터를 M개의 모델로 결과를 예측한다. 이 때 각 모델은 최적화가 아주 잘되어있다.
그럼 총 m * M개의 예측 결과가 탄생한다.
해당 예측 결과를 다른 모델에 넣어 학습시킨다.
그 후 최종 결과를 낼 수 있다.

스태킹 앙상블 장점
각 모델의 장점만을 살릴 수 있다.

스태킹 분류기
스태킹 앙상블을 활용할 수 있는 분류기






