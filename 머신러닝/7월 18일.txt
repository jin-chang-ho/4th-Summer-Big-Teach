다중공선성 : 다수의 독립변수가 서로 지나치게 높은 상관관계를 가지는 문제
1. 분산팽창계수가 10보다 크면 다중공선성이 있다고 판단
2. 산포도 & 상관계수가 0.9 이상이면 다중공선성이 있다고 판단
3. 두 개의 독립변수 중 한 개의 독립변수를 종속변수로 나머지 독립변수를 독립변수로 하는 회귀분석을 실시했을 때
Tolerance가 0에 가까워질수록 다중공선성이 매우 높다.
4. 분산팽창지수가 크면 다중공선성이 크다. (연속형 변수에선 10보다 크면 문제, 범주형 변수에선 3보다 크면 문제)

다중공선성 해결책
1. 독립 변수를 잘 선택
2. 피처 삭제
3. 주성분 분석

다항회귀
다차방정식을 사용하는 회귀방식
차수가 낮으면 편향될 가능성이 커서 Underfitting될 가능성이 크고, 차수가 높으면 변동될 가능성이 커서 Overfitting될 가능성이 크다.
Error : bias + variance + c(데이터가 가지는 에러)

편향 & 분산 감소법
편향 감소 : 더 많은 피처 사용, 좋은 모델 알고리즘 사용
분산 감소 : 더 적은 피처 사용, 제약 사용

규제
데이터가 학습을 방해할 때 확인하기 유용하다.
Min(RSS(w) + alpha * w^2)
규제를 하게되면 w가 매우 낮아진다. 
alpha값이 너무 높으면 과소적합 가능성이 있다.

규제의 종류
L1 규제(Lasso) : 절댓값
L2 규제(Ridge) : 제곱값

보스턴 집값 특징의 이상한 특징
NOX 특징 : NOX가 높은 곳은 도심지다. 하지만 NOX가 높을수록 집값이 낮다. 이는 도심지가 집값이 높다는 일반적인 상식에 위배된다.
따라서 L2 규제를 실시해보니, NOX의 영향력이 작아질수록 예측 정확도가 높아진다.
즉, alpha를 높여보니 Feature의 순위가 바뀐다면 그 특징은 주의깊게 살필 필요가 있다.

규제의 방법론
Regression -> L2 -> L1 순으로 실행해본다.
L2는 특징에 따른 영향력을 볼 수 있지만, w가 0으로 수렴하진 않는다.
L1은 w가 0으로 수렴할 수 있다.

엘라스틱넷
L1규제 + L2규제를 동시에 사용 가능하다.
규제간의 비율 조절이 가능하다.

Logistic Regression
Linear Regression을 2개로 분류해 본 방법이다.
threshold를 조절하며 결과값을 살피기 좋다.

PCA
다중공선성이 있는 두 피처 중 하나를 없앤다면 없어지기 전 피처로 설명 가능한 부분이 없어지게 되는 경우가 생긴다.
따라서 설명력도 가지면서 차원을 축소하는 방법이 PCA이다.
딥러닝에서는 차원 축소가 어느정도 이뤄지기 때문에 PCA가 거의 필요하지 않다.









